{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "name": "ch10_Recipe-Tacotron.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "committed-honduras",
        "square-fellow",
        "unnecessary-wrestling",
        "hindu-yesterday",
        "bigger-liability",
        "demographic-simon",
        "limited-sheffield",
        "dressed-listening",
        "acoustic-dietary",
        "partial-ceramic",
        "british-computer",
        "engaged-invention",
        "white-peoples",
        "serious-senegal",
        "active-armstrong",
        "excited-africa",
        "tight-acceptance",
        "genetic-islam",
        "theoretical-delivery",
        "delayed-southeast",
        "collectible-controversy",
        "above-multiple",
        "loving-sleeping",
        "treated-information",
        "injured-aspect",
        "coated-administrator",
        "integral-consultancy",
        "cellular-director",
        "unlimited-plaza",
        "unique-notebook",
        "mechanical-brighton",
        "peaceful-virus",
        "lasting-visibility",
        "transsexual-wages",
        "worthy-module",
        "sharing-brisbane",
        "voluntary-peoples",
        "fancy-shadow",
        "general-conversion",
        "underlying-break",
        "intelligent-wildlife",
        "exotic-parliament",
        "naval-medication",
        "large-greenhouse",
        "conceptual-writer",
        "friendly-contribution",
        "searching-provider"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykato27/Text-to-Speech/blob/main/ch10_Recipe_Tacotron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "wired-flash"
      },
      "source": [
        "# 第10章 日本語Tacotronに基づく音声合成システムの実装\n",
        "\n",
        "Google colabでの実行における推定所要時間: 5時間\n",
        "\n",
        "このノートブックに記載のレシピの設定は、Google Colab上で実行した場合のタイムアウトを避けるため、学習条件を書籍に記載の設定から一部修正していることに注意してください (バッチサイズを減らす等)。\n",
        "参考までに、書籍に記載の条件で、著者 (山本) がレシピを実行した結果を以下で公開しています。\n",
        "\n",
        "- Tensorboard logs: https://tensorboard.dev/experiment/gHKogn7wRxa4B3NIVw27xw/\n",
        "- expディレクトリ(学習済みモデル、合成音声を含む) : https://drive.google.com/file/d/1RvRCmHhqUGFwpR4KYMu_bh6K_VGlJmt_/view?usp=sharing (226.9 MB)"
      ],
      "id": "wired-flash"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "detailed-memorabilia"
      },
      "source": [
        "## 準備"
      ],
      "id": "detailed-memorabilia"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "historic-documentation"
      },
      "source": [
        "### Google Colabを利用する場合"
      ],
      "id": "historic-documentation"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sunrise-reach"
      },
      "source": [
        "Google Colab上でこのノートブックを実行する場合は、メニューの「ランタイム -> ランタイムのタイムの変更」から、「ハードウェア アクセラレータ」を **GPU** に変更してください。"
      ],
      "id": "sunrise-reach"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "round-terminology"
      },
      "source": [
        "### Python version"
      ],
      "id": "round-terminology"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "about-russia"
      },
      "source": [
        "!python -VV"
      ],
      "id": "about-russia",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "herbal-short"
      },
      "source": [
        "### ttslearn のインストール"
      ],
      "id": "herbal-short"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "foster-prague"
      },
      "source": [
        "%%capture\n",
        "try:\n",
        "    import ttslearn\n",
        "except ImportError:\n",
        "    !pip install ttslearn"
      ],
      "id": "foster-prague",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cutting-trauma"
      },
      "source": [
        "import ttslearn\n",
        "ttslearn.__version__"
      ],
      "id": "cutting-trauma",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "freelance-prize"
      },
      "source": [
        "## 10.1 本章の日本語音声合成システムの実装"
      ],
      "id": "freelance-prize"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "structured-senator"
      },
      "source": [
        "### 学習済みモデルを用いた音声合成"
      ],
      "id": "structured-senator"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "written-russian"
      },
      "source": [
        "from ttslearn.tacotron import Tacotron2TTS\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import Audio\n",
        "\n",
        "engine = Tacotron2TTS()\n",
        "wav, sr = engine.tts(\"一貫学習にチャレンジしましょう！\", tqdm=tqdm)\n",
        "Audio(wav, rate=sr)"
      ],
      "id": "written-russian",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "documented-indication"
      },
      "source": [
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,2))\n",
        "librosa.display.waveplot(wav.astype(np.float32), sr, ax=ax)\n",
        "ax.set_xlabel(\"Time [sec]\")\n",
        "ax.set_ylabel(\"Amplitude\")\n",
        "plt.tight_layout()"
      ],
      "id": "documented-indication",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "desperate-surname"
      },
      "source": [
        "### レシピ実行の前準備"
      ],
      "id": "desperate-surname"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guilty-ukraine"
      },
      "source": [
        "%%capture\n",
        "from ttslearn.env import is_colab\n",
        "from os.path import exists\n",
        "\n",
        "# pip install ttslearn ではレシピはインストールされないので、手動でダウンロード\n",
        "if is_colab() and not exists(\"recipes.zip\"):\n",
        "    !curl -LO https://github.com/r9y9/ttslearn/releases/download/v{ttslearn.__version__}/recipes.zip\n",
        "    !unzip -o recipes.zip"
      ],
      "id": "guilty-ukraine",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guilty-charity"
      },
      "source": [
        "import os\n",
        "# recipeのディレクトリに移動\n",
        "cwd = os.getcwd()\n",
        "if cwd.endswith(\"notebooks\"):\n",
        "    os.chdir(\"../recipes/tacotron/\")\n",
        "elif is_colab():\n",
        "    os.chdir(\"recipes/tacotron/\")   "
      ],
      "id": "guilty-charity",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "parallel-train"
      },
      "source": [
        "import time\n",
        "start_time = time.time()"
      ],
      "id": "parallel-train",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "committed-honduras"
      },
      "source": [
        "### パッケージのインポート"
      ],
      "id": "committed-honduras"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cathedral-guyana"
      },
      "source": [
        "%pylab inline\n",
        "%load_ext autoreload\n",
        "%load_ext tensorboard\n",
        "%autoreload\n",
        "import IPython\n",
        "from IPython.display import Audio\n",
        "import tensorboard as tb\n",
        "import os"
      ],
      "id": "cathedral-guyana",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "functioning-meditation"
      },
      "source": [
        "# 数値演算\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "# 音声波形の読み込み\n",
        "from scipy.io import wavfile\n",
        "# フルコンテキストラベル、質問ファイルの読み込み\n",
        "from nnmnkwii.io import hts\n",
        "# 音声分析\n",
        "import pyworld\n",
        "# 音声分析、可視化\n",
        "import librosa\n",
        "import librosa.display\n",
        "import pandas as pd\n",
        "# Pythonで学ぶ音声合成\n",
        "import ttslearn"
      ],
      "id": "functioning-meditation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "capital-mathematics"
      },
      "source": [
        "# シードの固定\n",
        "from ttslearn.util import init_seed\n",
        "init_seed(773)"
      ],
      "id": "capital-mathematics",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "closing-falls"
      },
      "source": [
        "torch.__version__"
      ],
      "id": "closing-falls",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "square-fellow"
      },
      "source": [
        "### 描画周りの設定"
      ],
      "id": "square-fellow"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liquid-stretch"
      },
      "source": [
        "from ttslearn.notebook import get_cmap, init_plot_style, savefig\n",
        "cmap = get_cmap()\n",
        "init_plot_style()"
      ],
      "id": "liquid-stretch",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unnecessary-wrestling"
      },
      "source": [
        "### レシピの設定"
      ],
      "id": "unnecessary-wrestling"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "streaming-slope"
      },
      "source": [
        "# run.shを利用した学習スクリプトをnotebookから行いたい場合は、True\n",
        "# google colab の場合は、True とします\n",
        "# ローカル環境の場合、run.sh をターミナルから実行することを推奨します。\n",
        "# その場合、このノートブックは可視化・学習済みモデルのテストのために利用します。\n",
        "run_sh = is_colab()\n",
        "\n",
        "# 注意: WaveNetを利用した評価データに対する音声生成は時間がかかることに注意\n",
        "run_stage6 = True\n",
        "\n",
        "# run.sh経由で実行するスクリプトのtqdm\n",
        "run_sh_tqdm = \"none\"\n",
        "\n",
        "# CUDA\n",
        "# NOTE: run.shの引数として渡すので、boolではなく文字列で定義しています\n",
        "cudnn_benchmark = \"true\"\n",
        "cudnn_deterministic = \"false\"\n",
        "\n",
        "# 特徴抽出時の並列処理のジョブ数\n",
        "n_jobs = os.cpu_count()//2\n",
        "\n",
        "# 音響モデル (Tacotron) の設定ファイル名\n",
        "acoustic_config_name=\"tacotron2_rf2\"\n",
        "# WaveNetボコーダの設定ファイル名\n",
        "wavenet_config_name=\"wavenet_sr16k_mulaw256_30layers\"\n",
        "\n",
        "# Tacotron学習におけるバッチサイズ\n",
        "tacotron_batch_size = 16\n",
        "# Tacotron学習のイテレーション数\n",
        "# 注意: 十分な品質を得るために必要な値: 50k ~ 100k steps\n",
        "tacotron_max_train_steps = 5000\n",
        "\n",
        "# WaveNetボコーダの学習におけるバッチサイズ\n",
        "# 推奨バッチサイズ:  8以上\n",
        "# 動作確認のため、小さな値に設定しています\n",
        "wavenet_batch_size = 4\n",
        "# WavaNetの学習イテレーション数\n",
        "# 注意: 十分な品質を得るために必要な値: 300k ~ 500k steps\n",
        "wavenet_max_train_steps = 20000\n",
        "\n",
        "# 音声生成を行う発話数\n",
        "# WaveNetの推論は時間がかかるので、ノートブックで表示する5つのみ生成する\n",
        "num_eval_utts = 5\n",
        "\n",
        "# ノートブックで利用するテスト用の発話（学習データ、評価データ）\n",
        "train_utt = \"BASIC5000_0001\"\n",
        "test_utt = \"BASIC5000_5000\""
      ],
      "id": "streaming-slope",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hindu-yesterday"
      },
      "source": [
        "### Tensorboard によるログの可視化"
      ],
      "id": "hindu-yesterday"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thorough-concern"
      },
      "source": [
        "# ノートブック上から tensorboard のログを確認する場合、次の行を有効にしてください\n",
        "if is_colab():\n",
        "    %tensorboard --logdir tensorboard/"
      ],
      "id": "thorough-concern",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "polyphonic-release"
      },
      "source": [
        "## 10.2 Tacotron 2 を日本語に適用するための変更"
      ],
      "id": "polyphonic-release"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bigger-liability"
      },
      "source": [
        "### 音素列と韻律記号付き音素列の比較"
      ],
      "id": "bigger-liability"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dramatic-payday"
      },
      "source": [
        "import pyopenjtalk\n",
        "# この実装は後述します\n",
        "from ttslearn.tacotron.frontend.openjtalk import pp_symbols"
      ],
      "id": "dramatic-payday",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "optional-somewhere"
      },
      "source": [
        "print(\"音素列:\", pyopenjtalk.g2p(\"端が\"))\n",
        "print(\"音素列:\", pyopenjtalk.g2p(\"箸が\"))\n",
        "print(\"音素列:\", pyopenjtalk.g2p(\"橋が\"))"
      ],
      "id": "optional-somewhere",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mexican-organization"
      },
      "source": [
        "print(\"韻律記号付き音素列:\", \" \".join(pp_symbols(pyopenjtalk.extract_fullcontext(\"端が\"))))\n",
        "print(\"韻律記号付き音素列:\", \" \".join(pp_symbols(pyopenjtalk.extract_fullcontext(\"箸が\"))))\n",
        "print(\"韻律記号付き音素列:\", \" \".join(pp_symbols(pyopenjtalk.extract_fullcontext(\"橋が\"))))"
      ],
      "id": "mexican-organization",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "demographic-simon"
      },
      "source": [
        "### フルコンテキストラベルからの音素列および韻律記号の抽出"
      ],
      "id": "demographic-simon"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "presidential-carry"
      },
      "source": [
        "import re\n",
        "\n",
        "def numeric_feature_by_regex(regex, s):\n",
        "    match = re.search(regex, s)\n",
        "    # 未定義 (xx) の場合、コンテキストの取りうる値以外の適当な値\n",
        "    if match is None:\n",
        "        return -50\n",
        "    return int(match.group(1))"
      ],
      "id": "presidential-carry",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adjusted-contrary"
      },
      "source": [
        "labels = hts.load(ttslearn.util.example_label_file())\n",
        "labels.contexts[1]"
      ],
      "id": "adjusted-contrary",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "complicated-drain"
      },
      "source": [
        "numeric_feature_by_regex(r\"/A:([0-9\\-]+)\\+\", labels.contexts[1])"
      ],
      "id": "complicated-drain",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "declared-homework"
      },
      "source": [
        "def pp_symbols(labels, drop_unvoiced_vowels=True):\n",
        "    PP = []\n",
        "    N = len(labels)\n",
        "\n",
        "    # 各音素毎に順番に処理\n",
        "    for n in range(N):\n",
        "        lab_curr = labels[n]\n",
        "\n",
        "        # 当該音素\n",
        "        p3 = re.search(r\"\\-(.*?)\\+\", lab_curr).group(1)\n",
        "\n",
        "        # 無声化母音を通常の母音として扱う\n",
        "        if drop_unvoiced_vowels and p3 in \"AEIOU\":\n",
        "            p3 = p3.lower()\n",
        "\n",
        "        # 先頭と末尾の sil のみ例外対応\n",
        "        if p3 == \"sil\":\n",
        "            assert n == 0 or n == N - 1\n",
        "            if n == 0:\n",
        "                PP.append(\"^\")\n",
        "            elif n == N - 1:\n",
        "                # 疑問系かどうか\n",
        "                e3 = numeric_feature_by_regex(r\"!(\\d+)_\", lab_curr)\n",
        "                if e3 == 0:\n",
        "                    PP.append(\"$\")\n",
        "                elif e3 == 1:\n",
        "                    PP.append(\"?\")\n",
        "            continue\n",
        "        elif p3 == \"pau\":\n",
        "            PP.append(\"_\")\n",
        "            continue\n",
        "        else:\n",
        "            PP.append(p3)\n",
        "\n",
        "        # アクセント型および位置情報（前方または後方）\n",
        "        a1 = numeric_feature_by_regex(r\"/A:([0-9\\-]+)\\+\", lab_curr)\n",
        "        a2 = numeric_feature_by_regex(r\"\\+(\\d+)\\+\", lab_curr)\n",
        "        a3 = numeric_feature_by_regex(r\"\\+(\\d+)/\", lab_curr)\n",
        "        # アクセント句におけるモーラ数\n",
        "        f1 = numeric_feature_by_regex(r\"/F:(\\d+)_\", lab_curr)\n",
        "\n",
        "        a2_next = numeric_feature_by_regex(r\"\\+(\\d+)\\+\", labels[n + 1])\n",
        "\n",
        "        # アクセント句境界\n",
        "        if a3 == 1 and a2_next == 1:\n",
        "            PP.append(\"#\")\n",
        "        # ピッチの立ち下がり（アクセント核）\n",
        "        elif a1 == 0 and a2_next == a2 + 1 and a2 != f1:\n",
        "            PP.append(\"]\")\n",
        "        # ピッチの立ち上がり\n",
        "        elif a2 == 1 and a2_next == 2:\n",
        "            PP.append(\"[\")\n",
        "\n",
        "    return PP"
      ],
      "id": "declared-homework",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "small-contract"
      },
      "source": [
        "import pyopenjtalk\n",
        "\n",
        "text = \"今日の天気は？\"\n",
        "\n",
        "# テキストからフルコンテキストを抽出\n",
        "labels = pyopenjtalk.extract_fullcontext(text)\n",
        "# フルコンテキストから、韻律記号付き音素列に変換\n",
        "PP = pp_symbols(labels)\n",
        "\n",
        "print(\"入力文字列:\", text)\n",
        "print(\"音素列:\", pyopenjtalk.g2p(text))\n",
        "print(\"韻律記号付き音素列:\", \" \".join(PP))"
      ],
      "id": "small-contract",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "threatened-hours"
      },
      "source": [
        "## プログラム実装の前準備"
      ],
      "id": "threatened-hours"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "limited-sheffield"
      },
      "source": [
        "### stage -1: コーパスのダウンロード"
      ],
      "id": "limited-sheffield"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "selected-arnold"
      },
      "source": [
        "if is_colab():\n",
        "    ! ./run.sh --stage -1 --stop-stage -1"
      ],
      "id": "selected-arnold",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dressed-listening"
      },
      "source": [
        "### Stage 0: 学習/検証/評価データの分割"
      ],
      "id": "dressed-listening"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "labeled-binary"
      },
      "source": [
        "if run_sh:\n",
        "    ! ./run.sh --stage 0 --stop-stage 0"
      ],
      "id": "labeled-binary",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "freelance-register"
      },
      "source": [
        "! ls data/"
      ],
      "id": "freelance-register",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "legislative-translator"
      },
      "source": [
        "! head data/dev.list"
      ],
      "id": "legislative-translator",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "incorporated-station"
      },
      "source": [
        "## 10.3 データの前処理"
      ],
      "id": "incorporated-station"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acoustic-dietary"
      },
      "source": [
        "### Tacotron 2 のための前処理"
      ],
      "id": "acoustic-dietary"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "partial-ceramic"
      },
      "source": [
        "#### 1発話に対する前処理"
      ],
      "id": "partial-ceramic"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "incredible-opportunity"
      },
      "source": [
        "from ttslearn.tacotron.frontend.openjtalk import text_to_sequence, pp_symbols\n",
        "from ttslearn.dsp import mulaw_quantize, logmelspectrogram\n",
        "\n",
        "# 韻律記号付き音素列の抽出\n",
        "labels = hts.load(ttslearn.util.example_label_file())\n",
        "PP = pp_symbols(labels.contexts)\n",
        "in_feats = np.array(text_to_sequence(PP), dtype=np.int64)\n",
        "\n",
        "# メルスペクトログラムの計算\n",
        "sr = 16000\n",
        "_sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
        "x = (x / 32768).astype(np.float64)\n",
        "x = librosa.resample(x, _sr, sr)\n",
        "\n",
        "out_feats = logmelspectrogram(x, sr)\n",
        "\n",
        "# 冒頭と末尾の非音声区間の長さを調整\n",
        "assert \"sil\" in labels.contexts[0] and \"sil\" in labels.contexts[-1]\n",
        "start_frame = int(labels.start_times[1] / 125000)\n",
        "end_frame = int(labels.end_times[-2] / 125000)\n",
        "\n",
        "# 最初：50 ミリ秒、最後：100 ミリ秒\n",
        "start_frame = max(0, start_frame - int(0.050 / 0.0125))\n",
        "end_frame = min(len(out_feats), end_frame + int(0.100 / 0.0125))\n",
        "\n",
        "out_feats = out_feats[start_frame:end_frame]\n",
        "\n",
        "# 時間領域で音声の長さを調整\n",
        "x = x[int(start_frame * 0.0125 * sr) :]\n",
        "length = int(sr * 0.0125) * out_feats.shape[0]\n",
        "x = pad_1d(x, length) if len(x) < length else x[:length]\n",
        "\n",
        "# 特徴量のアップサンプリングを行う都合上、音声波形の長さはフレームシフトで割り切れる必要があります\n",
        "assert len(x) % int(sr * 0.0125) == 0\n",
        "\n",
        "# mu-law 量子化\n",
        "x = mulaw_quantize(x)"
      ],
      "id": "incredible-opportunity",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "explicit-steam"
      },
      "source": [
        "print(\"Tacotron の入力特徴量のサイズ:\", in_feats.shape)\n",
        "print(\"Tacotron の出力特徴量のサイズ:\", out_feats.shape)\n",
        "print(\"WaveNet ボコーダの出力の音声波形のサイズ:\", x.shape)"
      ],
      "id": "explicit-steam",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "temporal-armor"
      },
      "source": [
        "from ttslearn.tacotron.frontend.openjtalk import num_vocab\n",
        "from ttslearn.dsp import inv_mulaw_quantize\n",
        "from torch.nn import functional as F\n",
        "\n",
        "inp = F.one_hot(torch.from_numpy(in_feats), num_vocab()).numpy()\n",
        "\n",
        "fig, ax = plt.subplots(3, 1, figsize=(8,8))\n",
        "ax[0].set_title(\"Phoneme sequence + prosody symbols (one-hot)\")\n",
        "ax[1].set_title(\"Mel-spectrogram\")\n",
        "ax[2].set_title(\"Mu-law quantized waveform\")\n",
        "\n",
        "ax[0].imshow(inp.T, aspect=\"auto\", interpolation=\"nearest\", origin=\"lower\", cmap=cmap)\n",
        "ax[1].imshow(out_feats.T, aspect=\"auto\", interpolation=\"nearest\", origin=\"lower\", cmap=cmap)\n",
        "librosa.display.waveplot(x.astype(np.float32), ax=ax[2], sr=sr)\n",
        "\n",
        "ax[0].set_xlabel(\"Phoneme\")\n",
        "ax[0].set_ylabel(\"Binary value\")\n",
        "ax[1].set_xlabel(\"Time [frame]\")\n",
        "ax[1].set_ylabel(\"Mel filter channel\")\n",
        "ax[2].set_xlabel(\"Time [sec]\")\n",
        "ax[2].set_ylabel(\"Amplitude\")\n",
        "\n",
        "plt.tight_layout()\n",
        "savefig(\"fig/e2etts_impl_taco2_inout\")"
      ],
      "id": "temporal-armor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "british-computer"
      },
      "source": [
        "#### レシピの stage 1 の実行\n",
        "\n",
        "バッチ処理を行うコマンドラインプログラムは、 `preprocess.py` を参照してください。"
      ],
      "id": "british-computer"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "developmental-legislature"
      },
      "source": [
        "if run_sh:\n",
        "    ! ./run.sh --stage 1 --stop-stage 1"
      ],
      "id": "developmental-legislature",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "engaged-invention"
      },
      "source": [
        "### 特徴量の正規化"
      ],
      "id": "engaged-invention"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abandoned-aurora"
      },
      "source": [
        "正規化のための統計量を計算するコマンドラインプログラムは、 `recipes/common/fit_scaler.py` を参照してください。また、正規化を行うコマンドラインプログラムは、 `recipes/common/preprocess_normalize.py` を参照してください。"
      ],
      "id": "abandoned-aurora"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "white-peoples"
      },
      "source": [
        "#### レシピの stage 2 の実行"
      ],
      "id": "white-peoples"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "developed-tours"
      },
      "source": [
        "if run_sh:\n",
        "    ! ./run.sh --stage 2 --stop-stage 2 --n-jobs $n_jobs"
      ],
      "id": "developed-tours",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "serious-senegal"
      },
      "source": [
        "#### 正規化の処理の結果の確認"
      ],
      "id": "serious-senegal"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "needed-colon"
      },
      "source": [
        "in_feats = np.load(f\"dump/jsut_sr16000/org/train/out_tacotron/{train_utt}-feats.npy\")\n",
        "in_feats_norm = np.load(f\"dump/jsut_sr16000/norm/train/out_tacotron/{train_utt}-feats.npy\")\n",
        "fig, ax = plt.subplots(2, 1, figsize=(8,6), sharex=True)\n",
        "ax[0].set_title(\"Mel-spectrogram (before normalization)\")\n",
        "ax[1].set_title(\"Mel-spectrogram (after normalization)\")\n",
        "\n",
        "hop_length = int(sr * 0.0125)\n",
        "mesh = librosa.display.specshow(\n",
        "    in_feats.T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"frames\", ax=ax[0], cmap=cmap)\n",
        "fig.colorbar(mesh, ax=ax[0])\n",
        "mesh = librosa.display.specshow(\n",
        "    in_feats_norm.T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"frames\",ax=ax[1], cmap=cmap)\n",
        "mesh.set_clim(-4, 4)\n",
        "fig.colorbar(mesh, ax=ax[1])\n",
        "\n",
        "for a in ax:\n",
        "    a.set_xlabel(\"Time [sec]\")\n",
        "    a.set_ylabel(\"Mel filter channel\")\n",
        "plt.tight_layout()"
      ],
      "id": "needed-colon",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "extra-silicon"
      },
      "source": [
        "## 10.4 Tacotron の学習スクリプトの作成"
      ],
      "id": "extra-silicon"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "active-armstrong"
      },
      "source": [
        "### DataLoader の実装"
      ],
      "id": "active-armstrong"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "excited-africa"
      },
      "source": [
        "#### collate_fn の実装"
      ],
      "id": "excited-africa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "addressed-renaissance"
      },
      "source": [
        "def ensure_divisible_by(feats, N):\n",
        "    if N == 1:\n",
        "        return feats\n",
        "    mod = len(feats) % N\n",
        "    if mod != 0:\n",
        "        feats = feats[: len(feats) - mod]\n",
        "    return feats"
      ],
      "id": "addressed-renaissance",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "checked-france"
      },
      "source": [
        "from ttslearn.util import pad_1d, pad_2d\n",
        "\n",
        "def collate_fn_tacotron(batch, reduction_factor=1):\n",
        "    xs = [x[0] for x in batch]\n",
        "    ys = [ensure_divisible_by(x[1], reduction_factor) for x in batch]\n",
        "    in_lens = [len(x) for x in xs]\n",
        "    out_lens = [len(y) for y in ys]\n",
        "    in_max_len = max(in_lens)\n",
        "    out_max_len = max(out_lens)\n",
        "    x_batch = torch.stack([torch.from_numpy(pad_1d(x, in_max_len)) for x in xs])\n",
        "    y_batch = torch.stack([torch.from_numpy(pad_2d(y, out_max_len)) for y in ys])\n",
        "    in_lens = torch.tensor(in_lens, dtype=torch.long)\n",
        "    out_lens = torch.tensor(out_lens, dtype=torch.long)\n",
        "    stop_flags = torch.zeros(y_batch.shape[0], y_batch.shape[1])\n",
        "    for idx, out_len in enumerate(out_lens):\n",
        "        stop_flags[idx, out_len - 1 :] = 1.0\n",
        "    return x_batch, in_lens, y_batch, out_lens, stop_flags"
      ],
      "id": "checked-france",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tight-acceptance"
      },
      "source": [
        "#### DataLoader の利用例"
      ],
      "id": "tight-acceptance"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "detailed-privilege"
      },
      "source": [
        "from pathlib import Path\n",
        "from ttslearn.train_util import Dataset, collate_fn_tacotron\n",
        "from functools import partial\n",
        "\n",
        "in_paths = sorted(Path(\"./dump/jsut_sr16000/norm/dev/in_tacotron/\").glob(\"*.npy\"))\n",
        "out_paths = sorted(Path(\"./dump/jsut_sr16000/norm/dev/out_tacotron/\").glob(\"*.npy\"))\n",
        "\n",
        "dataset = Dataset(in_paths, out_paths)\n",
        "collate_fn = partial(collate_fn_tacotron, reduction_factor=1)\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=8, collate_fn=collate_fn, num_workers=0)\n",
        "\n",
        "in_feats, in_lens, out_feats, out_lens, stop_flags = next(iter(data_loader))\n",
        "print(\"入力特徴量のサイズ:\", tuple(in_feats.shape))\n",
        "print(\"出力特徴量のサイズ:\", tuple(out_feats.shape))\n",
        "print(\"stop flags のサイズ:\", tuple(stop_flags.shape))"
      ],
      "id": "detailed-privilege",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "genetic-islam"
      },
      "source": [
        "#### ミニバッチの可視化"
      ],
      "id": "genetic-islam"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hawaiian-strengthening"
      },
      "source": [
        "fig, ax = plt.subplots(len(out_feats), 1, figsize=(8,10), sharex=True, sharey=True)\n",
        "for n in range(len(in_feats)):\n",
        "    x = out_feats[n].data.numpy()\n",
        "    hop_length = int(sr * 0.0125)\n",
        "    mesh = librosa.display.specshow(x.T, sr=sr, x_axis=\"time\", y_axis=\"frames\", hop_length=hop_length, cmap=cmap, ax=ax[n])\n",
        "    fig.colorbar(mesh, ax=ax[n])\n",
        "    mesh.set_clim(-4, 4)\n",
        "    # あとで付け直すので、ここではラベルを削除します\n",
        "    ax[n].set_xlabel(\"\")\n",
        "    \n",
        "ax[-1].set_xlabel(\"Time [sec]\")\n",
        "for a in ax:\n",
        "    a.set_ylabel(\"Mel channel\")\n",
        "\n",
        "plt.tight_layout()\n",
        "savefig(\"fig/e2etts_impl_minibatch\")"
      ],
      "id": "hawaiian-strengthening",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "theoretical-delivery"
      },
      "source": [
        "### 簡易的な学習スクリプトの実装"
      ],
      "id": "theoretical-delivery"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delayed-southeast"
      },
      "source": [
        "#### 学習の前準備"
      ],
      "id": "delayed-southeast"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "modern-houston"
      },
      "source": [
        "from ttslearn.tacotron import Tacotron2 as Tacotron\n",
        "from torch import optim\n",
        "\n",
        "# 動作確認用：層の数を減らした小さなTacotron\n",
        "model = Tacotron(\n",
        "    embed_dim=32, encoder_conv_layers=1, encoder_conv_channels=32, encoder_hidden_dim=32,\n",
        "    decoder_hidden_dim=32, postnet_channels=32, postnet_layers=1)\n",
        "\n",
        "# lr は学習率を表します\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# gamma は学習率の減衰係数を表します\n",
        "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, gamma=0.5, step_size=100000)"
      ],
      "id": "modern-houston",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "collectible-controversy"
      },
      "source": [
        "#### 学習ループの実装"
      ],
      "id": "collectible-controversy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fatty-constitutional"
      },
      "source": [
        "from ttslearn.util import make_non_pad_mask\n",
        "\n",
        "# DataLoader を用いたミニバッチの作成: ミニバッチ毎に処理する\n",
        "for in_feats, in_lens, out_feats, out_lens, stop_flags in tqdm(data_loader):\n",
        "    in_lens, indices = torch.sort(in_lens, dim=0, descending=True)\n",
        "    in_feats, out_feats, out_lens = in_feats[indices], out_feats[indices], out_lens[indices]\n",
        "    \n",
        "    # 順伝搬の計算\n",
        "    outs, outs_fine, logits, _ = model(in_feats, in_lens, out_feats)\n",
        "    \n",
        "    # ゼロパディグした部分を損失関数のの計算から除外するためにマスクを適用します\n",
        "    # Mask (B x T x 1)\n",
        "    mask = make_non_pad_mask(out_lens).unsqueeze(-1)\n",
        "    out_feats = out_feats.masked_select(mask)\n",
        "    outs = outs.masked_select(mask)\n",
        "    outs_fine = outs_fine.masked_select(mask)\n",
        "    stop_flags = stop_flags.masked_select(mask.squeeze(-1))\n",
        "    logits = logits.masked_select(mask.squeeze(-1))\n",
        "\n",
        "    # 損失の計算\n",
        "    decoder_out_loss = nn.MSELoss()(outs, out_feats)\n",
        "    postnet_out_loss = nn.MSELoss()(outs_fine, out_feats) \n",
        "    stop_token_loss = nn.BCEWithLogitsLoss()(logits, stop_flags)\n",
        "    \n",
        "    # 損失の合計\n",
        "    loss = decoder_out_loss + postnet_out_loss + stop_token_loss\n",
        "\n",
        "    # 損失の値を出力\n",
        "    print(f\"decoder_out_loss: {decoder_out_loss:.2f}, postnet_out_loss: {postnet_out_loss:.2f}, stop_token_loss: {stop_token_loss:.2f}\")\n",
        "    # optimizer に蓄積された勾配をリセット\n",
        "    optimizer.zero_grad()\n",
        "    # 誤差の逆伝播\n",
        "    loss.backward()\n",
        "    # パラメータの更新\n",
        "    optimizer.step()\n",
        "    # 学習率スケジューラの更新\n",
        "    lr_scheduler.step()"
      ],
      "id": "fatty-constitutional",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "above-multiple"
      },
      "source": [
        "### アテンション重みの可視化\n",
        "\n",
        "ここでは、学習が正常に進行していない場合の例として、意図的に学習済みモデルの一部のパラメータを乱数で初期化します。詳細は、`randomize_tts_engine_` 参照してください。"
      ],
      "id": "above-multiple"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "placed-assumption"
      },
      "source": [
        "from ttslearn.tacotron import Tacotron2TTS\n",
        "from ttslearn.tacotron.tts import randomize_tts_engine_\n",
        "\n",
        "tacotron_engine = Tacotron2TTS()\n",
        "\n",
        "tacotron_engine_bad = Tacotron2TTS()\n",
        "randomize_tts_engine_(tacotron_engine_bad)\n",
        "print(\"randomized some of network weights\")"
      ],
      "id": "placed-assumption",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "understood-friday"
      },
      "source": [
        "text = \"水をマレーシアから買わなくてはならないのです。\"\n",
        "\n",
        "import pyopenjtalk\n",
        "from ttslearn.tacotron.frontend.openjtalk import text_to_sequence, pp_symbols\n",
        "\n",
        "labels = pyopenjtalk.extract_fullcontext(text)\n",
        "# 韻律記号付き音素列\n",
        "in_feats = text_to_sequence(pp_symbols(labels))\n",
        "in_feats = torch.tensor(in_feats, dtype=torch.long)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outs, outs_fine, logits, att_ws = tacotron_engine.acoustic_model.inference(in_feats)\n",
        "    \n",
        "with torch.no_grad():\n",
        "    outs2, outs_fine2, logits2, att_ws2 = tacotron_engine_bad.acoustic_model.inference(in_feats)"
      ],
      "id": "understood-friday",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rapid-conversion"
      },
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
        "ax[0].set_title(\"Failure\")\n",
        "ax[1].set_title(\"Normal\")\n",
        "\n",
        "mesh = ax[0].imshow(att_ws2.cpu().data.numpy().T, aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\")\n",
        "fig.colorbar(mesh, ax=ax[0])\n",
        "ax[0].set_xlabel(\"Decoder time step [frame]\")\n",
        "ax[0].set_ylabel(\"Encoder time step [phoneme]\")\n",
        "\n",
        "mesh = ax[1].imshow(att_ws.cpu().data.numpy().T, aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\")\n",
        "fig.colorbar(mesh, ax=ax[1])\n",
        "ax[1].set_xlabel(\"Decoder time step [frame]\")\n",
        "ax[1].set_ylabel(\"Encoder time step [phoneme]\")\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# 図10-5\n",
        "savefig(\"./fig/e2etts_impl_attention_failure\")"
      ],
      "id": "rapid-conversion",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loving-sleeping"
      },
      "source": [
        "### 実用的な学習スクリプトの実装"
      ],
      "id": "loving-sleeping"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "white-wayne"
      },
      "source": [
        "`train_tacotron.py` を参照してください。"
      ],
      "id": "white-wayne"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "legitimate-techno"
      },
      "source": [
        "## 10.5 Tacotron の学習"
      ],
      "id": "legitimate-techno"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "treated-information"
      },
      "source": [
        "### Tacotron の設定ファイル"
      ],
      "id": "treated-information"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "compatible-giving"
      },
      "source": [
        "! cat conf/train_tacotron/model/{acoustic_config_name}.yaml"
      ],
      "id": "compatible-giving",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "injured-aspect"
      },
      "source": [
        "### Tacotron のインスタンス化"
      ],
      "id": "injured-aspect"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ethical-bahrain"
      },
      "source": [
        "import hydra\n",
        "from omegaconf import OmegaConf\n",
        "hydra.utils.instantiate(OmegaConf.load(f\"./conf/train_tacotron/model/{acoustic_config_name}.yaml\")[\"netG\"])"
      ],
      "id": "ethical-bahrain",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coated-administrator"
      },
      "source": [
        "### レシピの stage 3 の実行"
      ],
      "id": "coated-administrator"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "secure-efficiency"
      },
      "source": [
        "if run_sh:\n",
        "    ! ./run.sh --stage 3 --stop-stage 3 --acoustic-model $acoustic_config_name \\\n",
        "        --tqdm $run_sh_tqdm --tacotron-train-max-train-steps $tacotron_max_train_steps \\\n",
        "        --tacotron-data-batch-size $tacotron_batch_size \\\n",
        "        --cudnn-benchmark $cudnn_benchmark --cudnn-deterministic $cudnn_deterministic"
      ],
      "id": "secure-efficiency",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "integral-consultancy"
      },
      "source": [
        "### 損失関数の値の推移\n",
        "\n",
        "著者による実験結果です。Tensorboardのログは https://tensorboard.dev/ にアップロードされています。\n",
        "ログデータを`tensorboard` パッケージを利用してダウンロードします。\n",
        "\n",
        "https://tensorboard.dev/experiment/yXyg9qgfQRSGxvil5FA4xw/"
      ],
      "id": "integral-consultancy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beginning-picking"
      },
      "source": [
        "if exists(\"tensorboard/all_log.csv\"):\n",
        "    df = pd.read_csv(\"tensorboard/all_log.csv\")\n",
        "else:\n",
        "    experiment_id = \"gHKogn7wRxa4B3NIVw27xw\"\n",
        "    experiment = tb.data.experimental.ExperimentFromDev(experiment_id)\n",
        "    df = experiment.get_scalars() \n",
        "    df.to_csv(\"tensorboard/all_log.csv\", index=False)\n",
        "df[\"run\"].unique()"
      ],
      "id": "beginning-picking",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "finnish-comment"
      },
      "source": [
        "tacotron_loss = df[df.run.str.contains(\"tacotron2_rf2\")]\n",
        "\n",
        "tacotron_train_loss = tacotron_loss[tacotron_loss.tag.str.startswith(\"Loss/train\")]\n",
        "tacotron_dev_loss = tacotron_loss[tacotron_loss.tag.str.startswith(\"Loss/dev\")]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "ax.plot(tacotron_train_loss[\"step\"], tacotron_train_loss[\"value\"], label=\"Train\")\n",
        "ax.plot(tacotron_dev_loss[\"step\"], tacotron_dev_loss[\"value\"], \"--\", label=\"Dev\")\n",
        "ax.set_xlabel(\"Epoch\")\n",
        "ax.set_ylabel(\"Epoch loss\")\n",
        "plt.legend()\n",
        "\n",
        "# 図10-6\n",
        "savefig(\"fig/tacotron_impl_tacotron_loss\")"
      ],
      "id": "finnish-comment",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spoken-entry"
      },
      "source": [
        "## 10.6 WaveNet ボコーダ学習"
      ],
      "id": "spoken-entry"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cellular-director"
      },
      "source": [
        "### WaveNetボコーダ の設定ファイル"
      ],
      "id": "cellular-director"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unusual-vietnam"
      },
      "source": [
        "! cat conf/train_wavenet/model/{wavenet_config_name}.yaml"
      ],
      "id": "unusual-vietnam",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unlimited-plaza"
      },
      "source": [
        "### WaveNetボコーダ のインスタンス化"
      ],
      "id": "unlimited-plaza"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "italic-enterprise"
      },
      "source": [
        "import hydra\n",
        "from omegaconf import OmegaConf\n",
        "# WaveNet の 30層 すべてを表示すると長くなるため、ここでは省略します。\n",
        "# hydra.utils.instantiate(OmegaConf.load(f\"./conf/train_wavenet/model/{wavenet_config_name}.yaml\")[\"netG\"])"
      ],
      "id": "italic-enterprise",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unique-notebook"
      },
      "source": [
        "### レシピの stage 4 の実行"
      ],
      "id": "unique-notebook"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "contemporary-residence"
      },
      "source": [
        "if run_sh:\n",
        "    ! ./run.sh --stage 4 --stop-stage 4 --wavenet-model $wavenet_config_name \\\n",
        "        --tqdm $run_sh_tqdm --wavenet-train-max-train-steps $wavenet_max_train_steps \\\n",
        "        --wavenet-data-batch-size $wavenet_batch_size \\\n",
        "        --cudnn-benchmark $cudnn_benchmark --cudnn-deterministic $cudnn_deterministic"
      ],
      "id": "contemporary-residence",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mechanical-brighton"
      },
      "source": [
        "### 損失関数の値の推移"
      ],
      "id": "mechanical-brighton"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "square-playback"
      },
      "source": [
        "wavenet_loss = df[df.run.str.contains(\"wavenet\")]\n",
        "\n",
        "wavenet_train_loss = wavenet_loss[wavenet_loss.tag.str.contains(\"Loss/train\")]\n",
        "wavenet_dev_loss = wavenet_loss[wavenet_loss.tag.str.contains(\"Loss/dev\")]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "ax.plot(wavenet_train_loss[\"step\"], wavenet_train_loss[\"value\"], label=\"Train\")\n",
        "ax.plot(wavenet_dev_loss[\"step\"], wavenet_dev_loss[\"value\"], \"--\", label=\"Dev\")\n",
        "ax.set_xlabel(\"Epoch\")\n",
        "ax.set_ylabel(\"Epoch loss\")\n",
        "ax.set_ylim(1.6, 2.3)\n",
        "plt.legend()\n",
        "\n",
        "# 図10-7\n",
        "savefig(\"fig/tacotron_impl_wavenet_loss\")"
      ],
      "id": "square-playback",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faced-coordinator"
      },
      "source": [
        "## 10.7 学習済みモデルを用いてテキストから音声を合成"
      ],
      "id": "faced-coordinator"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peaceful-virus"
      },
      "source": [
        "### 学習済みモデルの読み込み"
      ],
      "id": "peaceful-virus"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "altered-peeing"
      },
      "source": [
        "import joblib\n",
        "device = torch.device(\"cpu\")"
      ],
      "id": "altered-peeing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lasting-visibility"
      },
      "source": [
        "#### Tacotronの読み込み"
      ],
      "id": "lasting-visibility"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "associate-festival"
      },
      "source": [
        "acoustic_config = OmegaConf.load(f\"exp/jsut_sr16000/{acoustic_config_name}/model.yaml\")\n",
        "acoustic_model = hydra.utils.instantiate(acoustic_config.netG)\n",
        "checkpoint = torch.load(f\"exp/jsut_sr16000/{acoustic_config_name}/latest.pth\", map_location=device)\n",
        "acoustic_model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "acoustic_model.eval();"
      ],
      "id": "associate-festival",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "transsexual-wages"
      },
      "source": [
        "#### WaveNetボコーダの読み込み"
      ],
      "id": "transsexual-wages"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frequent-hundred"
      },
      "source": [
        "wavenet_config = OmegaConf.load(f\"exp/jsut_sr16000/{wavenet_config_name}/model.yaml\")\n",
        "wavenet_model = hydra.utils.instantiate(wavenet_config.netG)\n",
        "checkpoint = torch.load(f\"exp/jsut_sr16000/{wavenet_config_name}/latest_ema.pth\", map_location=device)\n",
        "wavenet_model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "# weight normalization は推論時には不要なため除く\n",
        "wavenet_model.remove_weight_norm_()\n",
        "wavenet_model.eval();"
      ],
      "id": "frequent-hundred",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "worthy-module"
      },
      "source": [
        "#### 統計量の読み込み\n",
        "\n",
        "統計量は、Griffin-Limのアルゴリズムを利用する場合にのみ必要となります。"
      ],
      "id": "worthy-module"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "private-beijing"
      },
      "source": [
        "acoustic_out_scaler = joblib.load(\"./dump/jsut_sr16000/norm/out_tacotron_scaler.joblib\")"
      ],
      "id": "private-beijing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sharing-brisbane"
      },
      "source": [
        "### メルスペクトログラムの予測"
      ],
      "id": "sharing-brisbane"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "under-draft"
      },
      "source": [
        "from ttslearn.util import find_lab, find_feats\n",
        "\n",
        "labels = hts.load(find_lab(\"downloads/jsut-label/\", test_utt))\n",
        "\n",
        "in_feats = text_to_sequence(pp_symbols(labels.contexts))\n",
        "in_feats = torch.tensor(in_feats, dtype=torch.long).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    out_feats, out_feats_fine, stop_flags, alignment = acoustic_model.inference(in_feats)\n",
        "    \n",
        "# 比較用に、自然音声から抽出された音響特徴量を読み込みむ\n",
        "feats = np.load(find_feats(\"dump/jsut_sr16000/norm/\", test_utt, typ=\"out_tacotron\"))"
      ],
      "id": "under-draft",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voluntary-peoples"
      },
      "source": [
        "#### メルスペクトログラムの可視化"
      ],
      "id": "voluntary-peoples"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sustained-greenhouse"
      },
      "source": [
        "fig, ax = plt.subplots(2, 1, figsize=(8,6))\n",
        "ax[0].set_title(\"Mel-spectrogram of natural speech\")\n",
        "ax[1].set_title(\"Mel-spectrogram of Tacotron output\")\n",
        "\n",
        "mindb = min(feats.min(), out_feats_fine.min())\n",
        "maxdb = max(feats.max(), out_feats_fine.max())\n",
        "\n",
        "hop_length = int(sr * 0.0125)\n",
        "mesh = librosa.display.specshow(\n",
        "    feats.T, sr=sr, x_axis=\"time\", y_axis=\"frames\", hop_length=hop_length, cmap=cmap, ax=ax[0])\n",
        "mesh.set_clim(mindb, maxdb)\n",
        "fig.colorbar(mesh, ax=ax[0])\n",
        "mesh = librosa.display.specshow(\n",
        "    out_feats_fine.data.numpy().T, sr=sr, x_axis=\"time\", y_axis=\"frames\", hop_length=hop_length, cmap=cmap, ax=ax[1])\n",
        "mesh.set_clim(mindb, maxdb)\n",
        "fig.colorbar(mesh, ax=ax[1])\n",
        "\n",
        "for a in ax:\n",
        "    a.set_xlabel(\"Time [sec]\")\n",
        "    a.set_ylabel(\"Mel filter channel\")\n",
        "fig.tight_layout()\n",
        "\n",
        "# 図10-8\n",
        "savefig(\"./fig/e2etts_impl_logmel_comp\")"
      ],
      "id": "sustained-greenhouse",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fancy-shadow"
      },
      "source": [
        "#### アテンション重みの可視化"
      ],
      "id": "fancy-shadow"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "restricted-joining"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "im = ax.imshow(alignment.cpu().data.numpy().T, aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\")\n",
        "fig.colorbar(im, ax=ax)\n",
        "ax.set_xlabel(\"Decoder time step [frame]\")\n",
        "ax.set_ylabel(\"Encoder time step [phoneme]\");"
      ],
      "id": "restricted-joining",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "general-conversion"
      },
      "source": [
        "#### Stop token の可視化"
      ],
      "id": "general-conversion"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "found-maker"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "ax.plot(torch.sigmoid(stop_flags).cpu().numpy())\n",
        "ax.set_xlabel(\"Time [frame]\")\n",
        "ax.set_ylabel(\"Stop probability\");"
      ],
      "id": "found-maker",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "underlying-break"
      },
      "source": [
        "### 音声波形の生成"
      ],
      "id": "underlying-break"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "revolutionary-friendship"
      },
      "source": [
        "from ttslearn.dsp import inv_mulaw_quantize\n",
        "\n",
        "@torch.no_grad()\n",
        "def gen_waveform(wavenet_model, out_feats):\n",
        "    # (B, T, C) -> (B, C, T)\n",
        "    c = out_feats.view(1, -1, out_feats.size(-1)).transpose(1, 2)\n",
        "\n",
        "    # 音声のサンプル数を計算\n",
        "    upsample_scale = np.prod(wavenet_model.upsample_scales)\n",
        "    T = (\n",
        "        c.shape[-1] - wavenet_model.aux_context_window * 2\n",
        "    ) * upsample_scale\n",
        "\n",
        "    # WaveNet による音声波形の生成\n",
        "    # NOTE: 計算に時間がかかるため、tqdm によるプログレスバーを受け付けるようにしています\n",
        "    gen_wav = wavenet_model.inference(c, T, tqdm)\n",
        "\n",
        "    # One-hot ベクトルから1次元の信号に変換\n",
        "    gen_wav = gen_wav.max(1)[1].float().cpu().numpy().reshape(-1)\n",
        "\n",
        "    # Mu-law 量子化の逆変換\n",
        "    gen_wav = inv_mulaw_quantize(\n",
        "        gen_wav, wavenet_model.out_channels - 1\n",
        "    )\n",
        "    \n",
        "    return gen_wav"
      ],
      "id": "revolutionary-friendship",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intelligent-wildlife"
      },
      "source": [
        "### すべてのモデルを組み合わせて音声波形の生成"
      ],
      "id": "intelligent-wildlife"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "proved-cylinder"
      },
      "source": [
        "from ttslearn.util import find_lab, find_feats\n",
        "from ttslearn.dsp import logmelspectrogram_to_audio\n",
        "\n",
        "# WaveNetボコーダの代わりにGriffin-Limのアルゴリズムを利用する場合、以下をTrueにしてください。\n",
        "griffin_lim = False\n",
        "\n",
        "labels = hts.load(find_lab(\"downloads/jsut-label/\", test_utt))\n",
        "in_feats = text_to_sequence(pp_symbols(labels.contexts))\n",
        "in_feats = torch.tensor(in_feats, dtype=torch.long).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, out_feats, _, _ = acoustic_model.inference(in_feats)\n",
        "    \n",
        "if griffin_lim:\n",
        "    # Griffin-Lim のアルゴリズムに基づく音声波形生成\n",
        "    out_feats = out_feats.cpu().data.numpy()\n",
        "    # 正規化の逆変換\n",
        "    logmel = acoustic_out_scaler.inverse_transform(out_feats)\n",
        "    gen_wav = logmelspectrogram_to_audio(logmel, sr)\n",
        "else:\n",
        "    # WaveNet ボコーダによる音声波形の生成\n",
        "    gen_wav = gen_waveform(wavenet_model, out_feats)"
      ],
      "id": "proved-cylinder",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "south-aquarium"
      },
      "source": [
        "# 比較用に元音声の読み込み\n",
        "from scipy.io import wavfile\n",
        "_sr, ref_wav = wavfile.read(f\"./downloads/jsut_ver1.1/basic5000/wav/{test_utt}.wav\")\n",
        "ref_wav = (ref_wav / 32768.0).astype(np.float64)\n",
        "ref_wav = librosa.resample(ref_wav, _sr, sr)"
      ],
      "id": "south-aquarium",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fundamental-maryland"
      },
      "source": [
        "fig, ax = plt.subplots(2, 1, figsize=(8,6))\n",
        "\n",
        "hop_length = int(sr * 0.005)\n",
        "fft_size = pyworld.get_cheaptrick_fft_size(sr)\n",
        "\n",
        "# Tacotronの出力と粗く揃えるために、自然音声の冒頭と末尾の無音区間を削除\n",
        "ref_wav_trim = librosa.effects.trim(ref_wav, top_db=20)[0]\n",
        "\n",
        "spec_ref = librosa.stft(ref_wav_trim, n_fft=fft_size, hop_length=hop_length, window=\"hann\")\n",
        "logspec_ref = np.log(np.abs(spec_ref))\n",
        "spec_gen = librosa.stft(gen_wav, n_fft=fft_size, hop_length=hop_length, window=\"hann\")\n",
        "logspec_gen = np.log(np.abs(spec_gen))\n",
        "\n",
        "mindb = min(logspec_ref.min(), logspec_gen.min())\n",
        "maxdb = max(logspec_ref.max(), logspec_gen.max())\n",
        "\n",
        "mesh = librosa.display.specshow(logspec_ref, hop_length=hop_length, sr=sr, cmap=cmap, x_axis=\"time\", y_axis=\"hz\", ax=ax[0])\n",
        "mesh.set_clim(mindb, maxdb)\n",
        "fig.colorbar(mesh, ax=ax[0], format=\"%+2.fdB\")\n",
        "\n",
        "mesh = librosa.display.specshow(logspec_gen, hop_length=hop_length, sr=sr, cmap=cmap, x_axis=\"time\", y_axis=\"hz\", ax=ax[1])\n",
        "mesh.set_clim(mindb, maxdb)\n",
        "fig.colorbar(mesh, ax=ax[1], format=\"%+2.fdB\")\n",
        "\n",
        "ax[0].set_title(\"Spectrogram of natural speech\")\n",
        "ax[1].set_title(\"Spectrogram of generated speech\")\n",
        "\n",
        "for a in ax:\n",
        "    a.set_xlabel(\"Time [sec]\")\n",
        "    a.set_ylabel(\"Frequency [Hz]\")\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "print(\"自然音声\")\n",
        "IPython.display.display(Audio(ref_wav_trim, rate=sr))\n",
        "print(\"Tacotron 2による合成音声\")\n",
        "IPython.display.display(Audio(gen_wav, rate=sr))\n",
        "\n",
        "# 図10-9\n",
        "savefig(\"./fig/e2etts_impl_tts_spec_comp\")"
      ],
      "id": "fundamental-maryland",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exotic-parliament"
      },
      "source": [
        "### 合成音声のより詳細な比較 (bonus)"
      ],
      "id": "exotic-parliament"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meaning-reserve"
      },
      "source": [
        "# 比較用に、自然音声から抽出されたメルスペクトログラムから音声波形の生成を行います\n",
        "feats = np.load(find_feats(\"dump/jsut_sr16000/norm/\", test_utt, typ=\"out_tacotron\"))\n",
        "feats = torch.from_numpy(feats)\n",
        "gen_wav_wn_gt = gen_waveform(wavenet_model, feats)"
      ],
      "id": "meaning-reserve",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "specific-given"
      },
      "source": [
        "ref_wav_inv = np.load(find_feats(\"./dump/jsut_sr16000/org/\", test_utt, typ=\"out_wavenet\"))\n",
        "ref_wav_inv = inv_mulaw_quantize(ref_wav_inv, 255)"
      ],
      "id": "specific-given",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "retained-yorkshire"
      },
      "source": [
        "print(\"自然音声\")\n",
        "IPython.display.display(Audio(ref_wav, rate=sr))\n",
        "print(\"自然音声 (8-bit mu-law)\")\n",
        "IPython.display.display(Audio(ref_wav_inv, rate=sr))\n",
        "print(\"WaveNetボコーダの出力\")\n",
        "IPython.display.display(Audio(gen_wav_wn_gt, rate=sr))\n",
        "print(\"Tacotron + WaveNetボコーダの出力\")\n",
        "IPython.display.display(Audio(gen_wav, rate=sr))"
      ],
      "id": "retained-yorkshire",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naval-medication"
      },
      "source": [
        "### 評価データに対して音声波形生成"
      ],
      "id": "naval-medication"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "large-greenhouse"
      },
      "source": [
        "#### レシピの stage 5 の実行"
      ],
      "id": "large-greenhouse"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "desperate-darkness"
      },
      "source": [
        "if run_sh:\n",
        "    ! ./run.sh --stage 5 --stop-stage 5 --acoustic-model $acoustic_config_name \\\n",
        "        --tqdm $run_sh_tqdm --wavenet-model $wavenet_config_name \\\n",
        "        --reverse true --num-eval-utts $num_eval_utts"
      ],
      "id": "desperate-darkness",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conceptual-writer"
      },
      "source": [
        "#### レシピの stage 6 の実行"
      ],
      "id": "conceptual-writer"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "resident-harvest"
      },
      "source": [
        "if run_sh and run_stage6:\n",
        "    ! ./run.sh --stage 6 --stop-stage 6 --acoustic-model $acoustic_config_name \\\n",
        "        --tqdm $run_sh_tqdm --wavenet-model $wavenet_config_name \\\n",
        "        --reverse true --num-eval-utts $num_eval_utts"
      ],
      "id": "resident-harvest",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "united-poverty"
      },
      "source": [
        "## 自然音声と合成音声の比較 (bonus)"
      ],
      "id": "united-poverty"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "analyzed-trace"
      },
      "source": [
        "from pathlib import Path\n",
        "from ttslearn.util import load_utt_list\n",
        "\n",
        "with open(\"./downloads/jsut_ver1.1/basic5000/transcript_utf8.txt\") as f:\n",
        "    transcripts = {}\n",
        "    for l in f:\n",
        "        utt_id, script = l.split(\":\")\n",
        "        transcripts[utt_id] = script\n",
        "        \n",
        "eval_list = load_utt_list(\"data/eval.list\")[::-1][:5]\n",
        "\n",
        "for utt_id in eval_list:\n",
        "    # ref file \n",
        "    ref_file = f\"./downloads/jsut_ver1.1/basic5000/wav/{utt_id}.wav\"\n",
        "    _sr, ref_wav = wavfile.read(ref_file)\n",
        "    ref_wav = (ref_wav / 32768.0).astype(np.float64)\n",
        "    ref_wav = librosa.resample(ref_wav, _sr, sr)\n",
        "  \n",
        "    print(f\"{utt_id}: {transcripts[utt_id]}\")\n",
        "    print(\"自然音声\")\n",
        "    IPython.display.display(Audio(ref_wav, rate=sr))\n",
        "\n",
        "    gen_file = f\"exp/jsut_sr16000/synthesis_{acoustic_config_name}_griffin_lim/eval/{utt_id}.wav\"\n",
        "    if exists(gen_file):\n",
        "        _sr, gen_wav = wavfile.read(gen_file)\n",
        "        print(\"Tacotron + Griffin-Lim\")\n",
        "        IPython.display.display(Audio(gen_wav, rate=sr))\n",
        "    else:\n",
        "        print(\"Tacotron + Griffin-Lim: not found\")\n",
        "\n",
        "    gen_file_wn = f\"exp/jsut_sr16000/synthesis_{acoustic_config_name}_{wavenet_config_name}/eval/{utt_id}.wav\"\n",
        "    if exists(gen_file_wn):\n",
        "        _sr, gen_wav_wn = wavfile.read(gen_file_wn)\n",
        "        print(\"Tacotron + WaveNetボコーダ\")\n",
        "        IPython.display.display(Audio(gen_wav_wn, rate=sr))\n",
        "    else:\n",
        "        print(\"Tacotron + WaveNetボコーダ: not found\")        "
      ],
      "id": "analyzed-trace",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "proof-labor"
      },
      "source": [
        "## 学習済みモデルのパッケージング (bonus)\n",
        "\n",
        "学習済みモデルを利用したTTSに必要なファイルをすべて単一のディレクトリにまとめます。\n",
        "`ttslearn.tacotron.Tacotron2TTS` クラスには、まとめたディレクトリを指定し、TTSを行う機能が実装されています。"
      ],
      "id": "proof-labor"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "friendly-contribution"
      },
      "source": [
        "### レシピの stage 99 の実行"
      ],
      "id": "friendly-contribution"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stainless-history"
      },
      "source": [
        "if run_sh:\n",
        "    ! ./run.sh --stage 99 --stop-stage 99 --acoustic-model $acoustic_config_name \\\n",
        "        --wavenet-model $wavenet_config_name"
      ],
      "id": "stainless-history",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "broken-karen"
      },
      "source": [
        "!ls tts_models/jsut_sr16000_{acoustic_config_name}_{wavenet_config_name}"
      ],
      "id": "broken-karen",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "searching-provider"
      },
      "source": [
        "### パッケージングしたモデルを利用したTTS"
      ],
      "id": "searching-provider"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "postal-stationery"
      },
      "source": [
        "from ttslearn.tacotron import Tacotron2TTS\n",
        "\n",
        "# パッケージングしたモデルのパスを指定します\n",
        "engine = Tacotron2TTS(\n",
        "    model_dir=f\"./tts_models/jsut_sr16000_{acoustic_config_name}_{wavenet_config_name}\"\n",
        ")\n",
        "wav, sr = engine.tts(\"ここまでお読みいただき、ありがとうございました。\", tqdm=tqdm)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,2))\n",
        "librosa.display.waveplot(wav.astype(np.float32), sr, ax=ax)\n",
        "ax.set_xlabel(\"Time [sec]\")\n",
        "ax.set_ylabel(\"Amplitude\")\n",
        "plt.tight_layout()\n",
        "\n",
        "Audio(wav, rate=sr)"
      ],
      "id": "postal-stationery",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "athletic-aquatic"
      },
      "source": [
        "if is_colab():\n",
        "    from datetime import timedelta\n",
        "    elapsed = (time.time() - start_time)\n",
        "    print(\"所要時間:\", str(timedelta(seconds=elapsed)))"
      ],
      "id": "athletic-aquatic",
      "execution_count": null,
      "outputs": []
    }
  ]
}