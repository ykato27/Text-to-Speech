{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "ch09_Tacotron.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykato27/Text-to-Speech/blob/main/ch09_Tacotron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "celtic-greek"
      },
      "source": [
        "# 第9章 Tacotron 2: 一貫学習を狙った音声合成"
      ],
      "id": "celtic-greek"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "modular-biography"
      },
      "source": [
        "## 準備"
      ],
      "id": "modular-biography"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "configured-cause"
      },
      "source": [
        "### Python version"
      ],
      "id": "configured-cause"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "comfortable-conference",
        "outputId": "8681f0c6-a535-44d2-a431-7f74a61d6f5c"
      },
      "source": [
        "!python -VV"
      ],
      "id": "comfortable-conference",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.11 (default, Jul  3 2021, 18:01:19) \n",
            "[GCC 7.5.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "limited-senator"
      },
      "source": [
        "### ttslearn のインストール"
      ],
      "id": "limited-senator"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "innovative-conservative"
      },
      "source": [
        "%%capture\n",
        "try:\n",
        "    import ttslearn\n",
        "except ImportError:\n",
        "    !pip install ttslearn"
      ],
      "id": "innovative-conservative",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "physical-fleece",
        "outputId": "93948b12-c423-47d9-e6ef-0d29874f0aef"
      },
      "source": [
        "import ttslearn\n",
        "ttslearn.__version__"
      ],
      "id": "physical-fleece",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0.2.1'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "executive-bargain"
      },
      "source": [
        "### パッケージのインポート"
      ],
      "id": "executive-bargain"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "legal-livestock",
        "outputId": "cd2bc992-4761-4a19-a8da-b38f79aa9169"
      },
      "source": [
        "%pylab inline\n",
        "%load_ext autoreload\n",
        "%load_ext tensorboard\n",
        "%autoreload\n",
        "import IPython\n",
        "from IPython.display import Audio\n",
        "import tensorboard as tb\n",
        "import os"
      ],
      "id": "legal-livestock",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hungry-consolidation"
      },
      "source": [
        "# 数値演算\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "# 音声波形の読み込み\n",
        "from scipy.io import wavfile\n",
        "# フルコンテキストラベル、質問ファイルの読み込み\n",
        "from nnmnkwii.io import hts\n",
        "# 音声分析\n",
        "import pyworld\n",
        "# 音声分析、可視化\n",
        "import librosa\n",
        "import librosa.display\n",
        "# Pythonで学ぶ音声合成\n",
        "import ttslearn"
      ],
      "id": "hungry-consolidation",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "constitutional-compound"
      },
      "source": [
        "# シードの固定\n",
        "from ttslearn.util import init_seed\n",
        "init_seed(773)"
      ],
      "id": "constitutional-compound",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "decent-oliver",
        "outputId": "00c22a5c-43eb-4804-8938-8129164bd18a"
      },
      "source": [
        "torch.__version__"
      ],
      "id": "decent-oliver",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.9.0+cu102'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "needed-reader"
      },
      "source": [
        "### 描画周りの設定"
      ],
      "id": "needed-reader"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fundamental-formula"
      },
      "source": [
        "from ttslearn.notebook import get_cmap, init_plot_style, savefig\n",
        "cmap = get_cmap()\n",
        "init_plot_style()"
      ],
      "id": "fundamental-formula",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prescribed-advantage"
      },
      "source": [
        "## 9.3 エンコーダ"
      ],
      "id": "prescribed-advantage"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opened-effort"
      },
      "source": [
        "### 文字列から数値列への変換"
      ],
      "id": "opened-effort"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "palestinian-basement"
      },
      "source": [
        "# 語彙の定義\n",
        "characters = \"abcdefghijklmnopqrstuvwxyz!'(),-.:;? \"\n",
        "# その他特殊記号\n",
        "extra_symbols = [\n",
        "    \"^\",  # 文の先頭を表す特殊記号 <SOS>\n",
        "    \"$\",  # 文の末尾を表す特殊記号 <EOS>\n",
        "]\n",
        "_pad = \"~\"\n",
        "\n",
        "# NOTE: パディングを 0 番目に配置\n",
        "symbols = [_pad] + extra_symbols + list(characters)\n",
        "\n",
        "# 文字列⇔数値の相互変換のための辞書\n",
        "_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n",
        "_id_to_symbol = {i: s for i, s in enumerate(symbols)}"
      ],
      "id": "palestinian-basement",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "future-dialogue",
        "outputId": "e7069bdf-ee57-425c-8369-23f5a374a0fc"
      },
      "source": [
        "len(symbols)"
      ],
      "id": "future-dialogue",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "broad-madison"
      },
      "source": [
        "def text_to_sequence(text):\n",
        "    # 簡易のため、大文字と小文字を区別せず、全ての大文字を小文字に変換\n",
        "    text = text.lower()\n",
        "\n",
        "    # <SOS>\n",
        "    seq = [_symbol_to_id[\"^\"]]\n",
        "\n",
        "    # 本文\n",
        "    seq += [_symbol_to_id[s] for s in text]\n",
        "\n",
        "    # <EOS>\n",
        "    seq.append(_symbol_to_id[\"$\"])\n",
        "\n",
        "    return seq\n",
        "\n",
        "\n",
        "def sequence_to_text(seq):\n",
        "    return [_id_to_symbol[s] for s in seq]"
      ],
      "id": "broad-madison",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "partial-burst",
        "outputId": "85b3bcd7-8083-4048-f738-a4bd1418c027"
      },
      "source": [
        "seq = text_to_sequence(\"Hello!\")\n",
        "print(f\"文字列から数値列への変換: {seq}\")\n",
        "print(f\"数値列から文字列への逆変換: {sequence_to_text(seq)}\")"
      ],
      "id": "partial-burst",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "文字列から数値列への変換: [1, 10, 7, 14, 14, 17, 29, 2]\n",
            "数値列から文字列への逆変換: ['^', 'h', 'e', 'l', 'l', 'o', '!', '$']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "surface-sister"
      },
      "source": [
        "### 文字埋め込み"
      ],
      "id": "surface-sister"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fallen-broadcasting"
      },
      "source": [
        "class SimplestEncoder(nn.Module):\n",
        "    def __init__(self, num_vocab=40, embed_dim=256):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(num_vocab, embed_dim, padding_idx=0)\n",
        "    \n",
        "    def forward(self, seqs):\n",
        "        return self.embed(seqs)"
      ],
      "id": "fallen-broadcasting",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "welcome-theater",
        "outputId": "39050d22-396f-43e5-d83b-8d7cd6e2b715"
      },
      "source": [
        "SimplestEncoder()"
      ],
      "id": "welcome-theater",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimplestEncoder(\n",
              "  (embed): Embedding(40, 256, padding_idx=0)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "comparative-wiring"
      },
      "source": [
        "from ttslearn.util import pad_1d\n",
        "\n",
        "def get_dummy_input():\n",
        "    # バッチサイズに 2 を想定して、適当な文字列を作成\n",
        "    seqs = [\n",
        "        text_to_sequence(\"What is your favorite language?\"),\n",
        "        text_to_sequence(\"Hello world.\"),\n",
        "    ]\n",
        "    in_lens = torch.tensor([len(x) for x in seqs], dtype=torch.long)\n",
        "    max_len = max(len(x) for x in seqs)\n",
        "    seqs = torch.stack([torch.from_numpy(pad_1d(seq, max_len)) for seq in seqs])\n",
        "    \n",
        "    return seqs, in_lens"
      ],
      "id": "comparative-wiring",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "confidential-expert",
        "outputId": "daf04444-6b90-43aa-9baa-a4934cf4a471"
      },
      "source": [
        "seqs, in_lens = get_dummy_input()\n",
        "print(\"入力\", seqs)\n",
        "print(\"系列長:\", in_lens)"
      ],
      "id": "confidential-expert",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "入力 tensor([[ 1, 25, 10,  3, 22, 39, 11, 21, 39, 27, 17, 23, 20, 39,  8,  3, 24, 17,\n",
            "         20, 11, 22,  7, 39, 14,  3, 16,  9, 23,  3,  9,  7, 38,  2],\n",
            "        [ 1, 10,  7, 14, 14, 17, 39, 25, 17, 20, 14,  6, 35,  2,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
            "系列長: tensor([33, 14])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prospective-aggregate",
        "outputId": "e9971985-c10d-469d-cd09-208582cf3258"
      },
      "source": [
        "encoder = SimplestEncoder(num_vocab=40, embed_dim=256)\n",
        "seqs, in_lens = get_dummy_input()\n",
        "encoder_outs = encoder(seqs)\n",
        "print(f\"入力のサイズ: {tuple(seqs.shape)}\")\n",
        "print(f\"出力のサイズ: {tuple(encoder_outs.shape)}\")"
      ],
      "id": "prospective-aggregate",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "入力のサイズ: (2, 33)\n",
            "出力のサイズ: (2, 33, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irish-paintball",
        "outputId": "f31fe188-6fbb-4a84-fea0-82b723236309"
      },
      "source": [
        "# パディングの部分は0を取り、それ以外は連続値で表されます\n",
        "encoder_outs"
      ],
      "id": "irish-paintball",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.7055,  0.6891,  0.0332,  ...,  0.7174,  0.4686,  1.1468],\n",
              "         [-0.1568, -0.3719, -1.0086,  ..., -0.9326, -1.2187, -0.0714],\n",
              "         [-0.1901, -0.1983,  0.2274,  ...,  0.2284,  1.6452, -0.3408],\n",
              "         ...,\n",
              "         [-1.9353,  0.2628, -0.1449,  ...,  1.6056, -0.3912, -0.0740],\n",
              "         [ 0.4687,  0.3258, -0.6565,  ...,  1.0895,  0.9105,  0.2814],\n",
              "         [ 0.8940,  0.3002, -0.2105,  ...,  0.7973,  0.2230, -0.1975]],\n",
              "\n",
              "        [[ 0.7055,  0.6891,  0.0332,  ...,  0.7174,  0.4686,  1.1468],\n",
              "         [-0.1901, -0.1983,  0.2274,  ...,  0.2284,  1.6452, -0.3408],\n",
              "         [-1.9353,  0.2628, -0.1449,  ...,  1.6056, -0.3912, -0.0740],\n",
              "         ...,\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
              "       grad_fn=<EmbeddingBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quality-thanks"
      },
      "source": [
        "### 1次元畳み込みの導入"
      ],
      "id": "quality-thanks"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chubby-commander"
      },
      "source": [
        "class ConvEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_vocab=40,\n",
        "        embed_dim=256,\n",
        "        conv_layers=3,\n",
        "        conv_channels=256,\n",
        "        conv_kernel_size=5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # 文字埋め込み\n",
        "        self.embed = nn.Embedding(num_vocab, embed_dim, padding_idx=0)\n",
        "\n",
        "        # 1次元畳み込みの重ね合わせ：局所的な依存関係のモデル化\n",
        "        self.convs = nn.ModuleList()\n",
        "        for layer in range(conv_layers):\n",
        "            in_channels = embed_dim if layer == 0 else conv_channels\n",
        "            self.convs += [\n",
        "                nn.Conv1d(\n",
        "                    in_channels,\n",
        "                    conv_channels,\n",
        "                    conv_kernel_size,\n",
        "                    padding=(conv_kernel_size - 1) // 2,\n",
        "                    bias=False,\n",
        "                ),\n",
        "                nn.BatchNorm1d(conv_channels),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.5),\n",
        "            ]\n",
        "        self.convs = nn.Sequential(*self.convs)\n",
        "\n",
        "    def forward(self, seqs):\n",
        "        emb = self.embed(seqs)\n",
        "        # 1 次元畳み込みと embedding では、入力のサイズが異なるので注意\n",
        "        out = self.convs(emb.transpose(1, 2)).transpose(1, 2)\n",
        "        return out"
      ],
      "id": "chubby-commander",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "precise-leonard",
        "outputId": "6da3b760-6a86-44bf-efdc-b17085723317"
      },
      "source": [
        "ConvEncoder()"
      ],
      "id": "precise-leonard",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConvEncoder(\n",
              "  (embed): Embedding(40, 256, padding_idx=0)\n",
              "  (convs): Sequential(\n",
              "    (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
              "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
              "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): ReLU()\n",
              "    (7): Dropout(p=0.5, inplace=False)\n",
              "    (8): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
              "    (9): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): ReLU()\n",
              "    (11): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "interim-helmet",
        "outputId": "e285c9cc-c50b-4204-9e7a-3668251b4b51"
      },
      "source": [
        "encoder = ConvEncoder(num_vocab=40, embed_dim=256)\n",
        "seqs, in_lens = get_dummy_input()\n",
        "encoder_outs = encoder(seqs)\n",
        "print(f\"入力のサイズ: {tuple(seqs.shape)}\")\n",
        "print(f\"出力のサイズ: {tuple(encoder_outs.shape)}\")"
      ],
      "id": "interim-helmet",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "入力のサイズ: (2, 33)\n",
            "出力のサイズ: (2, 33, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "understanding-track"
      },
      "source": [
        "### 双方向LSTM の導入"
      ],
      "id": "understanding-track"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "driven-sullivan"
      },
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class Encoder(ConvEncoder):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_vocab=40,\n",
        "        embed_dim=512,\n",
        "        hidden_dim=512,\n",
        "        conv_layers=3,\n",
        "        conv_channels=512,\n",
        "        conv_kernel_size=5,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            num_vocab, embed_dim, conv_layers, conv_channels, conv_kernel_size\n",
        "        )\n",
        "        # 双方向 LSTM による長期依存関係のモデル化\n",
        "        self.blstm = nn.LSTM(\n",
        "            conv_channels, hidden_dim // 2, 1, batch_first=True, bidirectional=True\n",
        "        )\n",
        "\n",
        "    def forward(self, seqs, in_lens):\n",
        "        emb = self.embed(seqs)\n",
        "        # 1 次元畳み込みと embedding では、入力のサイズ が異なるので注意\n",
        "        out = self.convs(emb.transpose(1, 2)).transpose(1, 2)\n",
        "\n",
        "        # 双方向 LSTM の計算\n",
        "        out = pack_padded_sequence(out, in_lens, batch_first=True)\n",
        "        out, _ = self.blstm(out)\n",
        "        out, _ = pad_packed_sequence(out, batch_first=True)\n",
        "        return out"
      ],
      "id": "driven-sullivan",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geological-billion",
        "outputId": "c5a257ec-635a-4ddc-89b4-b389b98b0d43"
      },
      "source": [
        "Encoder()"
      ],
      "id": "geological-billion",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoder(\n",
              "  (embed): Embedding(40, 512, padding_idx=0)\n",
              "  (convs): Sequential(\n",
              "    (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
              "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
              "    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): ReLU()\n",
              "    (7): Dropout(p=0.5, inplace=False)\n",
              "    (8): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
              "    (9): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): ReLU()\n",
              "    (11): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (blstm): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "detected-growth",
        "outputId": "c810cbc5-f038-451d-fea3-32099d2f43b7"
      },
      "source": [
        "encoder = Encoder(num_vocab=40, embed_dim=256)\n",
        "seqs, in_lens = get_dummy_input()\n",
        "in_lens, indices = torch.sort(in_lens, dim=0, descending=True)\n",
        "seqs = seqs[indices]\n",
        "\n",
        "encoder_outs = encoder(seqs, in_lens)\n",
        "print(f\"入力のサイズ: {tuple(seqs.shape)}\")\n",
        "print(f\"出力のサイズ: {tuple(encoder_outs.shape)}\")"
      ],
      "id": "detected-growth",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "入力のサイズ: (2, 33)\n",
            "出力のサイズ: (2, 33, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "complicated-female"
      },
      "source": [
        "## 9.4 注意機構"
      ],
      "id": "complicated-female"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "therapeutic-kenya"
      },
      "source": [
        "### 内容依存の注意機構"
      ],
      "id": "therapeutic-kenya"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "realistic-range"
      },
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "# 書籍中の数式に沿って、わかりやすさを重視した実装\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, encoder_dim=512, decoder_dim=1024, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.V = nn.Linear(encoder_dim, hidden_dim)\n",
        "        self.W = nn.Linear(decoder_dim, hidden_dim, bias=False)\n",
        "        # NOTE: 本書の数式通りに実装するなら bias=False ですが、実用上は bias=True としても問題ありません\n",
        "        self.w = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, encoder_out, decoder_state, mask=None):\n",
        "        # 式 (9.11) の計算\n",
        "        erg = self.w(\n",
        "            torch.tanh(self.W(decoder_state).unsqueeze(1) + self.V(encoder_outs))\n",
        "        ).squeeze(-1)\n",
        "\n",
        "        if mask is not None:\n",
        "            erg.masked_fill_(mask, -float(\"inf\"))\n",
        "\n",
        "        attention_weights = F.softmax(erg, dim=1)\n",
        "\n",
        "        # エンコーダ出力の長さ方向に対して重み付き和を取ります\n",
        "        attention_context = torch.sum(\n",
        "            encoder_outs * attention_weights.unsqueeze(-1), dim=1\n",
        "        )\n",
        "\n",
        "        return attention_context, attention_weights"
      ],
      "id": "realistic-range",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "southeast-advisory",
        "outputId": "bb4e110f-36c5-42be-9c2b-dfd1668eb224"
      },
      "source": [
        "BahdanauAttention()"
      ],
      "id": "southeast-advisory",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BahdanauAttention(\n",
              "  (V): Linear(in_features=512, out_features=128, bias=True)\n",
              "  (W): Linear(in_features=1024, out_features=128, bias=False)\n",
              "  (w): Linear(in_features=128, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "defensive-satellite",
        "outputId": "902ec01d-c71a-4e6b-8ced-a7e5f605566c"
      },
      "source": [
        "from ttslearn.util import make_pad_mask\n",
        "\n",
        "mask =  make_pad_mask(in_lens).to(encoder_outs.device)\n",
        "attention = BahdanauAttention()\n",
        "\n",
        "decoder_input = torch.ones(len(seqs), 1024)\n",
        "\n",
        "attention_context, attention_weights = attention(encoder_outs, decoder_input, mask)\n",
        "\n",
        "print(f\"エンコーダの出力のサイズ: {tuple(encoder_outs.shape)}\")\n",
        "print(f\"デコーダの隠れ状態のサイズ: {tuple(decoder_input.shape)}\")\n",
        "print(f\"コンテキストベクトルのサイズ: {tuple(attention_context.shape)}\")\n",
        "print(f\"アテンション重みのサイズ: {tuple(attention_weights.shape)}\")"
      ],
      "id": "defensive-satellite",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "エンコーダの出力のサイズ: (2, 33, 512)\n",
            "デコーダの隠れ状態のサイズ: (2, 1024)\n",
            "コンテキストベクトルのサイズ: (2, 512)\n",
            "アテンション重みのサイズ: (2, 33)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "played-bacteria"
      },
      "source": [
        "### ハイブリッド注意機構"
      ],
      "id": "played-bacteria"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "empirical-storm"
      },
      "source": [
        "class LocationSensitiveAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder_dim=512,\n",
        "        decoder_dim=1024,\n",
        "        hidden_dim=128,\n",
        "        conv_channels=32,\n",
        "        conv_kernel_size=31,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.V = nn.Linear(encoder_dim, hidden_dim)\n",
        "        self.W = nn.Linear(decoder_dim, hidden_dim, bias=False)\n",
        "        self.U = nn.Linear(conv_channels, hidden_dim, bias=False)\n",
        "        self.F = nn.Conv1d(\n",
        "            1,\n",
        "            conv_channels,\n",
        "            conv_kernel_size,\n",
        "            padding=(conv_kernel_size - 1) // 2,\n",
        "            bias=False,\n",
        "        )\n",
        "        # NOTE: 本書の数式通りに実装するなら bias=False ですが、実用上は bias=True としても問題ありません\n",
        "        self.w = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, encoder_outs, src_lens, decoder_state, att_prev, mask=None):\n",
        "        # アテンション重みを一様分布で初期化\n",
        "        if att_prev is None:\n",
        "            att_prev = 1.0 - make_pad_mask(src_lens).to(\n",
        "                device=decoder_state.device, dtype=decoder_state.dtype\n",
        "            )\n",
        "            att_prev = att_prev / src_lens.unsqueeze(-1).to(encoder_outs.device)\n",
        "\n",
        "        # (B x T_enc) -> (B x 1 x T_enc) -> (B x conv_channels x T_enc) ->\n",
        "        # (B x T_enc x conv_channels)\n",
        "        f = self.F(att_prev.unsqueeze(1)).transpose(1, 2)\n",
        "\n",
        "        # 式 (9.13) の計算\n",
        "        erg = self.w(\n",
        "            torch.tanh(\n",
        "                self.W(decoder_state).unsqueeze(1) + self.V(encoder_outs) + self.U(f)\n",
        "            )\n",
        "        ).squeeze(-1)\n",
        "\n",
        "        if mask is not None:\n",
        "            erg.masked_fill_(mask, -float(\"inf\"))\n",
        "\n",
        "        attention_weights = F.softmax(erg, dim=1)\n",
        "\n",
        "        # エンコーダ出力の長さ方向に対して重み付き和を取ります\n",
        "        attention_context = torch.sum(\n",
        "            encoder_outs * attention_weights.unsqueeze(-1), dim=1\n",
        "        )\n",
        "\n",
        "        return attention_context, attention_weights"
      ],
      "id": "empirical-storm",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "virtual-walker",
        "outputId": "17650e5f-f99e-4710-c322-f90a2d3e8ec5"
      },
      "source": [
        "LocationSensitiveAttention()"
      ],
      "id": "virtual-walker",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LocationSensitiveAttention(\n",
              "  (V): Linear(in_features=512, out_features=128, bias=True)\n",
              "  (W): Linear(in_features=1024, out_features=128, bias=False)\n",
              "  (U): Linear(in_features=32, out_features=128, bias=False)\n",
              "  (F): Conv1d(1, 32, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)\n",
              "  (w): Linear(in_features=128, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fourth-bulletin",
        "outputId": "37b5d0b3-2e76-43c9-c391-e7c9a1d47821"
      },
      "source": [
        "from ttslearn.util import make_pad_mask\n",
        "\n",
        "mask =  make_pad_mask(in_lens).to(encoder_outs.device)\n",
        "attention = LocationSensitiveAttention()\n",
        "\n",
        "decoder_input = torch.ones(len(seqs), 1024)\n",
        "\n",
        "attention_context, attention_weights = attention(encoder_outs, in_lens, decoder_input, None, mask)\n",
        "\n",
        "print(f\"エンコーダの出力のサイズ: {tuple(encoder_outs.shape)}\")\n",
        "print(f\"デコーダの隠れ状態のサイズ: {tuple(decoder_input.shape)}\")\n",
        "print(f\"コンテキストベクトルのサイズ: {tuple(attention_context.shape)}\")\n",
        "print(f\"アテンション重みのサイズ: {tuple(attention_weights.shape)}\")"
      ],
      "id": "fourth-bulletin",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "エンコーダの出力のサイズ: (2, 33, 512)\n",
            "デコーダの隠れ状態のサイズ: (2, 1024)\n",
            "コンテキストベクトルのサイズ: (2, 512)\n",
            "アテンション重みのサイズ: (2, 33)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peaceful-tackle"
      },
      "source": [
        "## 9.5 デコーダ"
      ],
      "id": "peaceful-tackle"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "formal-facility"
      },
      "source": [
        "### Pre-Net"
      ],
      "id": "formal-facility"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "medieval-specific"
      },
      "source": [
        "class Prenet(nn.Module):\n",
        "    def __init__(self, in_dim, layers=2, hidden_dim=256, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.dropout = dropout\n",
        "        prenet = nn.ModuleList()\n",
        "        for layer in range(layers):\n",
        "            prenet += [\n",
        "                nn.Linear(in_dim if layer == 0 else hidden_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "            ]\n",
        "        self.prenet = nn.Sequential(*prenet)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.prenet:\n",
        "            # 学習時、推論時の両方で Dropout を適用します\n",
        "            x = F.dropout(layer(x), self.dropout, training=True)\n",
        "        return x"
      ],
      "id": "medieval-specific",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thousand-holocaust",
        "outputId": "62930806-9b77-4485-dbdf-73821d5851d8"
      },
      "source": [
        "Prenet(80)"
      ],
      "id": "thousand-holocaust",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Prenet(\n",
              "  (prenet): Sequential(\n",
              "    (0): Linear(in_features=80, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (3): ReLU()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "running-broadcasting",
        "outputId": "0e628167-2a21-4fb9-88f3-fb5d2eab9a89"
      },
      "source": [
        "decoder_input = torch.ones(len(seqs), 80)\n",
        "\n",
        "prenet = Prenet(80)\n",
        "out = prenet(decoder_input)\n",
        "print(f\"デコーダの入力のサイズ: {tuple(decoder_input.shape)}\")\n",
        "print(f\"Pre-Net の出力のサイズ: {tuple(out.shape)}\")"
      ],
      "id": "running-broadcasting",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "デコーダの入力のサイズ: (2, 80)\n",
            "Pre-Net の出力のサイズ: (2, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "second-junction"
      },
      "source": [
        "### 注意機構付きデコーダ"
      ],
      "id": "second-junction"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apparent-consensus"
      },
      "source": [
        "from ttslearn.tacotron.decoder import ZoneOutCell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder_hidden_dim=512,\n",
        "        out_dim=80,\n",
        "        layers=2,\n",
        "        hidden_dim=1024,\n",
        "        prenet_layers=2,\n",
        "        prenet_hidden_dim=256,\n",
        "        prenet_dropout=0.5,\n",
        "        zoneout=0.1,\n",
        "        reduction_factor=1,\n",
        "        attention_hidden_dim=128,\n",
        "        attention_conv_channels=32,\n",
        "        attention_conv_kernel_size=31,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.out_dim = out_dim\n",
        "\n",
        "        # 注意機構\n",
        "        self.attention = LocationSensitiveAttention(\n",
        "            encoder_hidden_dim,\n",
        "            hidden_dim,\n",
        "            attention_hidden_dim,\n",
        "            attention_conv_channels,\n",
        "            attention_conv_kernel_size,\n",
        "        )\n",
        "        self.reduction_factor = reduction_factor\n",
        "\n",
        "        # Prenet\n",
        "        self.prenet = Prenet(out_dim, prenet_layers, prenet_hidden_dim, prenet_dropout)\n",
        "\n",
        "        # 片方向LSTM\n",
        "        self.lstm = nn.ModuleList()\n",
        "        for layer in range(layers):\n",
        "            lstm = nn.LSTMCell(\n",
        "                encoder_hidden_dim + prenet_hidden_dim if layer == 0 else hidden_dim,\n",
        "                hidden_dim,\n",
        "            )\n",
        "            lstm = ZoneOutCell(lstm, zoneout)\n",
        "            self.lstm += [lstm]\n",
        "\n",
        "        # 出力への projection 層\n",
        "        proj_in_dim = encoder_hidden_dim + hidden_dim\n",
        "        self.feat_out = nn.Linear(proj_in_dim, out_dim * reduction_factor, bias=False)\n",
        "        self.prob_out = nn.Linear(proj_in_dim, reduction_factor)\n",
        "\n",
        "    def _zero_state(self, hs):\n",
        "        init_hs = hs.new_zeros(hs.size(0), self.lstm[0].hidden_size)\n",
        "        return init_hs\n",
        "\n",
        "    def forward(self, encoder_outs, in_lens, decoder_targets=None):\n",
        "        is_inference = decoder_targets is None\n",
        "\n",
        "        # Reduction factor に基づくフレーム数の調整\n",
        "        # (B, Lmax, out_dim) ->  (B, Lmax/r, out_dim)\n",
        "        if self.reduction_factor > 1 and not is_inference:\n",
        "            decoder_targets = decoder_targets[\n",
        "                :, self.reduction_factor - 1 :: self.reduction_factor\n",
        "            ]\n",
        "\n",
        "        # デコーダの系列長を保持\n",
        "        # 推論時は、エンコーダの系列長から経験的に上限を定める\n",
        "        if is_inference:\n",
        "            max_decoder_time_steps = int(encoder_outs.shape[1] * 10.0)\n",
        "        else:\n",
        "            max_decoder_time_steps = decoder_targets.shape[1]\n",
        "\n",
        "        # ゼロパディングされた部分に対するマスク\n",
        "        mask = make_pad_mask(in_lens).to(encoder_outs.device)\n",
        "\n",
        "        # LSTM の状態をゼロで初期化\n",
        "        h_list, c_list = [], []\n",
        "        for _ in range(len(self.lstm)):\n",
        "            h_list.append(self._zero_state(encoder_outs))\n",
        "            c_list.append(self._zero_state(encoder_outs))\n",
        "\n",
        "        # デコーダの最初の入力\n",
        "        go_frame = encoder_outs.new_zeros(encoder_outs.size(0), self.out_dim)\n",
        "        prev_out = go_frame\n",
        "\n",
        "        # 1つ前の時刻のアテンション重み\n",
        "        prev_att_w = None\n",
        "\n",
        "        # メインループ\n",
        "        outs, logits, att_ws = [], [], []\n",
        "        t = 0\n",
        "        while True:\n",
        "            # コンテキストベクトル、アテンション重みの計算\n",
        "            att_c, att_w = self.attention(\n",
        "                encoder_outs, in_lens, h_list[0], prev_att_w, mask\n",
        "            )\n",
        "\n",
        "            # Pre-Net\n",
        "            prenet_out = self.prenet(prev_out)\n",
        "\n",
        "            # LSTM\n",
        "            xs = torch.cat([att_c, prenet_out], dim=1)\n",
        "            h_list[0], c_list[0] = self.lstm[0](xs, (h_list[0], c_list[0]))\n",
        "            for i in range(1, len(self.lstm)):\n",
        "                h_list[i], c_list[i] = self.lstm[i](\n",
        "                    h_list[i - 1], (h_list[i], c_list[i])\n",
        "                )\n",
        "            # 出力の計算\n",
        "            hcs = torch.cat([h_list[-1], att_c], dim=1)\n",
        "            outs.append(self.feat_out(hcs).view(encoder_outs.size(0), self.out_dim, -1))\n",
        "            logits.append(self.prob_out(hcs))\n",
        "            att_ws.append(att_w)\n",
        "\n",
        "            # 次の時刻のデコーダの入力を更新\n",
        "            if is_inference:\n",
        "                prev_out = outs[-1][:, :, -1]  # (1, out_dim)\n",
        "            else:\n",
        "                # Teacher forcing\n",
        "                prev_out = decoder_targets[:, t, :]\n",
        "\n",
        "            # 累積アテンション重み\n",
        "            prev_att_w = att_w if prev_att_w is None else prev_att_w + att_w\n",
        "\n",
        "            t += 1\n",
        "            # 停止条件のチェック\n",
        "            if t >= max_decoder_time_steps:\n",
        "                break\n",
        "            if is_inference and (torch.sigmoid(logits[-1]) >= 0.5).any():\n",
        "                break\n",
        "                \n",
        "        # 各時刻の出力を結合\n",
        "        logits = torch.cat(logits, dim=1)  # (B, Lmax)\n",
        "        outs = torch.cat(outs, dim=2)  # (B, out_dim, Lmax)\n",
        "        att_ws = torch.stack(att_ws, dim=1)  # (B, Lmax, Tmax)\n",
        "\n",
        "        if self.reduction_factor > 1:\n",
        "            outs = outs.view(outs.size(0), self.out_dim, -1)  # (B, out_dim, Lmax)\n",
        "\n",
        "        return outs, logits, att_ws"
      ],
      "id": "apparent-consensus",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "integrated-lesbian",
        "outputId": "9b4594c2-86ef-41f3-8de6-fcafd2f36c3b"
      },
      "source": [
        "Decoder()"
      ],
      "id": "integrated-lesbian",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder(\n",
              "  (attention): LocationSensitiveAttention(\n",
              "    (V): Linear(in_features=512, out_features=128, bias=True)\n",
              "    (W): Linear(in_features=1024, out_features=128, bias=False)\n",
              "    (U): Linear(in_features=32, out_features=128, bias=False)\n",
              "    (F): Conv1d(1, 32, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)\n",
              "    (w): Linear(in_features=128, out_features=1, bias=True)\n",
              "  )\n",
              "  (prenet): Prenet(\n",
              "    (prenet): Sequential(\n",
              "      (0): Linear(in_features=80, out_features=256, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (3): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (lstm): ModuleList(\n",
              "    (0): ZoneOutCell(\n",
              "      (cell): LSTMCell(768, 1024)\n",
              "    )\n",
              "    (1): ZoneOutCell(\n",
              "      (cell): LSTMCell(1024, 1024)\n",
              "    )\n",
              "  )\n",
              "  (feat_out): Linear(in_features=1536, out_features=80, bias=False)\n",
              "  (prob_out): Linear(in_features=1536, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pharmaceutical-reasoning",
        "outputId": "2067ee25-d3f3-437f-8933-ded67c422d56"
      },
      "source": [
        "decoder_targets = torch.ones(encoder_outs.shape[0], 120, 80)\n",
        "decoder = Decoder(encoder_outs.shape[-1], 80)\n",
        "\n",
        "# Teaccher forcing: decoder_targets (教師データ) を与える\n",
        "with torch.no_grad():\n",
        "    outs, logits, att_ws = decoder(encoder_outs, in_lens, decoder_targets);\n",
        "\n",
        "print(f\"デコーダの入力のサイズ: {tuple(decoder_input.shape)}\")\n",
        "print(f\"デコーダの出力のサイズ: {tuple(outs.shape)}\")\n",
        "print(f\"stop token (logits) のサイズ: {tuple(logits.shape)}\")\n",
        "print(f\"アテンション重みのサイズ: {tuple(att_ws.shape)}\")"
      ],
      "id": "pharmaceutical-reasoning",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "デコーダの入力のサイズ: (2, 80)\n",
            "デコーダの出力のサイズ: (2, 80, 120)\n",
            "stop token (logits) のサイズ: (2, 120)\n",
            "アテンション重みのサイズ: (2, 120, 33)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "organic-roller"
      },
      "source": [
        "# 自己回帰に基づく推論\n",
        "with torch.no_grad():\n",
        "    decoder(encoder_outs[0], torch.tensor([in_lens[0]]))"
      ],
      "id": "organic-roller",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "handled-pixel"
      },
      "source": [
        "## 9.6 Post-Net"
      ],
      "id": "handled-pixel"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "further-apparel"
      },
      "source": [
        "class Postnet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_dim=80,\n",
        "        layers=5,\n",
        "        channels=512,\n",
        "        kernel_size=5,\n",
        "        dropout=0.5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        postnet = nn.ModuleList()\n",
        "        for layer in range(layers):\n",
        "            in_channels = in_dim if layer == 0 else channels\n",
        "            out_channels = in_dim if layer == layers - 1 else channels\n",
        "            postnet += [\n",
        "                nn.Conv1d(\n",
        "                    in_channels,\n",
        "                    out_channels,\n",
        "                    kernel_size,\n",
        "                    stride=1,\n",
        "                    padding=(kernel_size - 1) // 2,\n",
        "                    bias=False,\n",
        "                ),\n",
        "                nn.BatchNorm1d(out_channels),\n",
        "            ]\n",
        "            if layer != layers - 1:\n",
        "                postnet += [nn.Tanh()]\n",
        "            postnet += [nn.Dropout(dropout)]\n",
        "        self.postnet = nn.Sequential(*postnet)\n",
        "\n",
        "    def forward(self, xs):\n",
        "        return self.postnet(xs)"
      ],
      "id": "further-apparel",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sunrise-thumb",
        "outputId": "23663144-f3ee-4586-c9b9-3a65be4f6f2b"
      },
      "source": [
        "Postnet()"
      ],
      "id": "sunrise-thumb",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Postnet(\n",
              "  (postnet): Sequential(\n",
              "    (0): Conv1d(80, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
              "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): Tanh()\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
              "    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): Tanh()\n",
              "    (7): Dropout(p=0.5, inplace=False)\n",
              "    (8): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
              "    (9): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): Tanh()\n",
              "    (11): Dropout(p=0.5, inplace=False)\n",
              "    (12): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
              "    (13): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (14): Tanh()\n",
              "    (15): Dropout(p=0.5, inplace=False)\n",
              "    (16): Conv1d(512, 80, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
              "    (17): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (18): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mexican-watson",
        "outputId": "262827ed-c652-46c3-c104-2c04af8a80be"
      },
      "source": [
        "postnet = Postnet(80)\n",
        "residual = postnet(outs)\n",
        "\n",
        "print(f\"入力のサイズ: {tuple(outs.shape)}\")\n",
        "print(f\"出力のサイズ: {tuple(residual.shape)}\")"
      ],
      "id": "mexican-watson",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "入力のサイズ: (2, 80, 120)\n",
            "出力のサイズ: (2, 80, 120)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rational-promotion"
      },
      "source": [
        "## 9.7 Tacotron 2 の実装"
      ],
      "id": "rational-promotion"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "considered-lightning"
      },
      "source": [
        "### Tacotron 2 のモデル定義"
      ],
      "id": "considered-lightning"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "center-tyler"
      },
      "source": [
        "class Tacotron2(nn.Module):\n",
        "    def __init__(self\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "        self.postnet = Postnet()\n",
        "\n",
        "    def forward(self, seq, in_lens, decoder_targets):\n",
        "        # エンコーダによるテキストに潜在する表現の獲得\n",
        "        encoder_outs = self.encoder(seq, in_lens)\n",
        "\n",
        "        # デコーダによるメルスペクトログラム、stop token の予測\n",
        "        outs, logits, att_ws = self.decoder(encoder_outs, in_lens, decoder_targets)\n",
        "\n",
        "        # Post-Net によるメルスペクトログラムの残差の予測\n",
        "        outs_fine = outs + self.postnet(outs)\n",
        "\n",
        "        # (B, C, T) -> (B, T, C)\n",
        "        outs = outs.transpose(2, 1)\n",
        "        outs_fine = outs_fine.transpose(2, 1)\n",
        "\n",
        "        return outs, outs_fine, logits, att_ws\n",
        "    \n",
        "    def inference(self, seq):\n",
        "        seq = seq.unsqueeze(0) if len(seq.shape) == 1 else seq\n",
        "        in_lens = torch.tensor([seq.shape[-1]], dtype=torch.long, device=seq.device)\n",
        "\n",
        "        return self.forward(seq, in_lens, None)"
      ],
      "id": "center-tyler",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "united-rating",
        "outputId": "989f896a-4fdf-410a-d65c-2de3e4f277f4"
      },
      "source": [
        "seqs, in_lens = get_dummy_input()\n",
        "model = Tacotron2()\n",
        "\n",
        "# Tacotron 2 の計算\n",
        "outs, outs_fine, logits, att_ws = model(seqs, in_lens, decoder_targets)\n",
        "\n",
        "print(f\"入力のサイズ: {tuple(seqs.shape)}\")\n",
        "print(f\"デコーダの出力のサイズ: {tuple(outs.shape)}\")\n",
        "print(f\"Post-Netの出力のサイズ: {tuple(outs_fine.shape)}\")\n",
        "print(f\"stop token (logits) のサイズ: {tuple(logits.shape)}\")\n",
        "print(f\"アテンション重みのサイズ: {tuple(att_ws.shape)}\")"
      ],
      "id": "united-rating",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "入力のサイズ: (2, 33)\n",
            "デコーダの出力のサイズ: (2, 120, 80)\n",
            "Post-Netの出力のサイズ: (2, 120, 80)\n",
            "stop token (logits) のサイズ: (2, 120)\n",
            "アテンション重みのサイズ: (2, 120, 33)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "resistant-nevada",
        "outputId": "ef5dd8c3-0702-4b8c-ee62-cdb86d1bb78d"
      },
      "source": [
        "model"
      ],
      "id": "resistant-nevada",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tacotron2(\n",
              "  (encoder): Encoder(\n",
              "    (embed): Embedding(40, 512, padding_idx=0)\n",
              "    (convs): Sequential(\n",
              "      (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
              "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "      (3): Dropout(p=0.5, inplace=False)\n",
              "      (4): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
              "      (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (6): ReLU()\n",
              "      (7): Dropout(p=0.5, inplace=False)\n",
              "      (8): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
              "      (9): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (10): ReLU()\n",
              "      (11): Dropout(p=0.5, inplace=False)\n",
              "    )\n",
              "    (blstm): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): LocationSensitiveAttention(\n",
              "      (V): Linear(in_features=512, out_features=128, bias=True)\n",
              "      (W): Linear(in_features=1024, out_features=128, bias=False)\n",
              "      (U): Linear(in_features=32, out_features=128, bias=False)\n",
              "      (F): Conv1d(1, 32, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)\n",
              "      (w): Linear(in_features=128, out_features=1, bias=True)\n",
              "    )\n",
              "    (prenet): Prenet(\n",
              "      (prenet): Sequential(\n",
              "        (0): Linear(in_features=80, out_features=256, bias=True)\n",
              "        (1): ReLU()\n",
              "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (3): ReLU()\n",
              "      )\n",
              "    )\n",
              "    (lstm): ModuleList(\n",
              "      (0): ZoneOutCell(\n",
              "        (cell): LSTMCell(768, 1024)\n",
              "      )\n",
              "      (1): ZoneOutCell(\n",
              "        (cell): LSTMCell(1024, 1024)\n",
              "      )\n",
              "    )\n",
              "    (feat_out): Linear(in_features=1536, out_features=80, bias=False)\n",
              "    (prob_out): Linear(in_features=1536, out_features=1, bias=True)\n",
              "  )\n",
              "  (postnet): Postnet(\n",
              "    (postnet): Sequential(\n",
              "      (0): Conv1d(80, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
              "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): Tanh()\n",
              "      (3): Dropout(p=0.5, inplace=False)\n",
              "      (4): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
              "      (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (6): Tanh()\n",
              "      (7): Dropout(p=0.5, inplace=False)\n",
              "      (8): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
              "      (9): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (10): Tanh()\n",
              "      (11): Dropout(p=0.5, inplace=False)\n",
              "      (12): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
              "      (13): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (14): Tanh()\n",
              "      (15): Dropout(p=0.5, inplace=False)\n",
              "      (16): Conv1d(512, 80, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
              "      (17): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (18): Dropout(p=0.5, inplace=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "affecting-extraction"
      },
      "source": [
        "### トイモデルを利用したTacotron 2の動作確認"
      ],
      "id": "affecting-extraction"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "expired-style"
      },
      "source": [
        "from ttslearn.tacotron import Tacotron2\n",
        "model = Tacotron2(encoder_conv_layers=1, decoder_prenet_layers=1, decoder_layers=1, postnet_layers=1)"
      ],
      "id": "expired-style",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "color-cabinet"
      },
      "source": [
        "def get_dummy_inout():\n",
        "    seqs, in_lens = get_dummy_input()\n",
        "   \n",
        "    # デコーダの出力（メルスペクトログラム）の教師データ\n",
        "    decoder_targets = torch.ones(2, 120, 80)\n",
        "    \n",
        "    # stop token の教師データ\n",
        "    # stop token の予測値は確率ですが、教師データは 二値のラベルです\n",
        "    # 1 は、デコーダの出力が完了したことを表します\n",
        "    stop_tokens = torch.zeros(2, 120)\n",
        "    stop_tokens[:, -1:] = 1.0\n",
        "    \n",
        "    return seqs, in_lens, decoder_targets, stop_tokens"
      ],
      "id": "color-cabinet",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cardiac-grant",
        "outputId": "51f3086c-a0a7-4ded-d2d3-3095006eaa7f"
      },
      "source": [
        "# 適当な入出力を生成\n",
        "seqs, in_lens, decoder_targets, stop_tokens = get_dummy_inout()\n",
        "\n",
        "# Tacotron 2 の出力を計算\n",
        "# NOTE: teacher-forcing のため、 decoder targets を明示的に与える\n",
        "outs, outs_fine, logits, att_ws = model(seqs, in_lens, decoder_targets)\n",
        "\n",
        "print(\"入力のサイズ:\", tuple(seqs.shape))\n",
        "print(\"デコーダの出力のサイズ:\", tuple(outs.shape))\n",
        "print(\"Stop token のサイズ:\", tuple(logits.shape))\n",
        "print(\"アテンション重みのサイズ:\", tuple(att_ws.shape))"
      ],
      "id": "cardiac-grant",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "入力のサイズ: (2, 33)\n",
            "デコーダの出力のサイズ: (2, 120, 80)\n",
            "Stop token のサイズ: (2, 120)\n",
            "アテンション重みのサイズ: (2, 120, 33)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "decreased-commercial"
      },
      "source": [
        "### Tacotron 2の損失関数の計算"
      ],
      "id": "decreased-commercial"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "communist-republican"
      },
      "source": [
        "# 1. デコーダの出力に対する損失\n",
        "out_loss = nn.MSELoss()(outs, decoder_targets)\n",
        "# 2. Post-Net のあとの出力に対する損失\n",
        "out_fine_loss = nn.MSELoss()(outs_fine, decoder_targets)\n",
        "# 3. Stop token に対する損失\n",
        "stop_token_loss = nn.BCEWithLogitsLoss()(logits, stop_tokens)"
      ],
      "id": "communist-republican",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "individual-motor",
        "outputId": "6887030e-d849-448f-b085-1ffb1da75d98"
      },
      "source": [
        "print(\"out_loss: \", out_loss.item())\n",
        "print(\"out_fine_loss: \", out_fine_loss.item())\n",
        "print(\"stop_token_loss: \", stop_token_loss.item())"
      ],
      "id": "individual-motor",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out_loss:  1.0082790851593018\n",
            "out_fine_loss:  2.966367244720459\n",
            "stop_token_loss:  0.7374292016029358\n"
          ]
        }
      ]
    }
  ]
}