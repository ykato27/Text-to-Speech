{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "toc-autonumbering": false,
    "toc-showcode": false,
    "colab": {
      "name": "ch06_Recipe-DNNTTS.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "whole-rehabilitation",
        "square-damage"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykato27/Text-to-Speech/blob/main/ch06_Recipe_DNNTTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "civic-merchandise"
      },
      "source": [
        "# 第6章: 日本語 DNN 音声合成システムの実装\n",
        "\n",
        "Google colabでの実行における推定所要時間: 1時間\n",
        "\n",
        "このノートブックに記載のレシピの設定は、Google Colab上で実行した場合のタイムアウトを避けるため、学習条件を書籍に記載の設定から一部修正していることに注意してください (バッチサイズを減らす等)。\n",
        "参考までに、書籍に記載の条件で、著者 (山本) がレシピを実行した結果を以下で公開しています。\n",
        "\n",
        "- Tensorboard logs: https://tensorboard.dev/experiment/ajmqiymoTx6rADKLF8d6sA/\n",
        "- expディレクトリ(学習済みモデル、合成音声を含む) : https://drive.google.com/file/d/1p8xj9wiX3TRtkjw_swayePXezxmu_UnG/view?usp=sharing (12.8 MB)"
      ],
      "id": "civic-merchandise"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chicken-religion"
      },
      "source": [
        "## 準備"
      ],
      "id": "chicken-religion"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "signal-murray"
      },
      "source": [
        "### Google Colabを利用する場合"
      ],
      "id": "signal-murray"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "respected-instrumentation"
      },
      "source": [
        "Google Colab上でこのノートブックを実行する場合は、メニューの「ランタイム -> ランタイムのタイムの変更」から、「ハードウェア アクセラレータ」を **GPU** に変更してください。"
      ],
      "id": "respected-instrumentation"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "animated-premium"
      },
      "source": [
        "### Python version"
      ],
      "id": "animated-premium"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "focal-horse"
      },
      "source": [
        "!python -VV"
      ],
      "id": "focal-horse",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "republican-ownership"
      },
      "source": [
        "### ttslearn のインストール"
      ],
      "id": "republican-ownership"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "average-spanking"
      },
      "source": [
        "%%capture\n",
        "try:\n",
        "    import ttslearn\n",
        "except ImportError:\n",
        "    !pip install ttslearn"
      ],
      "id": "average-spanking",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruled-rapid"
      },
      "source": [
        "import ttslearn\n",
        "ttslearn.__version__"
      ],
      "id": "ruled-rapid",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lovely-coaching"
      },
      "source": [
        "## 6.1 本章の日本語音声合成システムの実装"
      ],
      "id": "lovely-coaching"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "compact-champagne"
      },
      "source": [
        "### 学習済みモデルを用いた音声合成"
      ],
      "id": "compact-champagne"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "demographic-posting"
      },
      "source": [
        "from ttslearn.dnntts import DNNTTS\n",
        "from IPython.display import Audio\n",
        "\n",
        "engine = DNNTTS()\n",
        "wav, sr = engine.tts(\"深層学習に基づく音声合成システムです。\")\n",
        "Audio(wav, rate=sr)"
      ],
      "id": "demographic-posting",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chief-ivory"
      },
      "source": [
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,2))\n",
        "librosa.display.waveplot(wav.astype(np.float32), sr, ax=ax)\n",
        "ax.set_xlabel(\"Time [sec]\")\n",
        "ax.set_ylabel(\"Amplitude\")\n",
        "plt.tight_layout()"
      ],
      "id": "chief-ivory",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "organized-relations"
      },
      "source": [
        "### レシピ実行の前準備"
      ],
      "id": "organized-relations"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "driving-soccer"
      },
      "source": [
        "%%capture\n",
        "from ttslearn.env import is_colab\n",
        "from os.path import exists\n",
        "\n",
        "# pip install ttslearn ではレシピはインストールされないので、手動でダウンロード\n",
        "if is_colab() and not exists(\"recipes.zip\"):\n",
        "    !curl -LO https://github.com/r9y9/ttslearn/releases/download/v{ttslearn.__version__}/recipes.zip\n",
        "    !unzip -o recipes.zip"
      ],
      "id": "driving-soccer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "impaired-mount"
      },
      "source": [
        "import os\n",
        "# recipeのディレクトリに移動\n",
        "cwd = os.getcwd()\n",
        "if cwd.endswith(\"notebooks\"):\n",
        "    os.chdir(\"../recipes/dnntts/\")\n",
        "elif is_colab():\n",
        "    os.chdir(\"recipes/dnntts/\")"
      ],
      "id": "impaired-mount",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "numerical-daily"
      },
      "source": [
        "import time\n",
        "start_time = time.time()"
      ],
      "id": "numerical-daily",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "given-interaction"
      },
      "source": [
        "### パッケージのインポート"
      ],
      "id": "given-interaction"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "friendly-flavor"
      },
      "source": [
        "%pylab inline\n",
        "%load_ext autoreload\n",
        "%load_ext tensorboard\n",
        "%autoreload\n",
        "import IPython\n",
        "from IPython.display import Audio\n",
        "import tensorboard as tb\n",
        "import os"
      ],
      "id": "friendly-flavor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "electric-journey"
      },
      "source": [
        "# 数値演算\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "# 音声波形の読み込み\n",
        "from scipy.io import wavfile\n",
        "# フルコンテキストラベル、質問ファイルの読み込み\n",
        "from nnmnkwii.io import hts\n",
        "# 音声分析\n",
        "import pysptk\n",
        "import pyworld\n",
        "# 音声分析、可視化\n",
        "import librosa\n",
        "import librosa.display\n",
        "import pandas as pd\n",
        "# Pythonで学ぶ音声合成\n",
        "import ttslearn"
      ],
      "id": "electric-journey",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "decreased-control"
      },
      "source": [
        "# シードの固定\n",
        "from ttslearn.util import init_seed\n",
        "init_seed(773)"
      ],
      "id": "decreased-control",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "latter-fireplace"
      },
      "source": [
        "torch.__version__"
      ],
      "id": "latter-fireplace",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "continued-spirituality"
      },
      "source": [
        "### 描画周りの設定"
      ],
      "id": "continued-spirituality"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blond-camera"
      },
      "source": [
        "from ttslearn.notebook import get_cmap, init_plot_style, savefig\n",
        "cmap = get_cmap()\n",
        "init_plot_style()"
      ],
      "id": "blond-camera",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opposite-relevance"
      },
      "source": [
        "### レシピの設定"
      ],
      "id": "opposite-relevance"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "automatic-albania"
      },
      "source": [
        "# run.shを利用した学習スクリプトをnotebookから行いたい場合は、True\n",
        "# google colab の場合は、True とします\n",
        "# ローカル環境の場合、run.sh をターミナルから実行することを推奨します。\n",
        "# その場合、このノートブックは可視化・学習済みモデルのテストのために利用します。\n",
        "run_sh = is_colab()\n",
        "\n",
        "# CUDA\n",
        "# NOTE: run.shの引数として渡すので、boolではなく文字列で定義しています\n",
        "cudnn_benchmark = \"true\"\n",
        "cudnn_deterministic = \"false\"\n",
        "\n",
        "# 特徴抽出時の並列処理のジョブ数\n",
        "n_jobs = os.cpu_count()//2\n",
        "\n",
        "# 継続長モデルの設定ファイル名\n",
        "duration_config_name=\"duration_dnn\"\n",
        "# 音響モデルの設定ファイル名\n",
        "acoustic_config_name=\"acoustic_dnn_sr16k\"\n",
        "\n",
        "# 継続長モデル & 音響モデル学習におけるバッチサイズ\n",
        "batch_size = 32\n",
        "# 継続長モデル & 音響モデル学習におけるエポック数\n",
        "# 注意: 計算時間を少なくするために、やや少なく設定しています。品質を向上させるためには、30 ~ 50 のエポック数を試してみてください。\n",
        "nepochs = 10\n",
        "\n",
        "# run.sh経由で実行するスクリプトのtqdm\n",
        "run_sh_tqdm = \"none\""
      ],
      "id": "automatic-albania",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hollow-extreme"
      },
      "source": [
        "# ノートブックで利用するテスト用の発話（学習データ、評価データ）\n",
        "train_utt = \"BASIC5000_0001\"\n",
        "test_utt = \"BASIC5000_5000\""
      ],
      "id": "hollow-extreme",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sweet-maximum"
      },
      "source": [
        "### Tensorboard によるログの可視化"
      ],
      "id": "sweet-maximum"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "breeding-asset"
      },
      "source": [
        "# ノートブック上から tensorboard のログを確認する場合、次の行を有効にしてください\n",
        "if is_colab():\n",
        "    %tensorboard --logdir tensorboard/"
      ],
      "id": "breeding-asset",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "finite-notion"
      },
      "source": [
        "## 6.2 プログラム実装の前準備"
      ],
      "id": "finite-notion"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ordered-matter"
      },
      "source": [
        "### stage -1: コーパスのダウンロード"
      ],
      "id": "ordered-matter"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "interpreted-baptist"
      },
      "source": [
        "if is_colab():\n",
        "    ! ./run.sh --stage -1 --stop-stage -1"
      ],
      "id": "interpreted-baptist",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dimensional-synthesis"
      },
      "source": [
        "### Stage 0: 学習/検証/評価データの分割"
      ],
      "id": "dimensional-synthesis"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bigger-logging"
      },
      "source": [
        "if run_sh:\n",
        "    ! ./run.sh --stage 0 --stop-stage 0"
      ],
      "id": "bigger-logging",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cardiovascular-vocabulary"
      },
      "source": [
        "! ls data/"
      ],
      "id": "cardiovascular-vocabulary",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "facial-active"
      },
      "source": [
        "! head data/dev.list"
      ],
      "id": "facial-active",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "higher-trail"
      },
      "source": [
        "## 6.3 継続長モデルのための前処理"
      ],
      "id": "higher-trail"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heard-avatar"
      },
      "source": [
        "### 継続長モデルのための1発話に対する前処理"
      ],
      "id": "heard-avatar"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "documentary-encyclopedia"
      },
      "source": [
        "import ttslearn\n",
        "from nnmnkwii.io import hts\n",
        "from nnmnkwii.frontend import merlin as fe\n",
        "\n",
        "# 言語特徴量の抽出に使うための質問ファイル\n",
        "binary_dict, numeric_dict = hts.load_question_set(ttslearn.util.example_qst_file())\n",
        "\n",
        "# 音声のフルコンテキストラベルを読み込む\n",
        "labels = hts.load(ttslearn.util.example_label_file())\n",
        "\n",
        "# 継続長モデルの入力：言語特徴量\n",
        "in_feats = fe.linguistic_features(labels, binary_dict, numeric_dict)\n",
        "\n",
        "# 継続長モデルの出力：音素継続長\n",
        "out_feats = fe.duration_features(labels)"
      ],
      "id": "documentary-encyclopedia",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eight-exemption"
      },
      "source": [
        "print(\"入力特徴量のサイズ:\", in_feats.shape)\n",
        "print(\"出力特徴量のサイズ:\", out_feats.shape)"
      ],
      "id": "eight-exemption",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "supreme-illinois"
      },
      "source": [
        "# 可視化用に正規化\n",
        "in_feats_norm = in_feats / np.maximum(1, np.abs(in_feats).max(0))\n",
        "fig, ax = plt.subplots(2, 1, figsize=(8,6))\n",
        "ax[0].set_title(\"Duration model's input: linguistic features\")\n",
        "ax[1].set_title(\"Duration model's output: phoneme durations\")\n",
        "ax[0].imshow(in_feats_norm.T, aspect=\"auto\", interpolation=\"nearest\", origin=\"lower\", cmap=cmap)\n",
        "ax[0].set_ylabel(\"Context\")\n",
        "\n",
        "ax[1].bar(np.arange(len(out_feats)), out_feats.reshape(-1))\n",
        "\n",
        "for a in ax:\n",
        "    a.set_xlim(-0.5, len(in_feats)-0.5)\n",
        "    a.set_xlabel(\"Phoneme\")\n",
        "ax[1].set_ylabel(\"Duration (the number of frames)\")\n",
        "plt.tight_layout()\n",
        "\n",
        "# 図6-3\n",
        "savefig(\"fig/dnntts_impl_duration_inout\")"
      ],
      "id": "supreme-illinois",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whole-rehabilitation"
      },
      "source": [
        "### レシピの stage 1 の実行\n",
        "\n",
        "バッチ処理を行うコマンドラインプログラムは、 `preprocess_duration.py` を参照してください。"
      ],
      "id": "whole-rehabilitation"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "contrary-rehabilitation"
      },
      "source": [
        "if run_sh:\n",
        "    ! ./run.sh --stage 1 --stop-stage 1 --n-jobs $n_jobs "
      ],
      "id": "contrary-rehabilitation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "determined-representation"
      },
      "source": [
        "## 6.4: 音響モデルのための前処理"
      ],
      "id": "determined-representation"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opposed-interstate"
      },
      "source": [
        "### 音響モデルのための 1 発話に対する前処理"
      ],
      "id": "opposed-interstate"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "constant-authorization"
      },
      "source": [
        "from ttslearn.dsp import world_spss_params\n",
        "\n",
        "# 言語特徴量の抽出に使うための質問ファイル\n",
        "binary_dict, numeric_dict = hts.load_question_set(ttslearn.util.example_qst_file())\n",
        "\n",
        "# 音声のフルコンテキストラベルを読み込む\n",
        "labels = hts.load(ttslearn.util.example_label_file())\n",
        "\n",
        "# 音響モデルの入力：言語特徴量\n",
        "in_feats = fe.linguistic_features(labels, binary_dict, numeric_dict, add_frame_features=True, subphone_features=\"coarse_coding\")\n",
        "\n",
        "# 音声の読み込み\n",
        "_sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
        "sr = 16000\n",
        "x = (x / 32768).astype(np.float64)\n",
        "x = librosa.resample(x, _sr, sr)\n",
        "\n",
        "# 音響モデルの出力：音響特徴量\n",
        "out_feats = world_spss_params(x, sr)\n",
        "\n",
        "# フレーム数の調整\n",
        "minL = min(in_feats.shape[0], out_feats.shape[0])\n",
        "in_feats, out_feats = in_feats[:minL], out_feats[:minL]\n",
        "\n",
        "# 冒頭と末尾の非音声区間の長さを調整\n",
        "assert \"sil\" in labels.contexts[0] and \"sil\" in labels.contexts[-1]\n",
        "start_frame = int(labels.start_times[1] / 50000)\n",
        "end_frame = int(labels.end_times[-2] / 50000)\n",
        "\n",
        "# 冒頭：50 ミリ秒、末尾：100 ミリ秒\n",
        "start_frame = max(0, start_frame - int(0.050 / 0.005))\n",
        "end_frame = min(minL, end_frame + int(0.100 / 0.005))\n",
        "\n",
        "in_feats = in_feats[start_frame:end_frame]\n",
        "out_feats = out_feats[start_frame:end_frame]"
      ],
      "id": "constant-authorization",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "literary-palace"
      },
      "source": [
        "print(\"入力特徴量のサイズ:\", in_feats.shape)\n",
        "print(\"出力特徴量のサイズ:\", out_feats.shape)"
      ],
      "id": "literary-palace",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thick-assault"
      },
      "source": [
        "#### 音響特徴量を分離して確認"
      ],
      "id": "thick-assault"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "passive-internship"
      },
      "source": [
        "from ttslearn.dnntts.multistream import get_static_features\n",
        "\n",
        "sr = 16000\n",
        "hop_length = int(sr * 0.005)\n",
        "alpha = pysptk.util.mcepalpha(sr)\n",
        "fft_size = pyworld.get_cheaptrick_fft_size(sr)\n",
        "\n",
        "# 動的特徴量を除いて、各音響特徴量を取り出します\n",
        "mgc, lf0, vuv, bap = get_static_features(\n",
        "    out_feats, num_windows=3, stream_sizes=[120, 3, 1, 3],\n",
        "    has_dynamic_features=[True, True, False, True]) \n",
        "print(\"メルケプストラムのサイズ:\", mgc.shape)\n",
        "print(\"連続対数基本周波数のサイズ:\", lf0.shape)\n",
        "print(\"有声/無声フラグのサイズ:\", vuv.shape)\n",
        "print(\"帯域非周期性指標のサイズ:\", bap.shape)"
      ],
      "id": "passive-internship",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "overhead-format"
      },
      "source": [
        "def vis_out_feats(mgc, lf0, vuv, bap):\n",
        "    fig, ax = plt.subplots(3, 1, figsize=(8,8))\n",
        "    ax[0].set_title(\"Spectral envelope\")\n",
        "    ax[1].set_title(\"Fundamental frequency\")\n",
        "    ax[2].set_title(\"Aperiodicity\")\n",
        "    \n",
        "    logsp = np.log(pysptk.mc2sp(mgc, alpha, fft_size))\n",
        "    librosa.display.specshow(logsp.T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"hz\", cmap=cmap, ax=ax[0])\n",
        "    \n",
        "    timeaxis = np.arange(len(lf0)) * 0.005\n",
        "    f0 = np.exp(lf0)\n",
        "    f0[vuv < 0.5] = 0\n",
        "    ax[1].plot(timeaxis, f0, linewidth=2)\n",
        "    ax[1].set_xlim(0, len(f0)*0.005)\n",
        "\n",
        "    aperiodicity = pyworld.decode_aperiodicity(bap.astype(np.float64), sr, fft_size)\n",
        "    librosa.display.specshow(aperiodicity.T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"hz\", cmap=cmap, ax=ax[2])\n",
        "    \n",
        "    for a in ax:\n",
        "        a.set_xlabel(\"Time [sec]\")\n",
        "        a.set_ylabel(\"Frequency [Hz]\")\n",
        "        # 末尾の非音声区間を除く\n",
        "        a.set_xlim(0, 2.55)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "\n",
        "# 音響特徴量の可視化\n",
        "vis_out_feats(mgc, lf0, vuv, bap)\n",
        "# 図6-4\n",
        "savefig(\"./fig/dnntts_impl_acoustic_out_feats\")"
      ],
      "id": "overhead-format",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mediterranean-coalition"
      },
      "source": [
        "#### 音響モデルの入力と出力の可視化"
      ],
      "id": "mediterranean-coalition"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "handmade-prediction"
      },
      "source": [
        "# 可視化用に正規化\n",
        "from scipy.stats import zscore \n",
        "\n",
        "in_feats_norm = in_feats / np.maximum(1, np.abs(in_feats).max(0))\n",
        "out_feats_norm = zscore(out_feats)\n",
        "\n",
        "fig, ax = plt.subplots(2, 1, figsize=(8,6))\n",
        "ax[0].set_title(\"Acoustic model's input: linguistic features\")\n",
        "ax[1].set_title(\"Acoustic model's output: acoustic features\")\n",
        "mesh = librosa.display.specshow(\n",
        "    in_feats_norm.T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"frames\", ax=ax[0], cmap=cmap)\n",
        "fig.colorbar(mesh, ax=ax[0])\n",
        "mesh = librosa.display.specshow(\n",
        "    out_feats_norm.T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"frames\",ax=ax[1], cmap=cmap)\n",
        "# NOTE: 実際には [-4, 4]の範囲外の値もありますが、視認性のために [-4, 4]に設定します\n",
        "mesh.set_clim(-4, 4)\n",
        "fig.colorbar(mesh, ax=ax[1])\n",
        "\n",
        "ax[0].set_ylabel(\"Context\")\n",
        "ax[1].set_ylabel(\"Feature\")\n",
        "\n",
        "for a in ax:\n",
        "    a.set_xlabel(\"Time [sec]\")\n",
        "    # 末尾の非音声区間を除く\n",
        "    a.set_xlim(0, 2.55)\n",
        "    \n",
        "plt.tight_layout()"
      ],
      "id": "handmade-prediction",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dried-region"
      },
      "source": [
        "### レシピの stage 2 の実行\n",
        "\n",
        "バッチ処理を行うコマンドラインプログラムは、 `preprocess_acoustic.py` を参照してください。"
      ],
      "id": "dried-region"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enclosed-mercy"
      },
      "source": [
        "if run_sh:\n",
        "    ! ./run.sh --stage 2 --stop-stage 2 --n-jobs $n_jobs"
      ],
      "id": "enclosed-mercy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trying-surveillance"
      },
      "source": [
        "## 6.5 特徴量の正規化"
      ],
      "id": "trying-surveillance"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "destroyed-albuquerque"
      },
      "source": [
        "正規化のための統計量を計算するコマンドラインプログラムは、 `recipes/common/fit_scaler.py` を参照してください。また、正規化を行うコマンドラインプログラムは、 `recipes/common/preprocess_normalize.py` を参照してください。"
      ],
      "id": "destroyed-albuquerque"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fifty-error"
      },
      "source": [
        "### レシピの stage 3 の実行"
      ],
      "id": "fifty-error"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arctic-liberty"
      },
      "source": [
        "if run_sh:\n",
        "    ! ./run.sh --stage 3 --stop-stage 3 --n-jobs $n_jobs"
      ],
      "id": "arctic-liberty",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "israeli-grade"
      },
      "source": [
        "### 正規化の処理の結果の確認"
      ],
      "id": "israeli-grade"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anonymous-saint"
      },
      "source": [
        "# 言語特徴量の正規化前後\n",
        "in_feats = np.load(f\"dump/jsut_sr16000/org/train/in_acoustic/{train_utt}-feats.npy\")\n",
        "in_feats_norm = np.load(f\"dump/jsut_sr16000/norm/train/in_acoustic/{train_utt}-feats.npy\")\n",
        "fig, ax = plt.subplots(2, 1, figsize=(8,6))\n",
        "ax[0].set_title(\"Linguistic features (before normalization)\")\n",
        "ax[1].set_title(\"Linguistic features (after normalization)\")\n",
        "mesh = librosa.display.specshow(\n",
        "    in_feats.T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"frames\", ax=ax[0], cmap=cmap)\n",
        "fig.colorbar(mesh, ax=ax[0])\n",
        "mesh = librosa.display.specshow(\n",
        "    in_feats_norm.T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"frames\",ax=ax[1], cmap=cmap)\n",
        "# NOTE: 実際には [-4, 4]の範囲外の値もありますが、視認性のために [-4, 4]に設定します\n",
        "mesh.set_clim(-4, 4)\n",
        "fig.colorbar(mesh, ax=ax[1])\n",
        "\n",
        "for a in ax:\n",
        "    a.set_xlabel(\"Time [sec]\")\n",
        "    a.set_ylabel(\"Context\")\n",
        "    # 末尾の非音声区間を除く\n",
        "    a.set_xlim(0, 2.55) \n",
        "plt.tight_layout()\n",
        "# 図6-5\n",
        "savefig(\"./fig/dnntts_impl_in_feats_norm\")"
      ],
      "id": "anonymous-saint",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awful-pavilion"
      },
      "source": [
        "# 音響特徴量の正規化前後\n",
        "out_feats = np.load(f\"dump/jsut_sr16000/org/train/out_acoustic/{train_utt}-feats.npy\")\n",
        "out_feats_norm = np.load(f\"dump/jsut_sr16000/norm/train/out_acoustic/{train_utt}-feats.npy\")\n",
        "fig, ax = plt.subplots(2, 1, figsize=(8,6))\n",
        "ax[0].set_title(\"Acoustic features (before normalization)\")\n",
        "ax[1].set_title(\"Acoustic features (after normalization)\")\n",
        "mesh = librosa.display.specshow(\n",
        "    out_feats.T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"frames\", ax=ax[0], cmap=cmap)\n",
        "fig.colorbar(mesh, ax=ax[0])\n",
        "mesh = librosa.display.specshow(\n",
        "    out_feats_norm.T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"frames\",ax=ax[1], cmap=cmap)\n",
        "# NOTE: 実際には [-4, 4]の範囲外の値もありますが、視認性のために [-4, 4]に設定します\n",
        "mesh.set_clim(-4, 4)\n",
        "fig.colorbar(mesh, ax=ax[1])\n",
        "\n",
        "for a in ax:\n",
        "    a.set_xlabel(\"Time [sec]\")\n",
        "    a.set_ylabel(\"Feature\")\n",
        "    # 末尾の非音声区間を除く\n",
        "    a.set_xlim(0, 2.55)\n",
        "plt.tight_layout()"
      ],
      "id": "awful-pavilion",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "subjective-federal"
      },
      "source": [
        "## 6.6 ニューラルネットワークの実装"
      ],
      "id": "subjective-federal"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "removed-marine"
      },
      "source": [
        "### 全結合型ニューラルネットワーク"
      ],
      "id": "removed-marine"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dated-limitation"
      },
      "source": [
        "class DNN(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, num_layers=2):\n",
        "        super(DNN, self).__init__()\n",
        "        model = [nn.Linear(in_dim, hidden_dim), nn.ReLU()]\n",
        "        for _ in range(num_layers):\n",
        "            model.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "            model.append(nn.ReLU())\n",
        "        model.append(nn.Linear(hidden_dim, out_dim))\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x, lens=None):\n",
        "        return self.model(x)"
      ],
      "id": "dated-limitation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "martial-municipality"
      },
      "source": [
        "DNN(in_dim=325, hidden_dim=64, out_dim=1, num_layers=2)"
      ],
      "id": "martial-municipality",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "satellite-boutique"
      },
      "source": [
        "### LSTM-RNN"
      ],
      "id": "satellite-boutique"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "searching-booking"
      },
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class LSTMRNN(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_dim, hidden_dim, out_dim, num_layers=1, bidirectional=True, dropout=0.0\n",
        "    ):\n",
        "        super(LSTMRNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        num_direction = 2 if bidirectional else 1\n",
        "        self.lstm = nn.LSTM(\n",
        "            in_dim,\n",
        "            hidden_dim,\n",
        "            num_layers,\n",
        "            bidirectional=bidirectional,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.hidden2out = nn.Linear(num_direction * hidden_dim, out_dim)\n",
        "\n",
        "    def forward(self, seqs, lens):\n",
        "        seqs = pack_padded_sequence(seqs, lens, batch_first=True)\n",
        "        out, _ = self.lstm(seqs)\n",
        "        out, _ = pad_packed_sequence(out, batch_first=True)\n",
        "        out = self.hidden2out(out)\n",
        "        return out"
      ],
      "id": "searching-booking",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "traditional-parcel"
      },
      "source": [
        "LSTMRNN(in_dim=325, hidden_dim=64, out_dim=1, num_layers=2)"
      ],
      "id": "traditional-parcel",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "received-elder"
      },
      "source": [
        "## 6.7 学習スクリプトの実装"
      ],
      "id": "received-elder"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "connected-nashville"
      },
      "source": [
        "### DataLoader の実装"
      ],
      "id": "connected-nashville"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "received-sword"
      },
      "source": [
        "#### Datasetクラスの定義"
      ],
      "id": "received-sword"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cheap-confusion"
      },
      "source": [
        "from torch.utils import data as data_utils\n",
        "\n",
        "class Dataset(data_utils.Dataset):\n",
        "    def __init__(self, in_paths, out_paths):\n",
        "        self.in_paths = in_paths\n",
        "        self.out_paths = out_paths\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return np.load(self.in_paths[idx]), np.load(self.out_paths[idx])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.in_paths)"
      ],
      "id": "cheap-confusion",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "middle-texas"
      },
      "source": [
        "#### DataLoader の利用例"
      ],
      "id": "middle-texas"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "removed-coverage"
      },
      "source": [
        "from pathlib import Path\n",
        "from ttslearn.util import pad_2d\n",
        "\n",
        "def collate_fn_dnntts(batch):\n",
        "    lengths = [len(x[0]) for x in batch]\n",
        "    max_len = max(lengths)\n",
        "    x_batch = torch.stack([torch.from_numpy(pad_2d(x[0], max_len)) for x in batch])\n",
        "    y_batch = torch.stack([torch.from_numpy(pad_2d(x[1], max_len)) for x in batch])\n",
        "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
        "    return x_batch, y_batch, lengths\n",
        "\n",
        "\n",
        "in_paths = sorted(Path(\"./dump/jsut_sr16000/norm/dev/in_duration/\").glob(\"*.npy\"))\n",
        "out_paths = sorted(Path(\"./dump/jsut_sr16000/norm/dev/out_duration/\").glob(\"*.npy\"))\n",
        "\n",
        "dataset = Dataset(in_paths, out_paths)\n",
        "data_loader = data_utils.DataLoader(dataset, batch_size=8, collate_fn=collate_fn_dnntts, num_workers=0)\n",
        "\n",
        "in_feats, out_feats, lengths = next(iter(data_loader))\n",
        "\n",
        "print(\"入力特徴量のサイズ:\", tuple(in_feats.shape))\n",
        "print(\"出力特徴量のサイズ:\", tuple(out_feats.shape))\n",
        "print(\"系列長のサイズ:\", tuple(lengths.shape))"
      ],
      "id": "removed-coverage",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "executed-belfast"
      },
      "source": [
        "#### ミニバッチの可視化"
      ],
      "id": "executed-belfast"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bronze-conservation"
      },
      "source": [
        "fig, ax = plt.subplots(len(in_feats), 1, figsize=(8,10), sharex=True, sharey=True)\n",
        "for n in range(len(in_feats)):\n",
        "    x = in_feats[n].data.numpy()\n",
        "    mesh = ax[n].imshow(x.T, aspect=\"auto\", interpolation=\"nearest\", origin=\"lower\", cmap=cmap)\n",
        "    fig.colorbar(mesh, ax=ax[n])\n",
        "    # NOTE: 実際には [-4, 4]の範囲外の値もありますが、視認性のために [-4, 4]に設定します\n",
        "    mesh.set_clim(-4, 4)\n",
        "    \n",
        "ax[-1].set_xlabel(\"Phoneme\")\n",
        "for a in ax:\n",
        "    a.set_ylabel(\"Context\")\n",
        "plt.tight_layout()\n",
        "\n",
        "# 図6-6\n",
        "savefig(\"fig/dnntts_impl_minibatch\")"
      ],
      "id": "bronze-conservation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "single-abortion"
      },
      "source": [
        "### 学習の前準備"
      ],
      "id": "single-abortion"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cellular-apollo"
      },
      "source": [
        "from ttslearn.dnntts import DNN\n",
        "from torch import optim\n",
        "\n",
        "model = DNN(in_dim=325, hidden_dim=64, out_dim=1, num_layers=2)\n",
        "\n",
        "# lr は学習率を表します\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# gamma は学習率の減衰係数を表します\n",
        "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, gamma=0.5, step_size=10)"
      ],
      "id": "cellular-apollo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scenic-humidity"
      },
      "source": [
        "### 学習ループの実装"
      ],
      "id": "scenic-humidity"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "registered-factor"
      },
      "source": [
        "# DataLoader を用いたミニバッチの作成: ミニバッチ毎に処理する\n",
        "for in_feats, out_feats, lengths in data_loader:\n",
        "    # 順伝播の計算\n",
        "    pred_out_feats = model(in_feats, lengths)\n",
        "    # 損失の計算\n",
        "    loss = nn.MSELoss()(pred_out_feats, out_feats)\n",
        "    # 損失の値を出力\n",
        "    print(loss.item())\n",
        "    # optimizer に蓄積された勾配をリセット\n",
        "    optimizer.zero_grad()\n",
        "    # 誤差の逆伝播の計算\n",
        "    loss.backward()\n",
        "    # パラメータの更新\n",
        "    optimizer.step()"
      ],
      "id": "registered-factor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "square-damage"
      },
      "source": [
        "### hydra を用いたコマンドラインプログラムの実装\n",
        "\n",
        "`hydra/hydra_quick_start` と `hydra/hydra_composision` を参照してください。"
      ],
      "id": "square-damage"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stylish-failing"
      },
      "source": [
        "### hydra を用いた実用的な学習スクリプトの設定ファイル"
      ],
      "id": "stylish-failing"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "artistic-shell"
      },
      "source": [
        "`conf/train_dnntts` ディレクトリを参照してください"
      ],
      "id": "artistic-shell"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "familiar-adjustment"
      },
      "source": [
        "! cat conf/train_dnntts/config.yaml"
      ],
      "id": "familiar-adjustment",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "musical-performance"
      },
      "source": [
        "### hydra を用いた実用的な学習スクリプトの実装"
      ],
      "id": "musical-performance"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "completed-newman"
      },
      "source": [
        "`train_dnntts.py` を参照してください。"
      ],
      "id": "completed-newman"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "effective-screening"
      },
      "source": [
        "## 6.8 継続長モデルの学習"
      ],
      "id": "effective-screening"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "different-damage"
      },
      "source": [
        "### 継続長モデルの設定ファイル"
      ],
      "id": "different-damage"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "floating-initial"
      },
      "source": [
        "! cat conf/train_dnntts/model/{duration_config_name}.yaml"
      ],
      "id": "floating-initial",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "changed-batch"
      },
      "source": [
        "### 継続長モデルのインスタンス化"
      ],
      "id": "changed-batch"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "virtual-shooting"
      },
      "source": [
        "import hydra\n",
        "from omegaconf import OmegaConf\n",
        "hydra.utils.instantiate(OmegaConf.load(f\"conf/train_dnntts/model/{duration_config_name}.yaml\").netG)"
      ],
      "id": "virtual-shooting",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "front-toddler"
      },
      "source": [
        "### レシピの stage 4 の実行"
      ],
      "id": "front-toddler"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "practical-hazard"
      },
      "source": [
        "if run_sh:\n",
        "    ! ./run.sh --stage 4 --stop-stage 4 --duration-model $duration_config_name \\\n",
        "    --tqdm $run_sh_tqdm --dnntts-data-batch-size $batch_size --dnntts-train-nepochs $nepochs \\\n",
        "    --cudnn-benchmark $cudnn_benchmark --cudnn-deterministic $cudnn_deterministic"
      ],
      "id": "practical-hazard",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "korean-angle"
      },
      "source": [
        "### 損失関数の値の推移\n",
        "\n",
        "著者による実験結果です。Tensorboardのログは https://tensorboard.dev/ にアップロードされています。\n",
        "ログデータを`tensorboard` パッケージを利用してダウンロードします。\n",
        "\n",
        "https://tensorboard.dev/experiment/ajmqiymoTx6rADKLF8d6sA/"
      ],
      "id": "korean-angle"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "specialized-winner"
      },
      "source": [
        "if exists(\"tensorboard/all_log.csv\"):\n",
        "    df = pd.read_csv(\"tensorboard/all_log.csv\")\n",
        "else:\n",
        "    experiment_id = \"ajmqiymoTx6rADKLF8d6sA\"\n",
        "    experiment = tb.data.experimental.ExperimentFromDev(experiment_id)\n",
        "    df = experiment.get_scalars(pivot=True) \n",
        "    df.to_csv(\"tensorboard/all_log.csv\", index=False)\n",
        "df[\"run\"].unique()"
      ],
      "id": "specialized-winner",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suitable-traffic"
      },
      "source": [
        "duration_loss = df[df.run.str.contains(\"duration\")]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "ax.plot(duration_loss[\"step\"], duration_loss[\"Loss/train\"], label=\"Train\")\n",
        "ax.plot(duration_loss[\"step\"], duration_loss[\"Loss/dev\"], \"--\", label=\"Dev\")\n",
        "ax.set_xlabel(\"Epoch\")\n",
        "ax.set_ylabel(\"Epoch loss\")\n",
        "plt.legend()\n",
        "\n",
        "# 図6-8\n",
        "savefig(\"fig/dnntts_impl_duration_dnn_loss\")"
      ],
      "id": "suitable-traffic",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pleasant-david"
      },
      "source": [
        "## 6.9 音響モデルの学習"
      ],
      "id": "pleasant-david"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upset-jordan"
      },
      "source": [
        "### 音響モデルの設定ファイル"
      ],
      "id": "upset-jordan"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "preliminary-sapphire"
      },
      "source": [
        "! cat conf/train_dnntts/model/{acoustic_config_name}.yaml"
      ],
      "id": "preliminary-sapphire",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conventional-clinic"
      },
      "source": [
        "### 音響モデルのインスタンス化"
      ],
      "id": "conventional-clinic"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ranging-cinema"
      },
      "source": [
        "import hydra\n",
        "from omegaconf import OmegaConf\n",
        "hydra.utils.instantiate(OmegaConf.load(f\"conf/train_dnntts/model/{acoustic_config_name}.yaml\").netG)"
      ],
      "id": "ranging-cinema",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "featured-findings"
      },
      "source": [
        "### レシピの stage 5 の実行"
      ],
      "id": "featured-findings"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "processed-practitioner"
      },
      "source": [
        "if run_sh:\n",
        "    ! ./run.sh --stage 5 --stop-stage 5 --acoustic-model $acoustic_config_name \\\n",
        "    --tqdm $run_sh_tqdm --dnntts-data-batch-size $batch_size --dnntts-train-nepochs $nepochs \\\n",
        "    --cudnn-benchmark $cudnn_benchmark --cudnn-deterministic $cudnn_deterministic"
      ],
      "id": "processed-practitioner",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "large-mexico"
      },
      "source": [
        "### 損失関数の値の推移"
      ],
      "id": "large-mexico"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "forbidden-finish"
      },
      "source": [
        "acoustic_loss = df[df.run.str.contains(\"acoustic\")]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "ax.plot(acoustic_loss[\"step\"], acoustic_loss[\"Loss/train\"], label=\"Train\")\n",
        "ax.plot(acoustic_loss[\"step\"], acoustic_loss[\"Loss/dev\"], \"--\", label=\"Dev\")\n",
        "ax.set_xlabel(\"Epoch\")\n",
        "ax.set_ylabel(\"Epoch loss\")\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "# 図6-9\n",
        "savefig(\"fig/dnntts_impl_acoustic_dnn_loss\")"
      ],
      "id": "forbidden-finish",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "contrary-horror"
      },
      "source": [
        "## 6.10 学習済みモデルを用いてテキストから音声を合成"
      ],
      "id": "contrary-horror"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acute-rating"
      },
      "source": [
        "### 学習済みモデルの読み込み"
      ],
      "id": "acute-rating"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "binary-rebel"
      },
      "source": [
        "import joblib\n",
        "device = torch.device(\"cpu\")"
      ],
      "id": "binary-rebel",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "electric-gasoline"
      },
      "source": [
        "#### 継続長モデルの読み込み"
      ],
      "id": "electric-gasoline"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "forward-situation"
      },
      "source": [
        "duration_config = OmegaConf.load(f\"exp/jsut_sr16000/{duration_config_name}/model.yaml\")\n",
        "duration_model = hydra.utils.instantiate(duration_config.netG)\n",
        "checkpoint = torch.load(f\"exp/jsut_sr16000/{duration_config_name}/latest.pth\", map_location=device)\n",
        "duration_model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "duration_model.eval();"
      ],
      "id": "forward-situation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ranking-sucking"
      },
      "source": [
        "#### 音響モデルの読み込み"
      ],
      "id": "ranking-sucking"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "correct-capitol"
      },
      "source": [
        "acoustic_config = OmegaConf.load(f\"exp/jsut_sr16000/{acoustic_config_name}/model.yaml\")\n",
        "acoustic_model = hydra.utils.instantiate(acoustic_config.netG)\n",
        "checkpoint = torch.load(f\"exp/jsut_sr16000/{acoustic_config_name}/latest.pth\", map_location=device)\n",
        "acoustic_model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "acoustic_model.eval();"
      ],
      "id": "correct-capitol",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "initial-angel"
      },
      "source": [
        "#### 統計量の読み込み"
      ],
      "id": "initial-angel"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "consolidated-registration"
      },
      "source": [
        "duration_in_scaler = joblib.load(\"./dump/jsut_sr16000/norm/in_duration_scaler.joblib\")\n",
        "duration_out_scaler = joblib.load(\"./dump/jsut_sr16000/norm/out_duration_scaler.joblib\")\n",
        "acoustic_in_scaler = joblib.load(\"./dump/jsut_sr16000/norm/in_acoustic_scaler.joblib\")\n",
        "acoustic_out_scaler = joblib.load(\"./dump/jsut_sr16000/norm/out_acoustic_scaler.joblib\")"
      ],
      "id": "consolidated-registration",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "optional-prediction"
      },
      "source": [
        "### 音素継続長の予測"
      ],
      "id": "optional-prediction"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beautiful-amendment"
      },
      "source": [
        "@torch.no_grad()\n",
        "def predict_duration(\n",
        "    device,  # cpu or cuda\n",
        "    labels,  # フルコンテキストラベル\n",
        "    duration_model,  # 学習済み継続長モデル\n",
        "    duration_config,  # 継続長モデルの設定\n",
        "    duration_in_scaler,  # 言語特徴量の正規化用 StandardScaler\n",
        "    duration_out_scaler,  # 音素継続長の正規化用 StandardScaler\n",
        "    binary_dict,  # 二値特徴量を抽出する正規表現\n",
        "    numeric_dict,  # 数値特徴量を抽出する正規表現\n",
        "):\n",
        "    # 言語特徴量の抽出\n",
        "    in_feats = fe.linguistic_features(labels, binary_dict, numeric_dict).astype(np.float32)\n",
        "\n",
        "    # 言語特徴量の正規化\n",
        "    in_feats = duration_in_scaler.transform(in_feats)\n",
        "\n",
        "    # 継続長の予測\n",
        "    x = torch.from_numpy(in_feats).float().to(device).view(1, -1, in_feats.shape[-1])\n",
        "    pred_durations = duration_model(x, [x.shape[1]]).squeeze(0).cpu().data.numpy()\n",
        "\n",
        "    # 予測された継続長に対して、正規化の逆変換を行います\n",
        "    pred_durations = duration_out_scaler.inverse_transform(pred_durations)\n",
        "\n",
        "    # 閾値処理\n",
        "    pred_durations[pred_durations <= 0] = 1\n",
        "    pred_durations = np.round(pred_durations)\n",
        "\n",
        "    return pred_durations"
      ],
      "id": "beautiful-amendment",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "furnished-melissa"
      },
      "source": [
        "from ttslearn.util import lab2phonemes, find_lab, find_feats\n",
        "\n",
        "labels = hts.load(find_lab(\"downloads/jsut_ver1.1/\", test_utt))\n",
        "\n",
        "# フルコンテキストラベルから音素のみを抽出\n",
        "test_phonemes = lab2phonemes(labels)\n",
        "\n",
        "# 言語特徴量の抽出に使うための質問ファイル\n",
        "binary_dict, numeric_dict = hts.load_question_set(ttslearn.util.example_qst_file())\n",
        "\n",
        "# 音素継続長の予測\n",
        "durations_test = predict_duration(\n",
        "    device, labels, duration_model, duration_config, duration_in_scaler, duration_out_scaler,\n",
        "    binary_dict, numeric_dict)\n",
        "durations_test_target = np.load(find_feats(\"dump/jsut_sr16000/org\", test_utt, typ=\"out_duration\"))\n",
        "\n",
        "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
        "ax.plot(durations_test_target, \"-+\", label=\"Target\")\n",
        "ax.plot(durations_test, \"--*\", label=\"Predicted\")\n",
        "ax.set_xticks(np.arange(len(test_phonemes)))\n",
        "ax.set_xticklabels(test_phonemes)\n",
        "ax.set_xlabel(\"Phoneme\")\n",
        "ax.set_ylabel(\"Duration (the number of frames)\")\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# 図6-10\n",
        "savefig(\"fig/dnntts_impl_duration_comp\")"
      ],
      "id": "furnished-melissa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "divine-profile"
      },
      "source": [
        "### 音響特徴量の予測"
      ],
      "id": "divine-profile"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "compatible-device"
      },
      "source": [
        "from ttslearn.dnntts.multistream import get_windows, multi_stream_mlpg\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_acoustic(\n",
        "    device,  # CPU or GPU\n",
        "    labels,  # フルコンテキストラベル\n",
        "    acoustic_model,  # 学習済み音響モデル\n",
        "    acoustic_config,  # 音響モデルの設定\n",
        "    acoustic_in_scaler,  # 言語特徴量の正規化用 StandardScaler\n",
        "    acoustic_out_scaler,  # 音響特徴量の正規化用 StandardScaler\n",
        "    binary_dict,  # 二値特徴量を抽出する正規表現\n",
        "    numeric_dict,  # 数値特徴量を抽出する正規表現\n",
        "    mlpg=True,  # MLPG を使用するかどうか\n",
        "):\n",
        "    # フレーム単位の言語特徴量の抽出\n",
        "    in_feats = fe.linguistic_features(\n",
        "        labels,\n",
        "        binary_dict,\n",
        "        numeric_dict,\n",
        "        add_frame_features=True,\n",
        "        subphone_features=\"coarse_coding\",\n",
        "    )\n",
        "    # 正規化\n",
        "    in_feats = acoustic_in_scaler.transform(in_feats)\n",
        "\n",
        "    # 音響特徴量の予測\n",
        "    x = torch.from_numpy(in_feats).float().to(device).view(1, -1, in_feats.shape[-1])\n",
        "    pred_acoustic = acoustic_model(x, [x.shape[1]]).squeeze(0).cpu().data.numpy()\n",
        "\n",
        "    # 予測された音響特徴量に対して、正規化の逆変換を行います\n",
        "    pred_acoustic = acoustic_out_scaler.inverse_transform(pred_acoustic)\n",
        "\n",
        "    # パラメータ生成アルゴリズム (MLPG) の実行\n",
        "    if mlpg and np.any(acoustic_config.has_dynamic_features):\n",
        "        # (T, D_out) -> (T, static_dim)\n",
        "        pred_acoustic = multi_stream_mlpg(\n",
        "            pred_acoustic,\n",
        "            acoustic_out_scaler.var_,\n",
        "            get_windows(acoustic_config.num_windows),\n",
        "            acoustic_config.stream_sizes,\n",
        "            acoustic_config.has_dynamic_features,\n",
        "        )\n",
        "\n",
        "    return pred_acoustic"
      ],
      "id": "compatible-device",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sapphire-slope"
      },
      "source": [
        "labels = hts.load(f\"./downloads/jsut_ver1.1/basic5000/lab/{test_utt}.lab\")\n",
        "\n",
        "# 音響特徴量の予測\n",
        "out_feats = predict_acoustic(\n",
        "    device, labels, acoustic_model, acoustic_config, acoustic_in_scaler,\n",
        "    acoustic_out_scaler, binary_dict, numeric_dict)"
      ],
      "id": "sapphire-slope",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spectacular-factor"
      },
      "source": [
        "from ttslearn.util import trim_silence\n",
        "from ttslearn.dnntts.multistream import split_streams\n",
        "\n",
        "# 特徴量は、前処理で冒頭と末尾に非音声区間が切り詰められているので、比較のためにここでも同様の処理を行います\n",
        "out_feats = trim_silence(out_feats, labels)\n",
        "# 結合された特徴量を分離\n",
        "mgc_gen, lf0_gen, vuv_gen, bap_gen = split_streams(out_feats, [40, 1, 1, 1])"
      ],
      "id": "spectacular-factor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geographic-example"
      },
      "source": [
        "# 比較用に、自然音声から抽出された音響特徴量を読み込みむ\n",
        "feats = np.load(f\"./dump/jsut_sr16000/org/eval/out_acoustic/{test_utt}-feats.npy\")\n",
        "# 特徴量の分離\n",
        "mgc_ref, lf0_ref, vuv_ref, bap_ref = get_static_features(\n",
        "    feats, acoustic_config.num_windows, acoustic_config.stream_sizes, acoustic_config.has_dynamic_features)"
      ],
      "id": "geographic-example",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "photographic-dubai"
      },
      "source": [
        "#### スペクトル包絡の可視化"
      ],
      "id": "photographic-dubai"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "starting-crazy"
      },
      "source": [
        "# 音響特徴量を、WORLDの音声パラメータへ変換する\n",
        "\n",
        "# メルケプストラムからスペクトル包絡への変換\n",
        "sp_gen= pysptk.mc2sp(mgc_gen, alpha, fft_size)\n",
        "sp_ref= pysptk.mc2sp(mgc_ref, alpha, fft_size)\n",
        "\n",
        "mindb = min(np.log(sp_ref).min(), np.log(sp_gen).min())\n",
        "maxdb = max(np.log(sp_ref).max(), np.log(sp_gen).max())\n",
        "\n",
        "fig, ax = plt.subplots(2, 1, figsize=(8,6))\n",
        "mesh = librosa.display.specshow(np.log(sp_ref).T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"linear\", cmap=cmap, ax=ax[0])\n",
        "mesh.set_clim(mindb, maxdb)\n",
        "fig.colorbar(mesh, ax=ax[0], format=\"%+2.fdB\")\n",
        "mesh = librosa.display.specshow(np.log(sp_gen).T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"linear\", cmap=cmap, ax=ax[1])\n",
        "mesh.set_clim(mindb, maxdb)\n",
        "fig.colorbar(mesh, ax=ax[1], format=\"%+2.fdB\")\n",
        "for a in ax:\n",
        "    a.set_xlabel(\"Time [sec]\")\n",
        "    a.set_ylabel(\"Frequency [Hz]\")\n",
        "    \n",
        "ax[0].set_title(\"Spectral envelope of natural speech\")\n",
        "ax[1].set_title(\"Spectral envelope of generated speech\")\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# 図6-11\n",
        "savefig(\"./fig/dnntts_impl_spec_comp\")"
      ],
      "id": "starting-crazy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "outstanding-advisory"
      },
      "source": [
        "#### F0の可視化"
      ],
      "id": "outstanding-advisory"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viral-harvard"
      },
      "source": [
        "# 対数基本周波数から基本周波数への変換\n",
        "f0_ref = np.exp(lf0_ref)\n",
        "f0_ref[vuv_ref < 0.5] = 0\n",
        "f0_gen = np.exp(lf0_gen)\n",
        "f0_gen[vuv_gen < 0.5] = 0\n",
        "\n",
        "timeaxis = librosa.frames_to_time(np.arange(len(f0_ref)), sr=sr, hop_length=int(0.005 * sr))\n",
        "\n",
        "fix, ax = plt.subplots(1,1, figsize=(8,3))\n",
        "ax.plot(timeaxis, f0_ref, linewidth=2, label=\"F0 of natural speech\")\n",
        "ax.plot(timeaxis, f0_gen, \"--\", linewidth=2, label=\"F0 of generated speech\")\n",
        "\n",
        "ax.set_xlabel(\"Time [sec]\")\n",
        "ax.set_ylabel(\"Frequency [Hz]\")\n",
        "ax.set_xlim(timeaxis[0], timeaxis[-1])\n",
        "\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "\n",
        "# 図6-12\n",
        "savefig(\"./fig/dnntts_impl_f0_comp\")"
      ],
      "id": "viral-harvard",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sustained-component"
      },
      "source": [
        "#### 帯域非周期性指標の可視化 (bonus)"
      ],
      "id": "sustained-component"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "historical-jersey"
      },
      "source": [
        "# 帯域非周期性指標\n",
        "ap_ref = pyworld.decode_aperiodicity(bap_ref.astype(np.float64), sr, fft_size)\n",
        "ap_gen = pyworld.decode_aperiodicity(bap_gen.astype(np.float64), sr, fft_size)\n",
        "\n",
        "mindb = min(np.log(ap_ref).min(), np.log(ap_gen).min())\n",
        "maxdb = max(np.log(ap_ref).max(), np.log(ap_gen).max())\n",
        "\n",
        "fig, ax = plt.subplots(2, 1, figsize=(8,6))\n",
        "mesh = librosa.display.specshow(np.log(ap_ref).T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"linear\", cmap=cmap, ax=ax[0])\n",
        "mesh.set_clim(mindb, maxdb)\n",
        "fig.colorbar(mesh, ax=ax[0])\n",
        "mesh = librosa.display.specshow(np.log(ap_gen).T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"linear\", cmap=cmap, ax=ax[1])\n",
        "mesh.set_clim(mindb, maxdb)\n",
        "fig.colorbar(mesh, ax=ax[1])\n",
        "\n",
        "for a in ax:\n",
        "    a.set_xlabel(\"Time [sec]\")\n",
        "    a.set_ylabel(\"Frequency [Hz]\")\n",
        "plt.tight_layout()"
      ],
      "id": "historical-jersey",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "positive-panel"
      },
      "source": [
        "### 音声波形の生成"
      ],
      "id": "positive-panel"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "universal-rabbit"
      },
      "source": [
        "from nnmnkwii.postfilters import merlin_post_filter\n",
        "from ttslearn.dnntts.multistream import get_static_stream_sizes\n",
        "\n",
        "def gen_waveform(\n",
        "    sample_rate,  # サンプリング周波数\n",
        "    acoustic_features,  # 音響特徴量\n",
        "    stream_sizes,  # ストリームサイズ\n",
        "    has_dynamic_features,  # 音響特徴量が動的特徴量を含むかどうか\n",
        "    num_windows=3,  # 動的特徴量の計算に使う窓数\n",
        "    post_filter=False,  # フォルマント強調のポストフィルタを使うかどうか\n",
        "):\n",
        "    # 静的特徴量の次元数を取得\n",
        "    if np.any(has_dynamic_features):\n",
        "        static_stream_sizes = get_static_stream_sizes(\n",
        "            stream_sizes, has_dynamic_features, num_windows\n",
        "        )\n",
        "    else:\n",
        "        static_stream_sizes = stream_sizes\n",
        "\n",
        "    # 結合された音響特徴量をストリーム毎に分離\n",
        "    mgc, lf0, vuv, bap = split_streams(acoustic_features, static_stream_sizes)\n",
        "\n",
        "    fftlen = pyworld.get_cheaptrick_fft_size(sample_rate)\n",
        "    alpha = pysptk.util.mcepalpha(sample_rate)\n",
        "\n",
        "    # フォルマント強調のポストフィルタ\n",
        "    if post_filter:\n",
        "        mgc = merlin_post_filter(mgc, alpha)\n",
        "\n",
        "    # 音響特徴量を音声パラメータに変換\n",
        "    spectrogram = pysptk.mc2sp(mgc, fftlen=fftlen, alpha=alpha)\n",
        "    aperiodicity = pyworld.decode_aperiodicity(\n",
        "        bap.astype(np.float64), sample_rate, fftlen\n",
        "    )\n",
        "    f0 = lf0.copy()\n",
        "    f0[vuv < 0.5] = 0\n",
        "    f0[np.nonzero(f0)] = np.exp(f0[np.nonzero(f0)])\n",
        "\n",
        "    # WORLD ボコーダを利用して音声生成\n",
        "    gen_wav = pyworld.synthesize(\n",
        "        f0.flatten().astype(np.float64),\n",
        "        spectrogram.astype(np.float64),\n",
        "        aperiodicity.astype(np.float64),\n",
        "        sample_rate,\n",
        "    )\n",
        "\n",
        "    return gen_wav"
      ],
      "id": "universal-rabbit",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "automotive-pressing"
      },
      "source": [
        "### すべてのモデルを組み合わせて音声波形の生成"
      ],
      "id": "automotive-pressing"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "environmental-intermediate"
      },
      "source": [
        "labels = hts.load(f\"./downloads/jsut_ver1.1/basic5000/lab/{test_utt}.lab\")\n",
        "\n",
        "binary_dict, numeric_dict = hts.load_question_set(ttslearn.util.example_qst_file())\n",
        "\n",
        "# 音素継続長の予測\n",
        "durations = predict_duration(\n",
        "    device, labels, duration_model, duration_config, duration_in_scaler, duration_out_scaler,\n",
        "    binary_dict, numeric_dict)\n",
        "\n",
        "# 予測された継続帳をフルコンテキストラベルに設定\n",
        "labels.set_durations(durations)\n",
        "\n",
        "# 音響特徴量の予測\n",
        "out_feats = predict_acoustic(\n",
        "    device, labels, acoustic_model, acoustic_config, acoustic_in_scaler,\n",
        "    acoustic_out_scaler, binary_dict, numeric_dict)\n",
        "\n",
        "# 音声波形の生成\n",
        "gen_wav = gen_waveform(\n",
        "    sr, out_feats,\n",
        "    acoustic_config.stream_sizes,\n",
        "    acoustic_config.has_dynamic_features,\n",
        "    acoustic_config.num_windows,\n",
        "    post_filter=False,\n",
        ")"
      ],
      "id": "environmental-intermediate",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inclusive-headline"
      },
      "source": [
        "# 比較用に元音声の読み込み\n",
        "_sr, ref_wav = wavfile.read(f\"./downloads/jsut_ver1.1/basic5000/wav/{test_utt}.wav\")\n",
        "ref_wav = (ref_wav / 32768.0).astype(np.float64)\n",
        "ref_wav = librosa.resample(ref_wav, _sr, sr)\n",
        "\n",
        "# スペクトログラムを計算\n",
        "spec_ref = librosa.stft(ref_wav, n_fft=fft_size, hop_length=hop_length, window=\"hann\")\n",
        "logspec_ref = np.log(np.abs(spec_ref))\n",
        "spec_gen = librosa.stft(gen_wav, n_fft=fft_size, hop_length=hop_length, window=\"hann\")\n",
        "logspec_gen = np.log(np.abs(spec_gen))\n",
        "\n",
        "mindb = min(logspec_ref.min(), logspec_gen.min())\n",
        "maxdb = max(logspec_ref.max(), logspec_gen.max())\n",
        "\n",
        "fig, ax = plt.subplots(2, 1, figsize=(8,6))\n",
        "mesh = librosa.display.specshow(logspec_ref, hop_length=hop_length, sr=sr, cmap=cmap, x_axis=\"time\", y_axis=\"hz\", ax=ax[0])\n",
        "mesh.set_clim(mindb, maxdb)\n",
        "fig.colorbar(mesh, ax=ax[0], format=\"%+2.fdB\")\n",
        "\n",
        "mesh = librosa.display.specshow(logspec_gen, hop_length=hop_length, sr=sr, cmap=cmap, x_axis=\"time\", y_axis=\"hz\", ax=ax[1])\n",
        "mesh.set_clim(mindb, maxdb)\n",
        "fig.colorbar(mesh, ax=ax[1], format=\"%+2.fdB\")\n",
        "\n",
        "for a in ax:\n",
        "    a.set_xlabel(\"Time [sec]\")\n",
        "    a.set_ylabel(\"Frequency [Hz]\")\n",
        "    \n",
        "ax[0].set_title(\"Spectrogram of natural speech\")\n",
        "ax[1].set_title(\"Spectrogram of generated speech\")\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "print(\"自然音声\")\n",
        "IPython.display.display(Audio(ref_wav, rate=sr))\n",
        "print(\"DNN音声合成\")\n",
        "IPython.display.display(Audio(gen_wav, rate=sr))\n",
        "\n",
        "# 図6-13\n",
        "savefig(\"./fig/dnntts_impl_tts_spec_comp\")"
      ],
      "id": "inclusive-headline",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "expensive-minute"
      },
      "source": [
        "### 評価データに対して音声波形生成"
      ],
      "id": "expensive-minute"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "removed-lotus"
      },
      "source": [
        "#### レシピのstage 5 の実行"
      ],
      "id": "removed-lotus"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "developing-temperature"
      },
      "source": [
        "if run_sh:\n",
        "    ! ./run.sh --stage 6 --stop-stage 6 --duration-model $duration_config_name --acoustic-model $acoustic_config_name \\\n",
        "    --tqdm $run_sh_tqdm "
      ],
      "id": "developing-temperature",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bridal-basin"
      },
      "source": [
        "## 自然音声と合成音声の比較 (bonus)"
      ],
      "id": "bridal-basin"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "relative-shannon"
      },
      "source": [
        "from pathlib import Path\n",
        "from ttslearn.util import load_utt_list\n",
        "\n",
        "with open(\"./downloads/jsut_ver1.1/basic5000/transcript_utf8.txt\") as f:\n",
        "    transcripts = {}\n",
        "    for l in f:\n",
        "        utt_id, script = l.split(\":\")\n",
        "        transcripts[utt_id] = script\n",
        "        \n",
        "eval_list = load_utt_list(\"data/eval.list\")[::-1][:5]\n",
        "\n",
        "for utt_id in eval_list:\n",
        "    # ref file \n",
        "    ref_file = f\"./downloads/jsut_ver1.1/basic5000/wav/{utt_id}.wav\"\n",
        "    _sr, ref_wav = wavfile.read(ref_file)\n",
        "    ref_wav = (ref_wav / 32768.0).astype(np.float64)\n",
        "    ref_wav = librosa.resample(ref_wav, _sr, sr)\n",
        "    \n",
        "    gen_file = f\"exp/jsut_sr16000/synthesis_{duration_config_name}_{acoustic_config_name}/eval/{utt_id}.wav\"\n",
        "    _sr, gen_wav = wavfile.read(gen_file)\n",
        "    print(f\"{utt_id}: {transcripts[utt_id]}\")\n",
        "    print(\"自然音声\")\n",
        "    IPython.display.display(Audio(ref_wav, rate=sr))\n",
        "    print(\"DNN音声合成\")\n",
        "    IPython.display.display(Audio(gen_wav, rate=sr))"
      ],
      "id": "relative-shannon",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "outdoor-senator"
      },
      "source": [
        "フルコンテキストラベルではなく、漢字かな交じり文を入力としたTTSの実装は、`ttslearn.dnntts.tts` モジュールを参照してください。本章の冒頭で示した学習済みモデルを利用したTTSは、そのモジュールを利用しています。"
      ],
      "id": "outdoor-senator"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hundred-shark"
      },
      "source": [
        "## 学習済みモデルのパッケージング (bonus)\n",
        "\n",
        "学習済みモデルを利用したTTSに必要なファイルをすべて単一のディレクトリにまとめます。\n",
        "`ttslearn.dnntts.DNNTTS` クラスには、まとめたディレクトリを指定し、TTSを行う機能が実装されています。"
      ],
      "id": "hundred-shark"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hybrid-examination"
      },
      "source": [
        "### レシピの stage 99 の実行"
      ],
      "id": "hybrid-examination"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "present-snake"
      },
      "source": [
        "if run_sh:\n",
        "    ! ./run.sh --stage 99 --stop-stage 99 --duration-model $duration_config_name --acoustic-model $acoustic_config_name"
      ],
      "id": "present-snake",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "straight-aspect"
      },
      "source": [
        "!ls tts_models/jsut_sr16000_{duration_config_name}_{acoustic_config_name}"
      ],
      "id": "straight-aspect",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "popular-subject"
      },
      "source": [
        "### パッケージングしたモデルを利用したTTS"
      ],
      "id": "popular-subject"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "certified-pattern"
      },
      "source": [
        "from ttslearn.dnntts import DNNTTS\n",
        "\n",
        "# パッケージングしたモデルのパスを指定します\n",
        "model_dir = f\"./tts_models/jsut_sr16000_{duration_config_name}_{acoustic_config_name}\"\n",
        "engine = DNNTTS(model_dir)\n",
        "wav, sr = engine.tts(\"ここまでお読みいただき、ありがとうございました。\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,2))\n",
        "librosa.display.waveplot(wav.astype(np.float32), sr, ax=ax)\n",
        "ax.set_xlabel(\"Time [sec]\")\n",
        "ax.set_ylabel(\"Amplitude\")\n",
        "plt.tight_layout()\n",
        "\n",
        "Audio(wav, rate=sr)"
      ],
      "id": "certified-pattern",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suitable-execution"
      },
      "source": [
        "if is_colab():\n",
        "    from datetime import timedelta\n",
        "    elapsed = (time.time() - start_time)\n",
        "    print(\"所要時間:\", str(timedelta(seconds=elapsed)))"
      ],
      "id": "suitable-execution",
      "execution_count": null,
      "outputs": []
    }
  ]
}