{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "name": "ch08_Recipe-WaveNet.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykato27/Text-to-Speech/blob/main/ch08_Recipe_WaveNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "recovered-superintendent"
      },
      "source": [
        "# 第8章 日本語 WaveNet 音声合成システムの実装\n",
        "\n",
        "Google colabでの実行における推定所要時間: 5時間\n",
        "\n",
        "このノートブックに記載のレシピの設定は、Google Colab上で実行した場合のタイムアウトを避けるため、学習条件を書籍に記載の設定から一部修正していることに注意してください (バッチサイズを減らす等)。\n",
        "参考までに、書籍に記載の条件で、著者 (山本) がレシピを実行した結果を以下で公開しています。\n",
        "\n",
        "- Tensorboard logs: https://tensorboard.dev/experiment/yXyg9qgfQRSGxvil5FA4xw/\n",
        "- expディレクトリ(学習済みモデル、合成音声を含む) : https://drive.google.com/file/d/1cuDxWW0KIUJLzY5Gvk3I_u8VL4bacBq0/view?usp=sharing (135.2 MB)"
      ],
      "id": "recovered-superintendent"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "communist-charter"
      },
      "source": [
        "## 準備"
      ],
      "id": "communist-charter"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "super-terminology"
      },
      "source": [
        "### Google Colabを利用する場合"
      ],
      "id": "super-terminology"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oriental-daisy"
      },
      "source": [
        "Google Colab上でこのノートブックを実行する場合は、メニューの「ランタイム -> ランタイムのタイムの変更」から、「ハードウェア アクセラレータ」を **GPU** に変更してください。"
      ],
      "id": "oriental-daisy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "touched-teach"
      },
      "source": [
        "### Python version"
      ],
      "id": "touched-teach"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prerequisite-living"
      },
      "source": [
        "!python -VV"
      ],
      "id": "prerequisite-living",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gorgeous-friday"
      },
      "source": [
        "### ttslearn のインストール"
      ],
      "id": "gorgeous-friday"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "previous-shannon"
      },
      "source": [
        "%%capture\n",
        "try:\n",
        "    import ttslearn\n",
        "except ImportError:\n",
        "    !pip install ttslearn"
      ],
      "id": "previous-shannon",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "impressed-california"
      },
      "source": [
        "import ttslearn\n",
        "ttslearn.__version__"
      ],
      "id": "impressed-california",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "secure-marker"
      },
      "source": [
        "## 8.1 本章の日本語音声合成システムの実装"
      ],
      "id": "secure-marker"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tight-marker"
      },
      "source": [
        "### 学習済みモデルを用いた音声合成"
      ],
      "id": "tight-marker"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "genetic-beatles"
      },
      "source": [
        "from ttslearn.wavenet import WaveNetTTS\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import Audio\n",
        "\n",
        "engine = WaveNetTTS()\n",
        "wav, sr = engine.tts(\"ウェーブネットにチャレンジしましょう！\", tqdm=tqdm)\n",
        "Audio(wav, rate=sr)"
      ],
      "id": "genetic-beatles",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "explicit-commissioner"
      },
      "source": [
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,2))\n",
        "librosa.display.waveplot(wav.astype(np.float32), sr, ax=ax)\n",
        "ax.set_xlabel(\"Time [sec]\")\n",
        "ax.set_ylabel(\"Amplitude\")\n",
        "plt.tight_layout()"
      ],
      "id": "explicit-commissioner",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "falling-northern"
      },
      "source": [
        "### レシピ実行の前準備"
      ],
      "id": "falling-northern"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "controversial-innocent"
      },
      "source": [
        "%%capture\n",
        "from ttslearn.env import is_colab\n",
        "from os.path import exists\n",
        "\n",
        "# pip install ttslearn ではレシピはインストールされないので、手動でダウンロード\n",
        "if is_colab() and not exists(\"recipes.zip\"):\n",
        "    !curl -LO https://github.com/r9y9/ttslearn/releases/download/v{ttslearn.__version__}/recipes.zip\n",
        "    !unzip -o recipes.zip"
      ],
      "id": "controversial-innocent",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "demanding-arcade"
      },
      "source": [
        "import os\n",
        "# recipeのディレクトリに移動\n",
        "cwd = os.getcwd()\n",
        "if cwd.endswith(\"notebooks\"):\n",
        "    os.chdir(\"../recipes/wavenet/\")\n",
        "elif is_colab():\n",
        "    os.chdir(\"recipes/wavenet/\")"
      ],
      "id": "demanding-arcade",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quantitative-tongue"
      },
      "source": [
        "import time\n",
        "start_time = time.time()"
      ],
      "id": "quantitative-tongue",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "destroyed-radical"
      },
      "source": [
        "### パッケージのインポート"
      ],
      "id": "destroyed-radical"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "accessible-gather"
      },
      "source": [
        "%pylab inline\n",
        "%load_ext autoreload\n",
        "%load_ext tensorboard\n",
        "%autoreload\n",
        "import IPython\n",
        "from IPython.display import Audio\n",
        "import tensorboard as tb\n",
        "import os"
      ],
      "id": "accessible-gather",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "incorporated-spell"
      },
      "source": [
        "# 数値演算\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "# 音声波形の読み込み\n",
        "from scipy.io import wavfile\n",
        "# フルコンテキストラベル、質問ファイルの読み込み\n",
        "from nnmnkwii.io import hts\n",
        "# 音声分析\n",
        "import pyworld\n",
        "# 音声分析、可視化\n",
        "import librosa\n",
        "import librosa.display\n",
        "import pandas as pd\n",
        "# Pythonで学ぶ音声合成\n",
        "import ttslearn"
      ],
      "id": "incorporated-spell",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unnecessary-apartment"
      },
      "source": [
        "# シードの固定\n",
        "from ttslearn.util import init_seed\n",
        "init_seed(773)"
      ],
      "id": "unnecessary-apartment",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "polished-plasma"
      },
      "source": [
        "torch.__version__"
      ],
      "id": "polished-plasma",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hollywood-merchandise"
      },
      "source": [
        "### 描画周りの設定"
      ],
      "id": "hollywood-merchandise"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "contrary-uganda"
      },
      "source": [
        "from ttslearn.notebook import get_cmap, init_plot_style, savefig\n",
        "cmap = get_cmap()\n",
        "init_plot_style()"
      ],
      "id": "contrary-uganda",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cordless-split"
      },
      "source": [
        "### レシピの設定"
      ],
      "id": "cordless-split"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peripheral-baseline"
      },
      "source": [
        "# run.shを利用した学習スクリプトをnotebookから行いたい場合は、True\n",
        "# google colab の場合は、True とします\n",
        "# ローカル環境の場合、run.sh をターミナルから実行することを推奨します。\n",
        "# その場合、このノートブックは可視化・学習済みモデルのテストのために利用します。\n",
        "run_sh = is_colab()\n",
        "\n",
        "# 注意: WaveNetを利用した評価データに対する音声生成は時間がかかることに注意\n",
        "run_stage8 = True\n",
        "\n",
        "# run.sh経由で実行するスクリプトのtqdm\n",
        "run_sh_tqdm = \"none\"\n",
        "\n",
        "# CUDA\n",
        "# NOTE: run.shの引数として渡すので、boolではなく文字列で定義しています\n",
        "cudnn_benchmark = \"true\"\n",
        "cudnn_deterministic = \"false\"\n",
        "\n",
        "# 特徴抽出時の並列処理のジョブ数\n",
        "n_jobs = os.cpu_count()//2\n",
        "\n",
        "# 継続長モデルの設定ファイル名\n",
        "duration_config_name=\"duration_rnn\"\n",
        "# 音響モデルの設定ファイル名\n",
        "logf0_config_name=\"logf0_rnn\"\n",
        "# WaveNetの設定ファイル名\n",
        "wavenet_config_name=\"wavenet_sr16k_mulaw256\"\n",
        "\n",
        "# 継続長モデル & 対数F0予測モデルの学習におけるバッチサイズ\n",
        "dnntts_batch_size = 32\n",
        "# 継続長モデル & 対数F0予測モデルの学習におけるエポック数\n",
        "# 注意: 計算時間を少なくするために、少なく設定しています。品質を向上させるためには、30 ~ 50 のエポック数を試してみてください。\n",
        "dnntts_nepochs = 5\n",
        "\n",
        "# WaveNet学習におけるバッチサイズ\n",
        "# 推奨バッチサイズ: 8以上\n",
        "# 動作確認のため、小さな値に設定しています\n",
        "wavenet_batch_size = 4\n",
        "# WavaNetの学習イテレーション数\n",
        "# 注意: 十分な品質を得るために必要な値: 300k ~ 500k steps\n",
        "wavenet_max_train_steps = 50000\n",
        "\n",
        "# 音声生成を行う発話数\n",
        "# WaveNetの推論は時間がかかるので、ノートブックで表示する5つのみ生成する\n",
        "num_eval_utts = 5\n",
        "\n",
        "# ノートブックで利用するテスト用の発話（学習データ、評価データ）\n",
        "train_utt = \"BASIC5000_0001\"\n",
        "test_utt = \"BASIC5000_5000\""
      ],
      "id": "peripheral-baseline",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "under-release"
      },
      "source": [
        "### Tensorboard によるログの可視化"
      ],
      "id": "under-release"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "formal-spiritual"
      },
      "source": [
        "# ノートブック上から tensorboard のログを確認する場合、次の行を有効にしてください\n",
        "if is_colab():\n",
        "    %tensorboard --logdir tensorboard/"
      ],
      "id": "formal-spiritual",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "violent-hartford"
      },
      "source": [
        "## プログラム実装の前準備"
      ],
      "id": "violent-hartford"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "metric-rehabilitation"
      },
      "source": [
        "### stage -1: コーパスのダウンロード"
      ],
      "id": "metric-rehabilitation"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dutch-turner"
      },
      "source": [
        "if is_colab():\n",
        "    ! ./run.sh --stage -1 --stop-stage -1"
      ],
      "id": "dutch-turner",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "measured-particular"
      },
      "source": [
        "### Stage 0: 学習/検証/評価データの分割"
      ],
      "id": "measured-particular"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "black-martin"
      },
      "source": [
        "if run_sh:\n",
        "    ! ./run.sh --stage 0 --stop-stage 0"
      ],
      "id": "black-martin",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "supposed-adolescent"
      },
      "source": [
        "! ls data/"
      ],
      "id": "supposed-adolescent",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alike-uruguay"
      },
      "source": [
        "! head data/dev.list"
      ],
      "id": "alike-uruguay",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "friendly-ideal"
      },
      "source": [
        "## 8.2 データの前処理"
      ],
      "id": "friendly-ideal"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "incorrect-express"
      },
      "source": [
        "### 継続長モデルのための前処理\n",
        "\n",
        "バッチ処理を行うコマンドラインプログラムは、 `recipes/dnntts/preprocess_duration.py` を参照してください。"
      ],
      "id": "incorrect-express"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "normal-delay"
      },
      "source": [
        "if run_sh:\n",
        "    ! ./run.sh --stage 1 --stop-stage 1 --n-jobs $n_jobs "
      ],
      "id": "normal-delay",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fatal-transmission"
      },
      "source": [
        "### 対数 F0 予測モデルのための前処理"
      ],
      "id": "fatal-transmission"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arranged-distributor"
      },
      "source": [
        "#### 対数F0 + 有声/無声フラグの計算"
      ],
      "id": "arranged-distributor"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imperial-reynolds"
      },
      "source": [
        "from nnmnkwii.preprocessing import delta_features\n",
        "from nnmnkwii.preprocessing.f0 import interp1d\n",
        "from ttslearn.dsp import f0_to_lf0\n",
        "\n",
        "def world_log_f0_vuv(x, sr):\n",
        "    f0, timeaxis = pyworld.dio(x, sr)\n",
        "    f0 = pyworld.stonemask(x, f0, timeaxis, sr)\n",
        "    vuv = (f0 > 0).astype(np.float32)\n",
        "\n",
        "    # 連続対数基本周波数\n",
        "    lf0 = f0_to_lf0(f0)\n",
        "    lf0 = interp1d(lf0)\n",
        "\n",
        "    # 連続基本周波数と有声/無声フラグを2次元の行列の形にしておく\n",
        "    lf0 = lf0[:, np.newaxis] if len(lf0.shape) == 1 else lf0\n",
        "    vuv = vuv[:, np.newaxis] if len(vuv.shape) == 1 else vuv\n",
        "\n",
        "    # 動的特徴量の計算\n",
        "    windows = [\n",
        "        [1.0],  # 静的特徴量に対する窓\n",
        "        [-0.5, 0.0, 0.5],  # 1 次動的特徴量に対する窓\n",
        "        [1.0, -2.0, 1.0],  # 2 次動的特徴量に対する窓\n",
        "    ]\n",
        "    lf0 = delta_features(lf0, windows)\n",
        "\n",
        "    # すべての特徴量を結合\n",
        "    feats = np.hstack([lf0, vuv]).astype(np.float32)\n",
        "\n",
        "    return feats"
      ],
      "id": "imperial-reynolds",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suitable-affect"
      },
      "source": [
        "#### 対数F0 + 有声/無声フラグの可視化"
      ],
      "id": "suitable-affect"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "injured-cassette"
      },
      "source": [
        "from ttslearn.dsp import lf0_to_f0\n",
        "\n",
        "sr = 16000\n",
        "_sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
        "x = (x / 32768).astype(np.float64)\n",
        "x = librosa.resample(x, _sr, sr)\n",
        "\n",
        "out_feats = world_log_f0_vuv(x, sr)\n",
        "lf0 = out_feats[:, 0]\n",
        "vuv = out_feats[:, -1]\n",
        "\n",
        "timeaxis = librosa.frames_to_time(np.arange(len(lf0)), sr, int(0.005 * sr))\n",
        "\n",
        "fig, ax = plt.subplots(3,1, figsize=(8,6))\n",
        "ax[0].set_title(\"Input waveform\")\n",
        "ax[1].set_title(\"Continuous log F0\")\n",
        "ax[2].set_title(\"V/UV\")\n",
        "\n",
        "librosa.display.waveplot(x, sr, x_axis=\"time\", ax=ax[0])\n",
        "ax[1].plot(timeaxis, lf0, linewidth=2)\n",
        "ax[2].plot(timeaxis, vuv, linewidth=2)\n",
        "\n",
        "for a in ax:\n",
        "    a.set_xlabel(\"Time [sec]\")\n",
        "    a.set_xlim(0, len(x)/sr)\n",
        "    a.set_xticks(np.arange(0, 3.5, 0.5))\n",
        "    a.xaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n",
        "\n",
        "ax[0].set_ylabel(\"Amplitude\")\n",
        "ax[1].set_ylabel(\"Logarithmic frequency\")\n",
        "ax[2].set_ylabel(\"Binary value\")\n",
        "\n",
        "plt.tight_layout()"
      ],
      "id": "injured-cassette",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "injured-worse"
      },
      "source": [
        "#### 1発話に対する前処理"
      ],
      "id": "injured-worse"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "white-motor"
      },
      "source": [
        "from nnmnkwii.frontend import merlin as fe\n",
        "\n",
        "# HTS 形式の質問ファイルを読み込み\n",
        "binary_dict, numeric_dict = hts.load_question_set(ttslearn.util.example_qst_file())\n",
        "\n",
        "# フルコンテキストラベルの読み込み\n",
        "labels = hts.load(ttslearn.util.example_label_file())\n",
        "\n",
        "# フレーム単位の言語特徴量を抽出\n",
        "in_feats = fe.linguistic_features(\n",
        "    labels,\n",
        "    binary_dict,\n",
        "    numeric_dict,\n",
        "    add_frame_features=True,\n",
        "    subphone_features=\"coarse_coding\",\n",
        ")\n",
        "\n",
        "# 音声ファイルの読み込み\n",
        "sr = 16000\n",
        "_sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
        "x = (x / 32768).astype(np.float64)\n",
        "x = librosa.resample(x, _sr, sr)\n",
        "\n",
        "# 連続対数基本周波数と有声/無声フラグを結合した特徴量の計算\n",
        "out_feats = world_log_f0_vuv(x.astype(np.float64), sr)\n",
        "\n",
        "# フレーム数の調整\n",
        "minL = min(in_feats.shape[0], out_feats.shape[0])\n",
        "in_feats, out_feats = in_feats[:minL], out_feats[:minL]\n",
        "\n",
        "# 冒頭と末尾の非音声区間の長さを調整\n",
        "assert \"sil\" in labels.contexts[0] and \"sil\" in labels.contexts[-1]\n",
        "start_frame = int(labels.start_times[1] / 50000)\n",
        "end_frame = int(labels.end_times[-2] / 50000)\n",
        "\n",
        "# 冒頭：50 ミリ秒、末尾：100 ミリ秒\n",
        "start_frame = max(0, start_frame - int(0.050 / 0.005))\n",
        "end_frame = min(minL, end_frame + int(0.100 / 0.005))\n",
        "\n",
        "in_feats = in_feats[start_frame:end_frame]\n",
        "out_feats = out_feats[start_frame:end_frame]"
      ],
      "id": "white-motor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "innocent-nightmare"
      },
      "source": [
        "print (\"入力特徴量のサイズ:\", in_feats.shape)\n",
        "print (\"出力特徴量のサイズ:\", out_feats.shape)"
      ],
      "id": "innocent-nightmare",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coupled-midnight"
      },
      "source": [
        "#### レシピの stage 2 の実行\n",
        "\n",
        "上記の処理を行うバッチ処理のプログラムは、`preprocess_logf0.py` にあります。"
      ],
      "id": "coupled-midnight"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "handed-metadata"
      },
      "source": [
        "if run_sh:\n",
        "    ! ./run.sh --stage 2 --stop-stage 2 --n-jobs $n_jobs"
      ],
      "id": "handed-metadata",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "advisory-attack"
      },
      "source": [
        "### WaveNet のための前処理"
      ],
      "id": "advisory-attack"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "convenient-spectacular"
      },
      "source": [
        "#### 1発話に対する前処理"
      ],
      "id": "convenient-spectacular"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sacred-knowing"
      },
      "source": [
        "from ttslearn.dsp import mulaw_quantize\n",
        "from ttslearn.dsp import world_log_f0_vuv\n",
        "\n",
        "# HTS 形式の質問ファイルを読み込み\n",
        "binary_dict, numeric_dict = hts.load_question_set(ttslearn.util.example_qst_file())\n",
        "\n",
        "# フルコンテキストラベルの読み込み\n",
        "labels = hts.load(ttslearn.util.example_label_file())\n",
        "\n",
        "# フレーム単位の言語特徴量の抽出\n",
        "in_feats = fe.linguistic_features(\n",
        "    labels,\n",
        "    binary_dict,\n",
        "    numeric_dict,\n",
        "    add_frame_features=True,\n",
        "    subphone_features=\"coarse_coding\",\n",
        ")\n",
        "\n",
        "# 音声ファイルの読み込み\n",
        "sr = 16000\n",
        "_sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
        "x = (x / 32768).astype(np.float64)\n",
        "x = librosa.resample(x, _sr, sr)\n",
        "\n",
        "# 連続対数基本周波数と有声/無声フラグを結合した特徴量の計算\n",
        "log_f0_vuv = world_log_f0_vuv(x.astype(np.float64), sr)\n",
        "\n",
        "# フレーム数の調整\n",
        "minL = min(in_feats.shape[0], log_f0_vuv.shape[0])\n",
        "in_feats, log_f0_vuv = in_feats[:minL], log_f0_vuv[:minL]\n",
        "\n",
        "# 冒頭と末尾の非音声区間の長さを調整\n",
        "assert \"sil\" in labels.contexts[0] and \"sil\" in labels.contexts[-1]\n",
        "start_frame = int(labels.start_times[1] / 50000)\n",
        "end_frame = int(labels.end_times[-2] / 50000)\n",
        "\n",
        "# 冒頭：50 ミリ秒、末尾：100 ミリ秒\n",
        "start_frame = max(0, start_frame - int(0.050 / 0.005))\n",
        "end_frame = min(minL, end_frame + int(0.100 / 0.005))\n",
        "\n",
        "in_feats = in_feats[start_frame:end_frame]\n",
        "log_f0_vuv = log_f0_vuv[start_frame:end_frame]\n",
        "\n",
        "# 言語特徴量と連続対数基本周波数を結合\n",
        "in_feats = np.hstack([in_feats, log_f0_vuv])\n",
        "\n",
        "# 時間領域で音声の長さを調整\n",
        "x = x[int(start_frame * 0.005 * sr) :]\n",
        "length = int(sr * 0.005) * in_feats.shape[0]\n",
        "x = pad_1d(x, length) if len(x) < length else x[:length]\n",
        "\n",
        "# mu-law 量子化\n",
        "quantized_x = mulaw_quantize(x)\n",
        "\n",
        "# 条件付け特徴量のアップサンプリングを考えるため、\n",
        "# 音声波形の長さはフレームシフトで割り切れることを確認\n",
        "assert len(quantized_x) % int(sr * 0.005) == 0"
      ],
      "id": "sacred-knowing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "preceding-bermuda"
      },
      "source": [
        "print (\"条件付け特徴量のサイズ:\", in_feats.shape)\n",
        "print (\"量子化された音声波形のサイズ:\", quantized_x.shape)"
      ],
      "id": "preceding-bermuda",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "contrary-joint"
      },
      "source": [
        "timeaxis = np.arange(len(x)) / sr\n",
        "\n",
        "fig, ax = subplots(2,1, figsize=(8,4))\n",
        "ax[0].set_title(\"Input waveform\")\n",
        "ax[1].set_title(\"Output waveform after mu-law\")\n",
        "\n",
        "for a in ax:\n",
        "    a.set_xlabel(\"Time [sec]\")\n",
        "    a.set_ylabel(\"Amplitude\")\n",
        "ax[0].plot(timeaxis, x)\n",
        "ax[1].plot(timeaxis, quantized_x)\n",
        "plt.tight_layout()"
      ],
      "id": "contrary-joint",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clinical-fifteen"
      },
      "source": [
        "#### レシピの stage 3 の実行\n",
        "\n",
        "上記の処理を行うバッチ処理のプログラムは、`preprocess_wavenet.py` にあります。"
      ],
      "id": "clinical-fifteen"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "simplified-jason"
      },
      "source": [
        "if run_sh:\n",
        "    ! ./run.sh --stage 3 --stop-stage 3 --n-jobs $n_jobs"
      ],
      "id": "simplified-jason",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "generic-uniform"
      },
      "source": [
        "### 特徴量の正規化"
      ],
      "id": "generic-uniform"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quiet-johnson"
      },
      "source": [
        "正規化のための統計量を計算するコマンドラインプログラムは、 `recipes/common/fit_scaler.py` を参照してください。また、正規化を行うコマンドラインプログラムは、 `recipes/common/preprocess_normalize.py` を参照してください。"
      ],
      "id": "quiet-johnson"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bigger-discretion"
      },
      "source": [
        "#### レシピの stage 4 の実行"
      ],
      "id": "bigger-discretion"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "regulated-distributor"
      },
      "source": [
        "if run_sh:\n",
        "    ! ./run.sh --stage 4 --stop-stage 4 --n-jobs $n_jobs"
      ],
      "id": "regulated-distributor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "short-college"
      },
      "source": [
        "#### 正規化の処理の結果の確認"
      ],
      "id": "short-college"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "combined-tamil"
      },
      "source": [
        "# 言語特徴量の正規化前後\n",
        "in_feats = np.load(f\"dump/jsut_sr16000/org/train/in_logf0/{train_utt}-feats.npy\")\n",
        "in_feats_norm = np.load(f\"dump/jsut_sr16000/norm/train/in_logf0/{train_utt}-feats.npy\")\n",
        "fig, ax = plt.subplots(2, 1, figsize=(8,6))\n",
        "ax[0].set_title(\"Linguistic features (before normalization)\")\n",
        "ax[1].set_title(\"Linguistic features (after normalization)\")\n",
        "hop_length = int(sr * 0.005)\n",
        "mesh = librosa.display.specshow(\n",
        "    in_feats.T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"frames\", ax=ax[0], cmap=cmap)\n",
        "fig.colorbar(mesh, ax=ax[0])\n",
        "mesh = librosa.display.specshow(\n",
        "    in_feats_norm.T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"frames\",ax=ax[1], cmap=cmap)\n",
        "# NOTE: 実際には [-4, 4]の範囲外の値もありますが、視認性のために [-4, 4]に設定します\n",
        "mesh.set_clim(-4, 4)\n",
        "fig.colorbar(mesh, ax=ax[1])\n",
        "\n",
        "for a in ax:\n",
        "    a.set_xlabel(\"Time [sec]\")\n",
        "    a.set_ylabel(\"Context\")\n",
        "    # 末尾の非音声区間を除く\n",
        "    a.set_xlim(0, 2.55)\n",
        "    \n",
        "plt.tight_layout()"
      ],
      "id": "combined-tamil",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pressing-beverage"
      },
      "source": [
        "## 8.3 継続長モデルの学習"
      ],
      "id": "pressing-beverage"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "behind-force"
      },
      "source": [
        "### 継続長モデルの設定ファイル"
      ],
      "id": "behind-force"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hollywood-afternoon"
      },
      "source": [
        "! cat conf/train_dnntts/model/{duration_config_name}.yaml"
      ],
      "id": "hollywood-afternoon",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adequate-portuguese"
      },
      "source": [
        "### 継続長モデルのインスタンス化"
      ],
      "id": "adequate-portuguese"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coastal-organizer"
      },
      "source": [
        "import hydra\n",
        "from omegaconf import OmegaConf\n",
        "hydra.utils.instantiate(OmegaConf.load(f\"conf/train_dnntts/model/{duration_config_name}.yaml\").netG)"
      ],
      "id": "coastal-organizer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "extensive-boxing"
      },
      "source": [
        "### レシピの stage 5 の実行"
      ],
      "id": "extensive-boxing"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bibliographic-amino"
      },
      "source": [
        "if run_sh:\n",
        "    ! ./run.sh --stage 5 --stop-stage 5 --duration-model $duration_config_name \\\n",
        "        --tqdm $run_sh_tqdm --dnntts-data-batch-size $dnntts_batch_size --dnntts-train-nepochs $dnntts_nepochs \\\n",
        "        --cudnn-benchmark $cudnn_benchmark --cudnn-deterministic $cudnn_deterministic"
      ],
      "id": "bibliographic-amino",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fixed-plaintiff"
      },
      "source": [
        "### 損失関数の値の推移\n",
        "\n",
        "著者による実験結果です。Tensorboardのログは https://tensorboard.dev/ にアップロードされています。\n",
        "ログデータを`tensorboard` パッケージを利用してダウンロードします。\n",
        "\n",
        "https://tensorboard.dev/experiment/yXyg9qgfQRSGxvil5FA4xw/"
      ],
      "id": "fixed-plaintiff"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "outdoor-vegetation"
      },
      "source": [
        "if exists(\"tensorboard/all_log.csv\"):\n",
        "    df = pd.read_csv(\"tensorboard/all_log.csv\")\n",
        "else:\n",
        "    experiment_id = \"yXyg9qgfQRSGxvil5FA4xw\"\n",
        "    experiment = tb.data.experimental.ExperimentFromDev(experiment_id)\n",
        "    df = experiment.get_scalars() \n",
        "    df.to_csv(\"tensorboard/all_log.csv\", index=False)\n",
        "df[\"run\"].unique()"
      ],
      "id": "outdoor-vegetation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "framed-culture"
      },
      "source": [
        "duration_loss = df[df.run.str.contains(\"duration\")]\n",
        "\n",
        "duration_train_loss = duration_loss[duration_loss.tag.str.contains(\"Loss/train\")]\n",
        "duration_dev_loss = duration_loss[duration_loss.tag.str.contains(\"Loss/dev\")]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "ax.plot(duration_train_loss[\"step\"], duration_train_loss[\"value\"], label=\"Train\")\n",
        "ax.plot(duration_dev_loss[\"step\"], duration_dev_loss[\"value\"], \"--\", label=\"Dev\")\n",
        "ax.set_xlabel(\"Epoch\")\n",
        "ax.set_ylabel(\"Epoch loss\")\n",
        "plt.legend()\n",
        "\n",
        "# 図8-3\n",
        "savefig(\"fig/wavenet_impl_duration_loss\")"
      ],
      "id": "framed-culture",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "approved-consultation"
      },
      "source": [
        "## 8.4 対数 F0 予測モデルの学習"
      ],
      "id": "approved-consultation"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "magnetic-firmware"
      },
      "source": [
        "### 対数 F0 予測モデルの設定ファイル"
      ],
      "id": "magnetic-firmware"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "multiple-sudan"
      },
      "source": [
        "! cat conf/train_dnntts/model/{logf0_config_name}.yaml"
      ],
      "id": "multiple-sudan",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "monthly-electric"
      },
      "source": [
        "### 対数 F0 予測モデルのインスタンス化"
      ],
      "id": "monthly-electric"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "silent-jones"
      },
      "source": [
        "import hydra\n",
        "from omegaconf import OmegaConf\n",
        "hydra.utils.instantiate(OmegaConf.load(f\"conf/train_dnntts/model/{logf0_config_name}.yaml\").netG)"
      ],
      "id": "silent-jones",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "photographic-present"
      },
      "source": [
        "### レシピの stage 6 の実行"
      ],
      "id": "photographic-present"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "curious-reporter"
      },
      "source": [
        "if run_sh:\n",
        "    ! ./run.sh --stage 6 --stop-stage 6 --logf0-model $logf0_config_name \\\n",
        "        --tqdm $run_sh_tqdm --dnntts-data-batch-size $dnntts_batch_size --dnntts-train-nepochs $dnntts_nepochs \\\n",
        "        --cudnn-benchmark $cudnn_benchmark --cudnn-deterministic $cudnn_deterministic"
      ],
      "id": "curious-reporter",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unavailable-biotechnology"
      },
      "source": [
        "### 損失関数の値の推移"
      ],
      "id": "unavailable-biotechnology"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saving-maintenance"
      },
      "source": [
        "logf0_loss = df[df.run.str.contains(\"logf0\")]\n",
        "\n",
        "logf0_train_loss = logf0_loss[logf0_loss.tag.str.contains(\"Loss/train\")]\n",
        "logf0_dev_loss = logf0_loss[logf0_loss.tag.str.contains(\"Loss/dev\")]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "ax.plot(logf0_train_loss[\"step\"], logf0_train_loss[\"value\"], label=\"Train\")\n",
        "ax.plot(logf0_dev_loss[\"step\"], logf0_dev_loss[\"value\"], \"--\", label=\"Dev\")\n",
        "ax.set_xlabel(\"Epoch\")\n",
        "ax.set_ylabel(\"Epoch loss\")\n",
        "plt.legend()\n",
        "\n",
        "# 図8-4\n",
        "savefig(\"fig/wavenet_impl_logf0_loss\")"
      ],
      "id": "saving-maintenance",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "concerned-mainstream"
      },
      "source": [
        "## 8.5 WaveNet の学習スクリプトの実装"
      ],
      "id": "concerned-mainstream"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "figured-small"
      },
      "source": [
        "### DataLoaderの実装"
      ],
      "id": "figured-small"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exciting-reducing"
      },
      "source": [
        "#### collate_fn の実装"
      ],
      "id": "exciting-reducing"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bulgarian-morgan"
      },
      "source": [
        "def collate_fn_wavenet(batch, max_time_frames=100, hop_size=80, aux_context_window=2):\n",
        "    max_time_steps = max_time_frames * hop_size\n",
        "\n",
        "    xs, cs = [b[1] for b in batch], [b[0] for b in batch]\n",
        "\n",
        "    # 条件付け特徴量の開始位置をランダム抽出した後、それに相当する短い音声波形を切り出します\n",
        "    c_lengths = [len(c) for c in cs]\n",
        "    start_frames = np.array(\n",
        "        [\n",
        "            np.random.randint(\n",
        "                aux_context_window, cl - aux_context_window - max_time_frames\n",
        "            )\n",
        "            for cl in c_lengths\n",
        "        ]\n",
        "    )\n",
        "    x_starts = start_frames * hop_size\n",
        "    x_ends = x_starts + max_time_steps\n",
        "    c_starts = start_frames - aux_context_window\n",
        "    c_ends = start_frames + max_time_frames + aux_context_window\n",
        "    x_batch = [x[s:e] for x, s, e in zip(xs, x_starts, x_ends)]\n",
        "    c_batch = [c[s:e] for c, s, e in zip(cs, c_starts, c_ends)]\n",
        "\n",
        "    # numpy.ndarray のリスト型から torch.Tensor 型に変換します\n",
        "    x_batch = torch.tensor(x_batch, dtype=torch.long)  # (B, T)\n",
        "    c_batch = torch.tensor(c_batch, dtype=torch.float).transpose(2, 1)  # (B, C, T')\n",
        "\n",
        "    return x_batch, c_batch"
      ],
      "id": "bulgarian-morgan",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "engaging-princeton"
      },
      "source": [
        "#### DataLoader の利用例"
      ],
      "id": "engaging-princeton"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "informal-crystal"
      },
      "source": [
        "from pathlib import Path\n",
        "from ttslearn.train_util import Dataset\n",
        "from functools import partial\n",
        "\n",
        "in_paths = sorted(Path(\"./dump/jsut_sr16000/norm/dev/in_wavenet/\").glob(\"*.npy\"))\n",
        "out_paths = sorted(Path(\"./dump/jsut_sr16000/org/dev/out_wavenet/\").glob(\"*.npy\"))\n",
        "\n",
        "dataset = Dataset(in_paths, out_paths)\n",
        "collate_fn = partial(collate_fn_wavenet, max_time_frames=100, hop_size=80, aux_context_window=0)\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=8, collate_fn=collate_fn, num_workers=0)\n",
        "\n",
        "wavs, feats = next(iter(data_loader))\n",
        "\n",
        "print(\"音声波形のサイズ:\", tuple(wavs.shape))\n",
        "print(\"条件付け特徴量のサイズ:\", tuple(feats.shape))"
      ],
      "id": "informal-crystal",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stunning-differential"
      },
      "source": [
        "#### ミニバッチの可視化"
      ],
      "id": "stunning-differential"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "virgin-serial"
      },
      "source": [
        "from ttslearn.dsp import inv_mulaw_quantize\n",
        "\n",
        "fig, ax = plt.subplots(len(wavs), 1, figsize=(8,10), sharex=True, sharey=True)\n",
        "for n in range(len(wavs)):\n",
        "    x = wavs[n].data.numpy()\n",
        "    x = inv_mulaw_quantize(x, 255)\n",
        "    ax[n].plot(x)\n",
        "\n",
        "ax[-1].set_xlabel(\"Time [sample]\")\n",
        "for a in ax:\n",
        "    a.set_ylabel(\"Amplitude\")\n",
        "plt.tight_layout()\n",
        "\n",
        "# 図8-5\n",
        "savefig(\"fig/wavenet_impl_minibatch\")"
      ],
      "id": "virgin-serial",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "practical-array"
      },
      "source": [
        "### 簡易的な学習スクリプトの実装"
      ],
      "id": "practical-array"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "continuing-eagle"
      },
      "source": [
        "#### モデルパラメータの指数移動平均"
      ],
      "id": "continuing-eagle"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opening-array"
      },
      "source": [
        "def moving_average_(model, model_test, beta=0.9999):\n",
        "    for param, param_test in zip(model.parameters(), model_test.parameters()):\n",
        "        param_test.data = torch.lerp(param.data, param_test.data, beta)"
      ],
      "id": "opening-array",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guided-billy"
      },
      "source": [
        "#### 学習の前準備"
      ],
      "id": "guided-billy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "romantic-statistics"
      },
      "source": [
        "from ttslearn.wavenet import WaveNet\n",
        "from torch import optim\n",
        "\n",
        "# 動作確認用：層の数を減らした小さなWaveNet\n",
        "ToyWaveNet = partial(WaveNet, out_channels=256, layers=2, stacks=1, kernel_size=2, cin_channels=333)\n",
        "\n",
        "model = ToyWaveNet()\n",
        "# モデルパラメータの指数移動平均\n",
        "model_ema = ToyWaveNet()\n",
        "model_ema.load_state_dict(model.state_dict())\n",
        "\n",
        "# lr は学習率を表します\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# gamma は学習率の減衰係数を表します\n",
        "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, gamma=0.5, step_size=100000)"
      ],
      "id": "romantic-statistics",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "relative-speech"
      },
      "source": [
        "#### 学習ループの実装"
      ],
      "id": "relative-speech"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "premier-participation"
      },
      "source": [
        "# DataLoader を用いたミニバッチの作成: ミニバッチ毎に処理を行う\n",
        "for x, c in data_loader:\n",
        "    # 順伝播の計算\n",
        "    x_hat = model(x, c)\n",
        "    # 負の対数尤度の計算\n",
        "    loss = nn.CrossEntropyLoss()(x_hat[:, :, :-1], x[:, 1:]).mean()\n",
        "    # 損失の値を出力\n",
        "    print(loss.item())\n",
        "    # optimizer に蓄積された勾配をリセット\n",
        "    optimizer.zero_grad()\n",
        "    # 誤差の逆伝播の計算\n",
        "    loss.backward()\n",
        "    # パラメータの更新\n",
        "    optimizer.step()\n",
        "    # 移動指数平均の計算\n",
        "    moving_average_(model, model_ema)\n",
        "    # 学習率スケジューラの更新\n",
        "    lr_scheduler.step()"
      ],
      "id": "premier-participation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waiting-medium"
      },
      "source": [
        "### 実用的な学習スクリプトの実装"
      ],
      "id": "waiting-medium"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "charitable-steps"
      },
      "source": [
        "`train_wavenet.py` を参照してください。"
      ],
      "id": "charitable-steps"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "figured-kenya"
      },
      "source": [
        "## 8.6 WaveNet の学習"
      ],
      "id": "figured-kenya"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weighted-computer"
      },
      "source": [
        "### WaveNet の設定ファイル"
      ],
      "id": "weighted-computer"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geographic-greece"
      },
      "source": [
        "! cat conf/train_wavenet/model/{wavenet_config_name}.yaml"
      ],
      "id": "geographic-greece",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "technical-andrew"
      },
      "source": [
        "### WaveNet のインスタンス化"
      ],
      "id": "technical-andrew"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "consecutive-triple"
      },
      "source": [
        "import hydra\n",
        "from omegaconf import OmegaConf\n",
        "# WaveNet の 30層 すべてを表示すると長くなるため、ここでは省略します。\n",
        "# hydra.utils.instantiate(OmegaConf.load(f\"./conf/train_wavenet/model/{wavenet_config_name}.yaml\")[\"netG\"])"
      ],
      "id": "consecutive-triple",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reported-while"
      },
      "source": [
        "### レシピの stage 7 の実行"
      ],
      "id": "reported-while"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "creative-negative"
      },
      "source": [
        "if run_sh:\n",
        "    ! ./run.sh --stage 7 --stop-stage 7 --wavenet-model $wavenet_config_name \\\n",
        "        --tqdm $run_sh_tqdm --wavenet-data-batch-size $wavenet_batch_size --wavenet-train-max-train-steps $wavenet_max_train_steps \\\n",
        "        --cudnn-benchmark $cudnn_benchmark --cudnn-deterministic $cudnn_deterministic"
      ],
      "id": "creative-negative",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elect-advertiser"
      },
      "source": [
        "### 損失関数の値の推移"
      ],
      "id": "elect-advertiser"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "functioning-popularity"
      },
      "source": [
        "wavenet_loss = df[df.run.str.contains(\"wavenet\")]\n",
        "\n",
        "wavenet_train_loss = wavenet_loss[wavenet_loss.tag.str.contains(\"Loss/train\")]\n",
        "wavenet_dev_loss = wavenet_loss[wavenet_loss.tag.str.contains(\"Loss/dev\")]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "ax.plot(wavenet_train_loss[\"step\"], wavenet_train_loss[\"value\"], label=\"Train\")\n",
        "ax.plot(wavenet_dev_loss[\"step\"], wavenet_dev_loss[\"value\"], \"--\", label=\"Dev\")\n",
        "ax.set_xlabel(\"Epoch\")\n",
        "ax.set_ylabel(\"Epoch loss\")\n",
        "ax.set_ylim(1.7, 2.2)\n",
        "plt.legend()\n",
        "\n",
        "# 図8-6\n",
        "savefig(\"fig/wavenet_impl_wavenet_loss\")"
      ],
      "id": "functioning-popularity",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "interior-sweden"
      },
      "source": [
        "## 8.7 学習済みモデルを用いてテキストから音声を合成"
      ],
      "id": "interior-sweden"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "handed-blanket"
      },
      "source": [
        "### 学習済みモデルの読み込み"
      ],
      "id": "handed-blanket"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alleged-computer"
      },
      "source": [
        "import joblib\n",
        "device = torch.device(\"cpu\")"
      ],
      "id": "alleged-computer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "insured-bhutan"
      },
      "source": [
        "#### 継続長モデルの読み込み"
      ],
      "id": "insured-bhutan"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foster-ridge"
      },
      "source": [
        "duration_config = OmegaConf.load(f\"exp/jsut_sr16000/{duration_config_name}/model.yaml\")\n",
        "duration_model = hydra.utils.instantiate(duration_config.netG)\n",
        "checkpoint = torch.load(f\"exp/jsut_sr16000/{duration_config_name}/latest.pth\", map_location=device)\n",
        "duration_model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "duration_model.eval();"
      ],
      "id": "foster-ridge",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stopped-hormone"
      },
      "source": [
        "#### 対数F0予測モデルの読み込み"
      ],
      "id": "stopped-hormone"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "compliant-factor"
      },
      "source": [
        "logf0_config = OmegaConf.load(f\"exp/jsut_sr16000/{logf0_config_name}/model.yaml\")\n",
        "logf0_model = hydra.utils.instantiate(logf0_config.netG)\n",
        "checkpoint = torch.load(f\"exp/jsut_sr16000/{logf0_config_name}/latest.pth\", map_location=device)\n",
        "logf0_model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "logf0_model.eval();"
      ],
      "id": "compliant-factor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "expired-passion"
      },
      "source": [
        "#### WaveNetの読み込み"
      ],
      "id": "expired-passion"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elegant-enhancement"
      },
      "source": [
        "wavenet_config = OmegaConf.load(f\"exp/jsut_sr16000/{wavenet_config_name}/model.yaml\")\n",
        "wavenet_model = hydra.utils.instantiate(wavenet_config.netG)\n",
        "checkpoint = torch.load(f\"exp/jsut_sr16000/{wavenet_config_name}/latest_ema.pth\", map_location=device)\n",
        "wavenet_model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "# weight normalization は推論時には不要なため除く\n",
        "wavenet_model.remove_weight_norm_()\n",
        "wavenet_model.eval();"
      ],
      "id": "elegant-enhancement",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "becoming-extreme"
      },
      "source": [
        "#### 統計量の読み込み"
      ],
      "id": "becoming-extreme"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "objective-sussex"
      },
      "source": [
        "duration_in_scaler = joblib.load(\"./dump/jsut_sr16000/norm/in_duration_scaler.joblib\")\n",
        "duration_out_scaler = joblib.load(\"./dump/jsut_sr16000/norm/out_duration_scaler.joblib\")\n",
        "logf0_in_scaler = joblib.load(\"./dump/jsut_sr16000/norm/in_logf0_scaler.joblib\")\n",
        "logf0_out_scaler = joblib.load(\"./dump/jsut_sr16000/norm/out_logf0_scaler.joblib\")\n",
        "wavenet_in_scaler = joblib.load(\"./dump/jsut_sr16000/norm/in_wavenet_scaler.joblib\")"
      ],
      "id": "objective-sussex",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cross-favorite"
      },
      "source": [
        "### 音素継続長の予測"
      ],
      "id": "cross-favorite"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "advance-visibility"
      },
      "source": [
        "from ttslearn.util import lab2phonemes, find_lab, find_feats\n",
        "from ttslearn.dnntts.gen import predict_duration\n",
        "\n",
        "labels = hts.load(find_lab(\"downloads/jsut_ver1.1/\", test_utt))\n",
        "\n",
        "# フルコンテキストラベルから音素のみを抽出\n",
        "test_phonemes = lab2phonemes(labels)\n",
        "\n",
        "# 言語特徴量の抽出に使うための質問ファイル\n",
        "binary_dict, numeric_dict = hts.load_question_set(ttslearn.util.example_qst_file())\n",
        "\n",
        "# 音素継続長の予測\n",
        "durations_test = predict_duration(\n",
        "    device, labels, duration_model, duration_config, duration_in_scaler, duration_out_scaler,\n",
        "    binary_dict, numeric_dict)\n",
        "durations_test_target = np.load(find_feats(\"dump/jsut_sr16000/org\", test_utt, typ=\"out_duration\"))\n",
        "\n",
        "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
        "ax.plot(durations_test_target, \"-+\", label=\"Target\")\n",
        "ax.plot(durations_test, \"--*\", label=\"Predicted\")\n",
        "ax.set_xticks(np.arange(len(test_phonemes)))\n",
        "ax.set_xticklabels(test_phonemes)\n",
        "ax.set_xlabel(\"Phoneme\")\n",
        "ax.set_ylabel(\"Duration (the number of frames)\")\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()"
      ],
      "id": "advance-visibility",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frank-flight"
      },
      "source": [
        "### 対数基本周波数の予測"
      ],
      "id": "frank-flight"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saving-tennis"
      },
      "source": [
        "from ttslearn.dnntts.gen import predict_acoustic\n",
        "\n",
        "labels = hts.load(find_lab(\"downloads/jsut_ver1.1/\", test_utt))\n",
        "# 対数基本周波数の予測\n",
        "out_feats = predict_acoustic(\n",
        "    device, labels, logf0_model, logf0_config, logf0_in_scaler,\n",
        "    logf0_out_scaler, binary_dict, numeric_dict)"
      ],
      "id": "saving-tennis",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prospective-bridges"
      },
      "source": [
        "from ttslearn.util import trim_silence\n",
        "from ttslearn.dnntts.multistream import split_streams\n",
        "\n",
        "# 結合された特徴量を分離\n",
        "out_feats = trim_silence(out_feats, labels)\n",
        "lf0_gen, vuv_gen = out_feats[:, 0], out_feats[:, 1]"
      ],
      "id": "prospective-bridges",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "speaking-municipality"
      },
      "source": [
        "from ttslearn.dnntts.multistream import get_static_features\n",
        "\n",
        "# 比較用に、自然音声から抽出された音響特徴量を読み込みむ\n",
        "feats = np.load(find_feats(\"dump/jsut_sr16000/org/\", test_utt, typ=\"out_logf0\"))\n",
        "# 特徴量の分離\n",
        "lf0_ref, vuv_ref = get_static_features(\n",
        "    feats, logf0_config.num_windows, logf0_config.stream_sizes, logf0_config.has_dynamic_features)"
      ],
      "id": "speaking-municipality",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "patent-imaging"
      },
      "source": [
        "#### F0の可視化"
      ],
      "id": "patent-imaging"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "published-modem"
      },
      "source": [
        "# 対数基本周波数から基本周波数への変換\n",
        "f0_ref = np.exp(lf0_ref)\n",
        "f0_ref[vuv_ref < 0.5] = 0\n",
        "f0_gen = np.exp(lf0_gen)\n",
        "f0_gen[vuv_gen < 0.5] = 0\n",
        "\n",
        "timeaxis = librosa.frames_to_time(np.arange(len(f0_ref)), sr=sr, hop_length=int(0.005 * sr))\n",
        "\n",
        "fix, ax = plt.subplots(1,1, figsize=(8,3))\n",
        "ax.plot(timeaxis, f0_ref, linewidth=2, label=\"F0 of natural speech\")\n",
        "ax.plot(timeaxis, f0_gen, \"--\", linewidth=2, label=\"F0 of generated speech\")\n",
        "\n",
        "ax.set_xlabel(\"Time [sec]\")\n",
        "ax.set_ylabel(\"Frequency [Hz]\")\n",
        "ax.set_xlim(timeaxis[0], timeaxis[-1])\n",
        "\n",
        "plt.legend()\n",
        "plt.tight_layout()"
      ],
      "id": "published-modem",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "configured-cloud"
      },
      "source": [
        "### 音声波形の生成"
      ],
      "id": "configured-cloud"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "white-miller"
      },
      "source": [
        "from ttslearn.dsp import inv_mulaw_quantize\n",
        "\n",
        "@torch.no_grad()\n",
        "def gen_waveform(\n",
        "    device,  # cpu or cuda\n",
        "    labels,  # フルコンテキストラベル\n",
        "    logf0_vuv,  # 連続対数基本周波数と有声/無声フラグ\n",
        "    wavenet_model,  # 学習済み WaveNet\n",
        "    wavenet_in_scaler,  # 条件付け特徴量の正規化用 StandardScaler\n",
        "    binary_dict,  # 二値特徴量を抽出する正規表現\n",
        "    numeric_dict,  # 数値特徴量を抽出する正規表現\n",
        "    tqdm=tqdm,  # プログレスバー\n",
        "):\n",
        "    # フレーム単位の言語特徴量の抽出\n",
        "    in_feats = fe.linguistic_features(\n",
        "        labels,\n",
        "        binary_dict,\n",
        "        numeric_dict,\n",
        "        add_frame_features=True,\n",
        "        subphone_features=\"coarse_coding\",\n",
        "    )\n",
        "    # フレーム単位の言語特徴量と、対数連続基本周波数・有声/無声フラグを結合\n",
        "    in_feats = np.hstack([in_feats, logf0_vuv])\n",
        "\n",
        "    # 特徴量の正規化\n",
        "    in_feats = wavenet_in_scaler.transform(in_feats)\n",
        "\n",
        "    # 条件付け特徴量を numpy.ndarray から torch.Tensor に変換\n",
        "    c = torch.from_numpy(in_feats).float().to(device)\n",
        "    # (B, T, C) -> (B, C, T)\n",
        "    c = c.view(1, -1, c.size(-1)).transpose(1, 2)\n",
        "\n",
        "    # 音声波形の長さを計算\n",
        "    upsample_scale = np.prod(wavenet_model.upsample_scales)\n",
        "    time_steps = (c.shape[-1] - wavenet_model.aux_context_window * 2) * upsample_scale\n",
        "\n",
        "    # WaveNet による音声波形の生成\n",
        "    # NOTE: 計算に時間を要するため、tqdm によるプログレスバーを利用します\n",
        "    gen_wav = wavenet_model.inference(c, time_steps, tqdm)\n",
        "\n",
        "    # One-hot ベクトルから 1 次元の信号に変換\n",
        "    gen_wav = gen_wav.max(1)[1].float().cpu().numpy().reshape(-1)\n",
        "\n",
        "    # Mu-law 量子化の逆変換\n",
        "    gen_wav = inv_mulaw_quantize(gen_wav, wavenet_model.out_channels - 1)\n",
        "\n",
        "    return gen_wav"
      ],
      "id": "white-miller",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "southeast-directive"
      },
      "source": [
        "### すべてのモデルを組み合わせて音声波形の生成"
      ],
      "id": "southeast-directive"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "medium-mention"
      },
      "source": [
        "# NOTE: False の場合、正解のdurationsを使います\n",
        "# すべてのモデルを連結する場合、True にしてください\n",
        "use_ground_truth_durations = True\n",
        "\n",
        "labels = hts.load(find_lab(\"downloads/jsut_ver1.1/\", test_utt))\n",
        "\n",
        "# 言語特徴量の抽出の下準備\n",
        "binary_dict, numeric_dict = hts.load_question_set(ttslearn.util.example_qst_file())\n",
        "\n",
        "if not use_ground_truth_durations:\n",
        "    # 音素継続長の予測\n",
        "    durations = predict_duration(\n",
        "        device, labels, duration_model, duration_config, duration_in_scaler, duration_out_scaler,\n",
        "        binary_dict, numeric_dict)\n",
        "\n",
        "    # 予測された継続長をフルコンテキストラベルに設定\n",
        "    labels.set_durations(durations)\n",
        "\n",
        "# 対数基本周波数の予測\n",
        "# NOTE: 動的特徴量をWaveNetの条件付け特徴量に用いるため、パラメータ生成 (mlpg) は行わない\n",
        "logf0_vuv = predict_acoustic(\n",
        "    device, labels, logf0_model, logf0_config, logf0_in_scaler,\n",
        "    logf0_out_scaler, binary_dict, numeric_dict, mlpg=False)\n",
        "\n",
        "# WaveNetによる音声波形の生成\n",
        "gen_wav = gen_waveform(\n",
        "    device, labels, logf0_vuv, wavenet_model, wavenet_in_scaler,\n",
        "    binary_dict, numeric_dict, tqdm)"
      ],
      "id": "medium-mention",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "broadband-theater"
      },
      "source": [
        "# 比較用に元音声の読み込み\n",
        "from scipy.io import wavfile\n",
        "_sr, ref_wav = wavfile.read(f\"./downloads/jsut_ver1.1/basic5000/wav/{test_utt}.wav\")\n",
        "ref_wav = (ref_wav / 32768.0).astype(np.float64)\n",
        "ref_wav = librosa.resample(ref_wav, _sr, sr)"
      ],
      "id": "broadband-theater",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enormous-cleaner"
      },
      "source": [
        "fig, ax = plt.subplots(2, 1, figsize=(8,6))\n",
        "\n",
        "hop_length = int(sr * 0.005)\n",
        "fft_size = pyworld.get_cheaptrick_fft_size(sr)\n",
        "\n",
        "spec_ref = librosa.stft(ref_wav, n_fft=fft_size, hop_length=hop_length, window=\"hann\")\n",
        "logspec_ref = np.log(np.abs(spec_ref))\n",
        "spec_gen = librosa.stft(gen_wav, n_fft=fft_size, hop_length=hop_length, window=\"hann\")\n",
        "logspec_gen = np.log(np.abs(spec_gen))\n",
        "\n",
        "mindb = min(logspec_ref.min(), logspec_gen.min())\n",
        "maxdb = max(logspec_ref.max(), logspec_gen.max())\n",
        "\n",
        "mesh = librosa.display.specshow(logspec_ref, hop_length=hop_length, sr=sr, cmap=cmap, x_axis=\"time\", y_axis=\"hz\", ax=ax[0])\n",
        "mesh.set_clim(mindb, maxdb)\n",
        "fig.colorbar(mesh, ax=ax[0], format=\"%+2.fdB\")\n",
        "\n",
        "mesh = librosa.display.specshow(logspec_gen, hop_length=hop_length, sr=sr, cmap=cmap, x_axis=\"time\", y_axis=\"hz\", ax=ax[1])\n",
        "mesh.set_clim(mindb, maxdb)\n",
        "fig.colorbar(mesh, ax=ax[1], format=\"%+2.fdB\")\n",
        "\n",
        "ax[0].set_title(\"Spectrogram of natural speech\")\n",
        "ax[1].set_title(\"Spectrogram of generated speech\")\n",
        "for a in ax:\n",
        "    a.set_xlabel(\"Time [sec]\")\n",
        "    a.set_ylabel(\"Frequency [Hz]\")\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "print(\"自然音声\")\n",
        "IPython.display.display(Audio(ref_wav, rate=sr))\n",
        "print(\"WaveNet音声合成\")\n",
        "IPython.display.display(Audio(gen_wav, rate=sr))\n",
        "\n",
        "# 図8-7\n",
        "savefig(\"./fig/wavenet_impl_tts_spec_comp\")"
      ],
      "id": "enormous-cleaner",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "multiple-wholesale"
      },
      "source": [
        "### 評価データに対して音声波形生成"
      ],
      "id": "multiple-wholesale"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "italian-external"
      },
      "source": [
        "#### レシピの stage 8 の実行"
      ],
      "id": "italian-external"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "corporate-cosmetic"
      },
      "source": [
        "if run_sh and run_stage8:\n",
        "    ! ./run.sh --stage 8 --stop-stage 8 \\\n",
        "        --tqdm $run_sh_tqdm --duration-model $duration_config_name --logf0-model $logf0_config_name --wavenet-model $wavenet_config_name \\\n",
        "        --reverse true --num-eval-utts $num_eval_utts"
      ],
      "id": "corporate-cosmetic",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foreign-giving"
      },
      "source": [
        "## 自然音声と合成音声の比較 (bonus)"
      ],
      "id": "foreign-giving"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "legislative-sampling"
      },
      "source": [
        "from pathlib import Path\n",
        "from ttslearn.util import load_utt_list\n",
        "\n",
        "with open(\"./downloads/jsut_ver1.1/basic5000/transcript_utf8.txt\") as f:\n",
        "    transcripts = {}\n",
        "    for l in f:\n",
        "        utt_id, script = l.split(\":\")\n",
        "        transcripts[utt_id] = script\n",
        "        \n",
        "eval_list = load_utt_list(\"data/eval.list\")[::-1][:5]\n",
        "\n",
        "for utt_id in eval_list:\n",
        "    # ref file \n",
        "    ref_file = f\"./downloads/jsut_ver1.1/basic5000/wav/{utt_id}.wav\"\n",
        "    _sr, ref_wav = wavfile.read(ref_file)\n",
        "    ref_wav = (ref_wav / 32768.0).astype(np.float64)\n",
        "    ref_wav = librosa.resample(ref_wav, _sr, sr)\n",
        "    \n",
        "    print(f\"{utt_id}: {transcripts[utt_id]}\")\n",
        "    print(\"自然音声\")\n",
        "    IPython.display.display(Audio(ref_wav, rate=sr))\n",
        "\n",
        "    gen_file = f\"exp/jsut_sr16000/synthesis_{duration_config_name}_{logf0_config_name}_{wavenet_config_name}/eval/{utt_id}.wav\"\n",
        "    if exists(gen_file):\n",
        "        _sr, gen_wav = wavfile.read(gen_file)    \n",
        "        print(\"WaveNet音声合成\")\n",
        "        IPython.display.display(Audio(gen_wav, rate=sr))\n",
        "    else:\n",
        "        # 音声生成が完了していない場合\n",
        "        print(\"WaveNet音声合成: not found\")"
      ],
      "id": "legislative-sampling",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "operating-worthy"
      },
      "source": [
        "## 学習済みモデルのパッケージング (bonus)\n",
        "\n",
        "学習済みモデルを利用したTTSに必要なファイルをすべて単一のディレクトリにまとめます。\n",
        "`ttslearn.wavenet.WaveNetTTS` クラスには、まとめたディレクトリを指定し、TTSを行う機能が実装されています。"
      ],
      "id": "operating-worthy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "provincial-delhi"
      },
      "source": [
        "### レシピの stage 99 の実行"
      ],
      "id": "provincial-delhi"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liable-mistake"
      },
      "source": [
        "if run_sh:\n",
        "    ! ./run.sh --stage 99 --stop-stage 99 \\\n",
        "        --duration-model $duration_config_name --logf0-model $logf0_config_name --wavenet-model $wavenet_config_name"
      ],
      "id": "liable-mistake",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meaning-bulgarian"
      },
      "source": [
        "!ls tts_models/jsut_sr16000_{duration_config_name}_{logf0_config_name}_{wavenet_config_name}"
      ],
      "id": "meaning-bulgarian",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pursuant-float"
      },
      "source": [
        "### パッケージングしたモデルを利用したTTS"
      ],
      "id": "pursuant-float"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "innocent-slovakia"
      },
      "source": [
        "from ttslearn.wavenet import WaveNetTTS\n",
        "\n",
        "# パッケージングしたモデルのパスを指定します\n",
        "engine = WaveNetTTS(\n",
        "    model_dir=f\"./tts_models/jsut_sr16000_{duration_config_name}_{logf0_config_name}_{wavenet_config_name}\"\n",
        ")\n",
        "wav, sr = engine.tts(\"ここまでお読みいただき、ありがとうございました。\", tqdm=tqdm)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,2))\n",
        "librosa.display.waveplot(wav.astype(np.float32), sr, ax=ax)\n",
        "ax.set_xlabel(\"Time [sec]\")\n",
        "ax.set_ylabel(\"Amplitude\")\n",
        "plt.tight_layout()\n",
        "\n",
        "Audio(wav, rate=sr)"
      ],
      "id": "innocent-slovakia",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "minute-church"
      },
      "source": [
        "if is_colab():\n",
        "    from datetime import timedelta\n",
        "    elapsed = (time.time() - start_time)\n",
        "    print(\"所要時間:\", str(timedelta(seconds=elapsed)))"
      ],
      "id": "minute-church",
      "execution_count": null,
      "outputs": []
    }
  ]
}